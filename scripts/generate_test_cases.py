#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆè„šæœ¬
åŠŸèƒ½ï¼šæ ¹æ®éœ€æ±‚æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
ä½œè€…: yuxiaoling
åˆ›å»ºæ—¥æœŸ: 2025-11-11
"""

import argparse
import difflib
import json
import os
import random
import re
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import requests
from colorama import Fore, init

# åˆå§‹åŒ–coloramaï¼ˆWindowså…¼å®¹ï¼‰
init(autoreset=True)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# é¢„å®šä¹‰çš„æ¨¡å‹é…ç½®ï¼ˆOllama å¯ç›´æ¥ä½¿ç”¨ï¼‰
MODEL_PRESETS = {
    # DeepSeek ç³»åˆ—
    "deepseek-coder": {
        "model": "deepseek-coder:6.7b",
        "description": "DeepSeek Coder 6.7B - ä»£ç èƒ½åŠ›å¼ºï¼Œé€‚åˆæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ",
        "recommended": True
    },
    "deepseek-chat": {
        "model": "deepseek-chat:6.7b",
        "description": "DeepSeek Chat 6.7B - é€šç”¨å¯¹è¯æ¨¡å‹",
        "recommended": False
    },
    "deepseek-r1": {
        "model": "deepseek-r1:7b",
        "description": "DeepSeek R1 7B - æ¨ç†æ¨¡å‹ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡",
        "recommended": False
    },
    # Qwen ç³»åˆ—ï¼ˆæ¨èï¼Œä¸­æ–‡èƒ½åŠ›å¼ºï¼‰
    "qwen2.5": {
        "model": "qwen2.5:7b",
        "description": "Qwen 2.5 7B - ä¸­æ–‡èƒ½åŠ›å¼ºï¼Œé€šç”¨å¯¹è¯ï¼ˆæ¨èï¼‰",
        "recommended": True
    },
    "qwen2.5-14b": {
        "model": "qwen2.5:14b",
        "description": "Qwen 2.5 14B - æ›´å¤§å‚æ•°ç‰ˆæœ¬ï¼Œè´¨é‡æ›´é«˜",
        "recommended": False
    },
    "qwen2.5-coder": {
        "model": "qwen2.5-coder:7b",
        "description": "Qwen 2.5 Coder 7B - ä»£ç ä¸“ç”¨ç‰ˆæœ¬",
        "recommended": False
    },
    # ChatGLM ç³»åˆ—
    "chatglm3": {
        "model": "chatglm3:6b",
        "description": "ChatGLM3 6B - æ¸…åå¯¹è¯æ¨¡å‹",
        "recommended": False
    },
    # å…¶ä»–æ¨¡å‹ï¼ˆéœ€è¦æ‰‹åŠ¨å¯¼å…¥åˆ° Ollamaï¼‰
    "minimax": {
        "model": "minimax-text-01",  # éœ€è¦æ‰‹åŠ¨å¯¼å…¥
        "description": "MiniMax Text-01 - éœ€è¦æ‰‹åŠ¨å¯¼å…¥åˆ° Ollama",
        "recommended": False
    },
    "kimi": {
        "model": "kimi-k2",  # éœ€è¦æ‰‹åŠ¨å¯¼å…¥
        "description": "Kimi K2 - éœ€è¦æ‰‹åŠ¨å¯¼å…¥åˆ° Ollama",
        "recommended": False
    }
}

# ============================================================
# é…ç½®åŒºåŸŸ - åœ¨è¿™é‡Œä¿®æ”¹è®¾ç½®
# ============================================================

# æ¨¡å‹é€‰æ‹©ï¼ˆåœ¨ä»£ç ä¸­ç›´æ¥é€‰æ‹©ï¼‰
# å¯é€‰å€¼ï¼š
#   - "deepseek-coder" (æ¨èï¼Œä»£ç èƒ½åŠ›å¼º)
#   - "qwen2.5" (æ¨èï¼Œä¸­æ–‡èƒ½åŠ›å¼º)
#   - "deepseek-chat" (é€šç”¨å¯¹è¯)
#   - "deepseek-r1" (æ¨ç†æ¨¡å‹)
#   - "qwen2.5-14b" (æ›´å¤§å‚æ•°ç‰ˆæœ¬)
#   - "qwen2.5-coder" (ä»£ç ä¸“ç”¨)
#   - "chatglm3" (æ¸…åå¯¹è¯æ¨¡å‹)
#   - æˆ–è€…ç›´æ¥å†™æ¨¡å‹åç§°ï¼Œå¦‚ "qwen2.5:7b"
SELECTED_MODEL = "qwen2.5"  # åœ¨è¿™é‡Œä¿®æ”¹è¦ä½¿ç”¨çš„æ¨¡å‹

# é»˜è®¤é…ç½®
DEFAULT_CONFIG = {
    "llm_base_url": "http://localhost:11434",  # Ollamaé»˜è®¤åœ°å€
    "default_model": "deepseek-coder:6.7b",
    "temperature": 0.7,
    "max_tokens": 8000,  # å¤§å¹…å¢åŠ tokenæ•°ä»¥æ”¯æŒç”Ÿæˆæ›´å¤šæµ‹è¯•ç”¨ä¾‹
    "timeout": 600  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°10åˆ†é’Ÿ
}

# é»˜è®¤éœ€æ±‚æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºè„šæœ¬ç›®å½•ï¼‰
DEFAULT_REQUIREMENT_FILE = "example_requirement.txt"

# æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶å¤„ç†çš„åŠŸèƒ½ç‚¹æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼Œè®¾ä¸ºNoneæˆ–0è¡¨ç¤ºå¤„ç†æ‰€æœ‰åŠŸèƒ½ç‚¹ï¼‰
TEST_FUNCTION_POINTS_LIMIT = 4  # æµ‹è¯•æ—¶è®¾ä¸º1ï¼Œæ­£å¼ä½¿ç”¨æ—¶è®¾ä¸ºNone

# é»˜è®¤è¾“å‡ºç›®å½•
DEFAULT_OUTPUT_DIR = "generated_test_cases"

# ä¿®å¤é…ç½®
class RepairConfig:
    """ä¿®å¤åŠŸèƒ½é…ç½®ç±»ï¼ˆé€šç”¨é…ç½®ï¼Œä¸ä¾èµ–ç‰¹å®šä¸šåŠ¡ï¼‰"""
    # æ ¼å¼ä¿®å¤é˜ˆå€¼
    FORMAT_FIX_MIN_LENGTH = 50
    FORMAT_FIX_KEYWORD_COUNT = 3

    # ç›¸ä¼¼åº¦åŒ¹é…é˜ˆå€¼ï¼ˆé™ä½é˜ˆå€¼ä»¥æé«˜åŒ¹é…ç‡ï¼‰
    SIMILARITY_THRESHOLD = 0.6  # ä»0.7é™ä½åˆ°0.6ï¼Œæé«˜åŒ¹é…ç‡
    KEYWORD_BONUS = 0.15  # ä»0.1å¢åŠ åˆ°0.15ï¼Œæé«˜å…³é”®è¯åŒ¹é…çš„æƒé‡

    # å€™é€‰å¥å­è¿‡æ»¤
    MIN_SENTENCE_LENGTH = 10
    MIN_VALID_CHARS = 5

    # åŒ¹é…æ£€æŸ¥
    KEY_PHRASE_COUNT = 3
    MIN_PHRASE_LENGTH = 5

    # preconditionsä¿®å¤
    PRECONDITIONS_DEFAULT = "æ»¡è¶³æµ‹è¯•å‰ç½®æ¡ä»¶"

# æ–‡æ¡£æå–é…ç½®
class ExtractionConfig:
    """æ–‡æ¡£æå–ç›¸å…³é…ç½®"""
    # å›é€€ç­–ç•¥ï¼šå½“æ‰¾ä¸åˆ°åŒ¹é…æ—¶ä½¿ç”¨çš„æ–‡æ¡£ç‰‡æ®µé•¿åº¦
    FALLBACK_SNIPPET_LENGTH = 2000

    # æœ€å°ç‰‡æ®µé•¿åº¦
    MIN_SNIPPET_LENGTH = 400

    # ä¸Šä¸‹æ–‡æ‰©å±•èŒƒå›´
    CONTEXT_BEFORE = 80
    CONTEXT_AFTER = 200
    EXTENDED_CONTEXT_BEFORE = 100
    EXTENDED_CONTEXT_AFTER = 400

    # ç« èŠ‚çª—å£èŒƒå›´
    SECTION_WINDOW_BEFORE = 20
    SECTION_WINDOW_AFTER = 80

    # åŒ¹é…ä½ç½®é™åˆ¶
    MAX_MATCH_POSITIONS = 10

    # æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼
    FUZZY_MATCH_THRESHOLD = 0.45

# é¢„ç¼–è¯‘å¸¸ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
RE_NORMALIZE = re.compile(r"[\s`~!@#$%^&*()_\-+=\[\]{}|\\;:'\",.<>/?Â·ï¼ï¿¥â€¦ï¼ˆï¼‰â€”ã€ã€‘ã€ï¼›ï¼š" "'â€™ã€Šã€‹ï¼Ÿ]")
RE_SPLIT_TOKENS = re.compile(r"[ã€ï¼Œ,;ï¼›ï¼š:ï¼ˆï¼‰()\\-\\s]+")
RE_SPLIT_PHRASES = re.compile(r'[ï¼Œ,ã€‚ï¼ï¼Ÿ!?\n]+')
RE_EXTRACT_KEYWORDS = re.compile(r'[^\w\u4e00-\u9fff]+')
RE_SENTENCE_SPLIT = re.compile(r"(?<=[ã€‚ï¼ï¼Ÿ!?])")
RE_TITLE_LINE = re.compile(r'^\d+\.\s*[^ã€‚ï¼ï¼Ÿ!?]{0,15}[ï¼š:]?\s*$')
RE_JSON_CASE = re.compile(r'\{\s*"case_name"[^}]+\}', re.DOTALL)

# ç¹ä½“æ ‡ç‚¹æ˜ å°„è¡¨ï¼ˆå®Œæ•´æ˜ å°„ï¼Œé€šç”¨ï¼‰
TRADITIONAL_TO_SIMPLIFIED_PUNCTUATION = {
    "ã€Œ": "ã€", "ã€": "ã€‘",
    "ã€": "ã€", "ã€": "ã€‘",
    "ï¹": "ã€", "ï¹‚": "ã€‘",
    "ï¹ƒ": "ã€", "ï¹„": "ã€‘",
    "ï¹™": "ï¼ˆ", "ï¹š": "ï¼‰",
    "ï¹›": "{", "ï¹œ": "}",
    "ï¹": "[", "ï¹": "]",
    "Â«": "ã€Š", "Â»": "ã€‹",
}


def slugify_function_point(function_point: str) -> str:
    """å°†åŠŸèƒ½ç‚¹åç§°è½¬æ¢ä¸ºé€‚åˆä½œä¸ºæ–‡ä»¶åçš„å­—ç¬¦ä¸²"""
    if not function_point:
        return "function_point"
    slug = re.sub(r"[^\w\u4e00-\u9fff-]+", "_", function_point)
    slug = slug.strip("_")
    return slug or "function_point"


class LLMService:
    """LLMæ¨¡å‹è°ƒç”¨æœåŠ¡"""

    def __init__(self, base_url: Optional[str] = None, model: Optional[str] = None,
                 temperature: Optional[float] = None, max_tokens: Optional[int] = None):
        self.base_url = base_url or DEFAULT_CONFIG["llm_base_url"]
        self.model = model or DEFAULT_CONFIG["default_model"]
        self.temperature = temperature or DEFAULT_CONFIG["temperature"]
        self.max_tokens = max_tokens or DEFAULT_CONFIG["max_tokens"]
        self.timeout = DEFAULT_CONFIG["timeout"]

    def generate(self, prompt: str) -> str:
        """
        è°ƒç”¨LLMç”Ÿæˆå†…å®¹

        Args:
            prompt: è¾“å…¥çš„æç¤ºè¯

        Returns:
            LLMç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        """
        try:
            # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
            if not self._check_service_available():
                raise ConnectionError(f"æ— æ³•è¿æ¥åˆ°LLMæœåŠ¡: {self.base_url}")

            # æ„å»ºè¯·æ±‚
            url = f"{self.base_url}/api/generate"
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": self.temperature,
                    "num_predict": self.max_tokens
                }
            }

            print(f"{Fore.CYAN}æ­£åœ¨è°ƒç”¨æ¨¡å‹: {self.model}...")
            response = requests.post(url, json=payload, timeout=self.timeout)
            response.raise_for_status()

            result = response.json()
            return result.get("response", "")

        except requests.exceptions.ConnectionError:
            raise ConnectionError(f"æ— æ³•è¿æ¥åˆ°LLMæœåŠ¡ï¼Œè¯·ç¡®ä¿æœåŠ¡è¿è¡Œåœ¨: {self.base_url}")
        except requests.exceptions.Timeout:
            raise TimeoutError(f"è¯·æ±‚è¶…æ—¶ï¼ˆ{self.timeout}ç§’ï¼‰ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–å¢åŠ è¶…æ—¶æ—¶é—´")
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"LLMæœåŠ¡è¯·æ±‚å¤±è´¥: {str(e)}")

    def _check_service_available(self) -> bool:
        """æ£€æŸ¥LLMæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False


class TestCaseGenerator:
    """æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨"""

    def __init__(self, llm_service: LLMService):
        self.llm_service = llm_service
        self._cached_requirement_doc: Optional[str] = None
        self._cached_doc_lines: List[str] = []
        self._cached_normalized_lines: List[str] = []
        self._cached_sections: List[Tuple[int, str]] = []

    @staticmethod
    def _normalize_text(text: str) -> str:
        """æ ‡å‡†åŒ–æ–‡æœ¬ç”¨äºåŒ¹é…"""
        cleaned = re.sub(r"[\s`~!@#$%^&*()_\-+=\[\]{}|\\;:'\",.<>/?Â·ï¼ï¿¥â€¦ï¼ˆï¼‰â€”ã€ã€‘ã€ï¼›ï¼šâ€œâ€â€˜â€™ã€Šã€‹ï¼Ÿ]", "", text or "")
        return cleaned.lower()

    @staticmethod
    def _fix_traditional_punctuation(text: str) -> str:
        """ä¿®å¤æ‰€æœ‰ç¹ä½“æ ‡ç‚¹ä¸ºç®€ä½“æ ‡ç‚¹ï¼ˆé€šç”¨æ–¹æ³•ï¼‰"""
        if not text:
            return text
        result = text
        for trad, simp in TRADITIONAL_TO_SIMPLIFIED_PUNCTUATION.items():
            result = result.replace(trad, simp)
        return result

    @staticmethod
    def _infer_preconditions_from_steps(steps: List[str]) -> str:
        """
        ä»æµ‹è¯•æ­¥éª¤æ¨æ–­å‰ç½®æ¡ä»¶ï¼ˆé€šç”¨æ–¹æ³•ï¼‰

        ç­–ç•¥ï¼š
        1. æå–ç¬¬ä¸€æ­¥çš„å…³é”®ä¿¡æ¯
        2. å¦‚æœç¬¬ä¸€æ­¥åŒ…å«çŠ¶æ€æè¿°ï¼Œç›´æ¥ä½¿ç”¨
        3. å¦åˆ™ä½¿ç”¨ç¬¬ä¸€æ­¥ä½œä¸ºå‰ç½®æ¡ä»¶
        """
        if not steps or not isinstance(steps, list) or len(steps) == 0:
            return RepairConfig.PRECONDITIONS_DEFAULT

        first_step = steps[0].strip() if steps[0] else ""
        if not first_step:
            return RepairConfig.PRECONDITIONS_DEFAULT

        # æå–ç¬¬ä¸€æ­¥çš„å…³é”®éƒ¨åˆ†ï¼ˆå»é™¤é€—å·åˆ†éš”çš„åç»­éƒ¨åˆ†ï¼‰
        preconditions = first_step.split("ï¼Œ")[0] if "ï¼Œ" in first_step else first_step
        preconditions = preconditions.split(",")[0] if "," in preconditions else preconditions

        # å¦‚æœç¬¬ä¸€æ­¥å¤ªçŸ­ï¼ˆå°‘äº5ä¸ªå­—ç¬¦ï¼‰ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if len(preconditions.strip()) < 5:
            return RepairConfig.PRECONDITIONS_DEFAULT

        return preconditions.strip()

    def _prepare_requirement_cache(self, requirement_doc: str):
        """ç¼“å­˜éœ€æ±‚æ–‡æ¡£çš„åˆ†è¡Œã€å½’ä¸€åŒ–å’Œç« èŠ‚ä¿¡æ¯ï¼Œé¿å…é‡å¤è®¡ç®—"""
        if self._cached_requirement_doc == requirement_doc:
            return

        self._cached_requirement_doc = requirement_doc
        self._cached_doc_lines = requirement_doc.splitlines()
        self._cached_normalized_lines = [self._normalize_text(line) for line in self._cached_doc_lines]
        self._cached_sections = self._detect_sections(self._cached_doc_lines)

    @staticmethod
    def _detect_sections(lines: List[str]) -> List[Tuple[int, str]]:
        """
        æ£€æµ‹æ–‡æ¡£ä¸­çš„ç« èŠ‚æ ‡é¢˜ï¼Œç”¨äºåç»­æˆªå–ä¸Šä¸‹æ–‡

        è§„åˆ™ï¼š
        - è¡Œå†…å®¹é•¿åº¦è¾ƒçŸ­ï¼ˆ< 80ï¼‰
        - ä¸ä»¥é¡¹ç›®ç¬¦å·/æ•°å­—åºå·å¼€å¤´
        - åŒ…å«ä¸­æ–‡æˆ–å¤§å†™å­—æ¯ï¼Œæˆ–è€…å½¢å¦‚â€œæ¨¡å—NPSâ€è¿™æ ·çš„æ ‡é¢˜
        """
        sections: List[Tuple[int, str]] = []
        heading_pattern = re.compile(r"^(?:[A-Za-z\u4e00-\u9fffã€].*)$")
        bullet_prefix = re.compile(r"^\s*(?:[-*â€¢â—â—¦Â·â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\d]+\s)")

        for idx, raw_line in enumerate(lines):
            line = raw_line.strip()
            if not line:
                continue
            if len(line) > 80:
                continue
            if bullet_prefix.match(line):
                continue
            if heading_pattern.match(line):
                sections.append((idx, line))

        if not sections or sections[0][0] != 0:
            sections.insert(0, (0, "__document_start__"))
        return sections

    def _locate_section_window(self, line_index: int, extra_before: int = 20, extra_after: int = 80) -> Tuple[int, int]:
        """
        æ ¹æ®ç« èŠ‚ä¿¡æ¯å’Œé¢å¤–ä¸Šä¸‹æ–‡ï¼Œè®¡ç®—éœ€è¦æˆªå–çš„èµ·æ­¢è¡Œå·
        """
        if not self._cached_sections:
            start = max(0, line_index - extra_before)
            end = min(len(self._cached_doc_lines), line_index + extra_after)
            return start, end

        start = 0
        end = len(self._cached_doc_lines)

        for idx, (section_line, _) in enumerate(self._cached_sections):
            if section_line <= line_index:
                start = section_line
                # ä¸‹ä¸€ä¸ªç« èŠ‚å¼€å§‹å‰ç»“æŸ
                if idx + 1 < len(self._cached_sections):
                    end = self._cached_sections[idx + 1][0]
                else:
                    end = len(self._cached_doc_lines)
            else:
                break

        start = max(0, start - extra_before)
        end = min(len(self._cached_doc_lines), end + extra_after)
        return start, end

    def build_prompt(self, requirement_doc: str) -> str:
        """
        æ„å»ºç”Ÿæˆæµ‹è¯•ç”¨ä¾‹çš„Prompt

        Args:
            requirement_doc: éœ€æ±‚æ–‡æ¡£å†…å®¹

        Returns:
            å®Œæ•´çš„Promptå­—ç¬¦ä¸²
        """
        prompt = f"""ä½ æ˜¯ä¸€ä½æœ‰ç€10å¹´æµ‹è¯•ç»éªŒçš„èµ„æ·±æµ‹è¯•å·¥ç¨‹å¸ˆã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹éœ€æ±‚æ–‡æ¡£ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æ–‡æ¡£å†…å®¹ç”Ÿæˆå…¨é¢çš„æµ‹è¯•ç”¨ä¾‹ã€‚

ã€ä»»åŠ¡è¦æ±‚ã€‘
æ ¹æ®ä»¥ä¸‹éœ€æ±‚æ–‡æ¡£ï¼Œç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ã€‚å¿…é¡»ä¸¥æ ¼æŒ‰ç…§éœ€æ±‚æ–‡æ¡£ä¸­çš„å®é™…åŠŸèƒ½ç‚¹ç”Ÿæˆã€‚

ã€éœ€æ±‚æ–‡æ¡£ã€‘
{requirement_doc}

ã€ç”Ÿæˆè¦æ±‚ã€‘
1. ä»”ç»†åˆ†æéœ€æ±‚æ–‡æ¡£ï¼Œè¯†åˆ«æ‰€æœ‰åŠŸèƒ½æ¨¡å—å’ŒåŠŸèƒ½ç‚¹
2. ä¸ºæ¯ä¸ªåŠŸèƒ½æ¨¡å—ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–ï¼š
   - UIå…ƒç´ æµ‹è¯•ï¼ˆæŒ‰é’®ã€å­—æ®µã€æ˜¾ç¤ºç­‰ï¼‰
   - äº¤äº’é€»è¾‘æµ‹è¯•ï¼ˆæŒ‰é’®çŠ¶æ€ã€å¼¹çª—é€»è¾‘ã€æµç¨‹è·³è½¬ç­‰ï¼‰
   - ä¸šåŠ¡è§„åˆ™æµ‹è¯•ï¼ˆä¸åŒæ¡ä»¶å¯¹åº”ä¸åŒè¡Œä¸ºç­‰ï¼‰
   - è¾¹ç•Œæ¡ä»¶æµ‹è¯•ï¼ˆå­—ç¬¦é™åˆ¶ã€æ•°å€¼èŒƒå›´ç­‰ï¼‰
3. å¯¹äºéœ€æ±‚æ–‡æ¡£ä¸­æåˆ°çš„æ¯ä¸ªå…·ä½“åŠŸèƒ½ã€æŒ‰é’®ã€å­—æ®µã€æµç¨‹ã€è§„åˆ™ã€é™åˆ¶ï¼Œéƒ½è¦æœ‰å¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹
4. æ ¹æ®éœ€æ±‚æ–‡æ¡£çš„å¤æ‚ç¨‹åº¦ï¼Œç”Ÿæˆè¶³å¤Ÿå…¨é¢çš„æµ‹è¯•ç”¨ä¾‹ï¼Œç¡®ä¿ä¸é—æ¼ä»»ä½•åŠŸèƒ½ç‚¹
5. **æ‰€æœ‰è¾“å‡ºå¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼Œä¸å¾—å‡ºç°ç¹ä½“å­—æˆ–ã€Œã€ç­‰ç¹ä½“æ ‡ç‚¹**
6. **ç¦æ­¢è‡†é€ éœ€æ±‚æ–‡æ¡£ä¸­æœªå‡ºç°çš„åŠŸèƒ½ã€æ–‡æ¡ˆæˆ–é¡µé¢ä½ç½®**

ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘
**é‡è¦ï¼šåªè¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—è¯´æ˜ï¼**

æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹å¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- case_name: æµ‹è¯•ç”¨ä¾‹åç§°ï¼ˆå¿…é¡»ä¸éœ€æ±‚æ–‡æ¡£ä¸­çš„åŠŸèƒ½ç‚¹å¯¹åº”ï¼‰
- description: ç”¨ä¾‹æè¿°ï¼ˆè¯¦ç»†è¯´æ˜æµ‹è¯•ç›®çš„ï¼Œå¼•ç”¨éœ€æ±‚æ–‡æ¡£ä¸­çš„å…·ä½“å†…å®¹ï¼‰
- preconditions: å‰ç½®æ¡ä»¶ï¼ˆåŸºäºéœ€æ±‚æ–‡æ¡£ä¸­çš„å‰ç½®è¦æ±‚ï¼‰
- steps: æµ‹è¯•æ­¥éª¤ï¼ˆæ•°ç»„æ ¼å¼ï¼Œæ¯ä¸ªæ­¥éª¤ä¸€è¡Œï¼Œå¿…é¡»ä¸éœ€æ±‚æ–‡æ¡£ä¸­çš„æ“ä½œæµç¨‹ä¸€è‡´ï¼‰
- expected_result: é¢„æœŸç»“æœï¼ˆå¿…é¡»ä¸éœ€æ±‚æ–‡æ¡£ä¸­çš„é¢„æœŸè¡Œä¸ºä¸€è‡´ï¼‰
- priority: ä¼˜å…ˆçº§ï¼ˆhigh/medium/lowï¼‰

ã€è¾“å‡ºæ ¼å¼ç¤ºä¾‹ã€‘
{{
  "test_cases": [
    {{
      "case_name": "ç”¨ä¾‹åç§°",
      "description": "ç”¨ä¾‹æè¿°",
      "preconditions": "å‰ç½®æ¡ä»¶",
      "steps": ["æ­¥éª¤1", "æ­¥éª¤2", "æ­¥éª¤3"],
      "expected_result": "é¢„æœŸç»“æœ",
      "priority": "high"
    }}
  ]
}}

ã€é‡è¦æé†’ã€‘
1. åªç”Ÿæˆéœ€æ±‚æ–‡æ¡£ä¸­æ˜ç¡®æåˆ°çš„åŠŸèƒ½
2. å¿…é¡»ç”Ÿæˆè¶³å¤Ÿå¤šçš„æµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–æ‰€æœ‰åŠŸèƒ½ç‚¹
3. **åªè¾“å‡ºJSONï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—è¯´æ˜**
4. **ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œå¯ä»¥ç›´æ¥è§£æ**"""

        return prompt

    def extract_function_points(self, requirement_doc: str) -> List[Dict]:
        """
        ç¬¬ä¸€æ­¥ï¼šæå–éœ€æ±‚æ–‡æ¡£ä¸­çš„æ‰€æœ‰åŠŸèƒ½ç‚¹ï¼ˆå¸¦å®šä½çº¿ç´¢ï¼‰

        Args:
            requirement_doc: éœ€æ±‚æ–‡æ¡£å†…å®¹

        Returns:
            åŠŸèƒ½ç‚¹åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«nameå’Œå®šä½çº¿ç´¢
        """
        print(f"{Fore.CYAN}ç¬¬ä¸€æ­¥ï¼šæå–éœ€æ±‚æ–‡æ¡£ä¸­çš„åŠŸèƒ½ç‚¹ï¼ˆå¸¦å®šä½çº¿ç´¢ï¼‰...")

        extract_prompt = f"""è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹éœ€æ±‚æ–‡æ¡£ï¼Œæå–æ‰€æœ‰åŠŸèƒ½ç‚¹ã€‚

éœ€æ±‚æ–‡æ¡£ï¼š
{requirement_doc}

è¯·è¯¦ç»†åˆ—å‡ºéœ€æ±‚æ–‡æ¡£ä¸­æåˆ°çš„æ‰€æœ‰åŠŸèƒ½ç‚¹ï¼ŒåŒ…æ‹¬ï¼š
- æ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼ˆå¦‚å…¨å±€NPSã€æ¨¡å—NPSç­‰ï¼‰
- æ‰€æœ‰æŒ‰é’®ï¼ˆå¦‚"å»è¯„åˆ†"ã€"å…³é—­"ã€"ä¸‹ä¸€æ­¥"ç­‰ï¼‰
- æ‰€æœ‰å­—æ®µï¼ˆå¦‚ä¸»æ ‡é¢˜ã€å‰¯æ ‡é¢˜ã€ç­”æ¡ˆé€‰é¡¹ç­‰ï¼‰
- æ‰€æœ‰æµç¨‹æ­¥éª¤ï¼ˆå¦‚å¼¹çª—å±•ç¤ºã€ç”¨æˆ·æ“ä½œã€æ•°æ®è®°å½•ç­‰ï¼‰
- æ‰€æœ‰è§„åˆ™å’Œé™åˆ¶ï¼ˆå¦‚å­—ç¬¦é™åˆ¶ã€æ—¶é—´é™åˆ¶ã€äººç¾¤é€‰æ‹©ç­‰ï¼‰
- æ‰€æœ‰å¼‚å¸¸åœºæ™¯ï¼ˆå¦‚è¶…å‡ºå­—ç¬¦é™åˆ¶ã€æœªç™»å½•ç­‰ï¼‰

**é‡è¦è¦æ±‚ï¼š**
- ä½¿ç”¨ç®€ä½“ä¸­æ–‡æè¿°åŠŸèƒ½ç‚¹ï¼Œä¸è¦ä½¿ç”¨ç¹ä½“å­—æˆ–ã€Œã€ç­‰ç¹ä½“æ ‡ç‚¹
- ä¸è¦é—æ¼éœ€æ±‚æ–‡æ¡£ä¸­å‡ºç°çš„ä»»æ„åŠŸèƒ½ç‚¹
- ä¸è¦è‡†é€ éœ€æ±‚æ–‡æ¡£ä¸­æ²¡æœ‰çš„å†…å®¹
- æ¯ä¸ªåŠŸèƒ½ç‚¹å¿…é¡»æä¾›å®šä½çº¿ç´¢ï¼Œå¸®åŠ©ç¨‹åºæ‰¾åˆ°ç›¸å…³åŸæ–‡

**é‡è¦ï¼šå¿…é¡»æå–æ‰€æœ‰åŠŸèƒ½ç‚¹ï¼Œä¸èƒ½é—æ¼ã€‚å¦‚æœéœ€æ±‚æ–‡æ¡£å¾ˆé•¿ï¼Œåº”è¯¥æå–è‡³å°‘20-30ä¸ªåŠŸèƒ½ç‚¹ã€‚**

**å…³é”®è¯æå–è¦æ±‚ï¼š**
- keywordsï¼šæä¾›2-4ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼Œè¿™äº›å…³é”®è¯å¿…é¡»æ˜¯æ–‡æ¡£ä¸­çœŸå®å­˜åœ¨çš„çŸ­è¯­ï¼Œç”¨äºç²¾ç¡®å®šä½åŸæ–‡
- é¿å…è¿‡äºæ³›åŒ–çš„è¯è¯­ï¼ˆå¦‚"å…³é—­"ã€"æ“ä½œ"ã€"æ˜¾ç¤º"ç­‰ï¼‰ï¼Œä¼˜å…ˆé€‰æ‹©å…·ä½“çš„æè¿°æ€§è¯è¯­
- exact_phrasesï¼šæä¾›1ä¸ªæ–‡æ¡£ä¸­çš„ç¡®åˆ‡çŸ­è¯­æˆ–å¥å­ï¼Œè¿™ä¸ªçŸ­è¯­å¿…é¡»é€å­—æ¥è‡ªæ–‡æ¡£åŸæ–‡
- section_hintï¼šç®€çŸ­çš„ç« èŠ‚åç§°æˆ–ä¸Šä¸‹æ–‡çº¿ç´¢ï¼ˆå¯é€‰ï¼‰

**é‡è¦æ ¼å¼è¦æ±‚ï¼š**
- å¿…é¡»ä½¿ç”¨åŒå¼•å·ï¼Œä¸è¦ä½¿ç”¨å•å¼•å·
- å­—ç¬¦ä¸²ä¸­çš„ç‰¹æ®Šå­—ç¬¦è¦æ­£ç¡®è½¬ä¹‰
- ç¡®ä¿JSONæ ¼å¼å®Œå…¨æ­£ç¡®

è¾“å‡ºæ ¼å¼ï¼š
{{
  "function_points": [
    {{
      "name": "æ•°æ®è®°å½•æµç¨‹",
      "keywords": ["ç”¨æˆ·ID", "å¡«å†™å†…å®¹", "è·Ÿè¸ª", "è®¿è°ˆ"],
      "section_hint": "æ³¨æ„äº‹é¡¹",
      "exact_phrases": ["éœ€è®°å½•å¡«å†™ç”¨æˆ·çš„IDå’Œå¡«å†™çš„å†…å®¹"]
    }}
  ]
}}

åªè¾“å‡ºJSONï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"""

        try:
            response = self.llm_service.generate(extract_prompt)
            print(f"{Fore.CYAN}  åŸå§‹å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")

            # æå–JSON
            start_idx = response.find("{")
            end_idx = response.rfind("}") + 1
            if start_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx]
                print(f"{Fore.CYAN}  æå–çš„JSONé•¿åº¦: {len(json_str)} å­—ç¬¦")
            else:
                print(f"{Fore.RED}âœ— æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONè¾¹ç•Œ")
                print(f"{Fore.WHITE}åŸå§‹å“åº”é¢„è§ˆ:\n{response[:500]}...")
                raise ValueError("æ— æ³•æå–JSONå†…å®¹")

            # å°è¯•è§£æJSON
            result = None
            try:
                result = json.loads(json_str)
            except json.JSONDecodeError as json_err:
                print(f"{Fore.RED}âœ— JSONè§£æå¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤: {str(json_err)}")

                # å°è¯•è‡ªåŠ¨ä¿®å¤å¸¸è§JSONé—®é¢˜
                fixed_json = json_str
                original_length = len(fixed_json)

                # ä¿®å¤1: ç§»é™¤å°¾éƒ¨å¤šä½™çš„é€—å·
                fixed_json = re.sub(r',(\s*[}\]])', r'\1', fixed_json)

                # ä¿®å¤2: ä¿®å¤å•å¼•å·ä¸ºåŒå¼•å·ï¼ˆæ›´ç²¾ç¡®çš„æ­£åˆ™ï¼Œé¿å…è¯¯ä¿®å¤å­—ç¬¦ä¸²å†…å®¹ï¼‰
                # åªä¿®å¤å¯¹è±¡é”®å’Œç®€å•å­—ç¬¦ä¸²å€¼ï¼Œä¸ä¿®å¤å­—ç¬¦ä¸²å†…å®¹ä¸­çš„å•å¼•å·
                # ä¿®å¤é”®ï¼š'key': -> "key":
                fixed_json = re.sub(r"'([^']*)':\s*", r'"\1": ', fixed_json)
                # ä¿®å¤ç®€å•å€¼ï¼š: 'value' -> : "value" (ä½†æ’é™¤åŒ…å«å•å¼•å·çš„å€¼)
                fixed_json = re.sub(r":\s*'([^']*)'(?=\s*[,}\]])", r': "\1"', fixed_json)

                # ä¿®å¤3: ç§»é™¤å¤šä½™çš„è½¬ä¹‰
                fixed_json = fixed_json.replace('\\"', '"')

                print(f"{Fore.CYAN}  å°è¯•ä¿®å¤JSON ({original_length} -> {len(fixed_json)} å­—ç¬¦)")

                try:
                    result = json.loads(fixed_json)
                    print(f"{Fore.GREEN}âœ“ JSONè‡ªåŠ¨ä¿®å¤æˆåŠŸ")
                except json.JSONDecodeError as fix_err:
                    print(f"{Fore.RED}âœ— è‡ªåŠ¨ä¿®å¤å¤±è´¥: {str(fix_err)}")
                    print(f"{Fore.YELLOW}é—®é¢˜ä½ç½®é¢„è§ˆ (é™„è¿‘50å­—ç¬¦):")
                    error_pos = json_err.pos
                    start_preview = max(0, error_pos - 25)
                    end_preview = min(len(json_str), error_pos + 25)
                    preview = json_str[start_preview:end_preview]
                    print(f"{Fore.WHITE}'{preview}'")
                    print(f"{Fore.RED}{' ' * (error_pos - start_preview)}^ è¿™é‡Œæœ‰é—®é¢˜")

                    # æä¾›ä¿®å¤å»ºè®®
                    if "Expecting ',' delimiter" in str(json_err):
                        print(f"{Fore.YELLOW}ğŸ’¡ å¯èƒ½æ˜¯ç¼ºå°‘é€—å·æˆ–æœ‰å¤šä½™é€—å·")
                    elif "Expecting ':' delimiter" in str(json_err):
                        print(f"{Fore.YELLOW}ğŸ’¡ å¯èƒ½æ˜¯ç¼ºå°‘å†’å·åˆ†éš”ç¬¦")
                    elif "Unterminated string" in str(json_err):
                        print(f"{Fore.YELLOW}ğŸ’¡ å¯èƒ½æ˜¯å­—ç¬¦ä¸²å¼•å·ä¸åŒ¹é…")

                    raise json_err

            function_points_data = result.get("function_points", [])

            # å…¼å®¹æ—§æ ¼å¼ï¼šå¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
            if function_points_data and isinstance(function_points_data[0], str):
                function_points_data = [
                    {
                        "name": fp,
                        "keywords": [fp],
                        "section_hint": "",
                        "exact_phrases": []
                    }
                    for fp in function_points_data
                ]

            print(f"{Fore.GREEN}âœ“ æå–åˆ° {len(function_points_data)} ä¸ªåŠŸèƒ½ç‚¹ï¼ˆå¸¦å®šä½çº¿ç´¢ï¼‰")
            return function_points_data

        except Exception as e:
            print(f"{Fore.YELLOW}âš  åŠŸèƒ½ç‚¹æå–å¤±è´¥ï¼Œå°†ç›´æ¥ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹: {str(e)}")
            return []

    def extract_relevant_section(self, requirement_doc: str, function_point: str, fp_data: Optional[Dict] = None) -> str:
        """
        æå–ä¸åŠŸèƒ½ç‚¹ç›¸å…³çš„éœ€æ±‚æ–‡æ¡£ç‰‡æ®µï¼ˆä½¿ç”¨AIæä¾›çš„å®šä½çº¿ç´¢è¿›è¡Œç²¾ç¡®åŒ¹é…ï¼‰
        """
        print(f"{Fore.CYAN}  æ­£åœ¨æå–ä¸'{function_point}'ç›¸å…³çš„éœ€æ±‚æ–‡æ¡£ç‰‡æ®µ...")

        self._prepare_requirement_cache(requirement_doc)

        # åˆå§‹åŒ–å€™é€‰ç´¢å¼•åˆ—è¡¨
        candidate_indices: List[int] = []

        # å¦‚æœæœ‰AIæä¾›çš„å®šä½çº¿ç´¢ï¼Œä¼˜å…ˆä½¿ç”¨
        if fp_data:

            # 1) ä½¿ç”¨exact_phrasesè¿›è¡Œç²¾ç¡®åŒ¹é…
            exact_phrases = fp_data.get("exact_phrases", [])
            for phrase in exact_phrases:
                if phrase:
                    normalized_phrase = self._normalize_text(phrase)
                    for idx, normalized_line in enumerate(self._cached_normalized_lines):
                        if normalized_phrase and normalized_phrase in normalized_line:
                            candidate_indices.append(idx)
                            break  # æ‰¾åˆ°ä¸€ä¸ªå°±å¤Ÿäº†

            # 2) ä½¿ç”¨keywordsè¿›è¡Œå…³é”®è¯åŒ¹é…
            if not candidate_indices:
                keywords = fp_data.get("keywords", [])
                if keywords:
                    print(f"{Fore.CYAN}    ä½¿ç”¨å…³é”®è¯è¿›è¡ŒåŒ¹é…: {', '.join(keywords[:5])}...")
                    # è¿‡æ»¤æ‰è¿‡äºæ³›åŒ–çš„å…³é”®è¯
                    filtered_keywords = []
                    for keyword in keywords:
                        # è·³è¿‡è¿‡äºçŸ­æˆ–è¿‡äºæ³›åŒ–çš„å…³é”®è¯
                        if len(keyword.strip()) < 2 or keyword.lower() in ['å…³é—­', 'æ“ä½œ', 'æ˜¾ç¤º', 'ç‚¹å‡»', 'æŒ‰é’®']:
                            continue
                        filtered_keywords.append(keyword)

                    if filtered_keywords:
                        # æ”¹è¿›ï¼šè‡³å°‘åŒ¹é…ä¸€åŠçš„å…³é”®è¯ï¼ˆæ›´å®½æ¾çš„åŒ¹é…ï¼‰
                        min_match_count = max(1, len(filtered_keywords) // 2)
                        for idx, normalized_line in enumerate(self._cached_normalized_lines):
                            line_text = normalized_line.lower()
                            matched_count = sum(1 for keyword in filtered_keywords if keyword.lower() in line_text)
                            if matched_count >= min_match_count:
                                candidate_indices.append(idx)
                        if candidate_indices:
                            print(f"{Fore.CYAN}  âœ“ ä½¿ç”¨å…³é”®è¯åŒ¹é…æ‰¾åˆ° {len(candidate_indices)} ä¸ªä½ç½®")

            # 3) ä½¿ç”¨section_hintç¼©å°èŒƒå›´
            section_hint = fp_data.get("section_hint", "")
            if section_hint and candidate_indices:
                # æŸ¥æ‰¾ç« èŠ‚æ ‡é¢˜
                section_indices = []
                for idx, line in enumerate(self._cached_doc_lines):
                    if section_hint.lower() in line.lower():
                        section_indices.append(idx)

                # åªä¿ç•™åœ¨ç›¸å…³ç« èŠ‚å†…çš„åŒ¹é…
                if section_indices:
                    filtered_indices = []
                    for candidate_idx in candidate_indices:
                        # æ£€æŸ¥æ˜¯å¦åœ¨ç« èŠ‚èŒƒå›´å†…
                        for section_idx in section_indices:
                            if section_idx <= candidate_idx < section_idx + 50:  # ç« èŠ‚èŒƒå›´å¤§çº¦50è¡Œ
                                filtered_indices.append(candidate_idx)
                                break
                    if filtered_indices:
                        candidate_indices = filtered_indices

            if candidate_indices:
                print(f"{Fore.CYAN}  âœ“ ä½¿ç”¨AIå®šä½çº¿ç´¢æ‰¾åˆ° {len(candidate_indices)} ä¸ªåŒ¹é…ä½ç½®")

        # å¦‚æœæ²¡æœ‰fp_dataï¼Œæˆ–è€…è™½ç„¶æœ‰fp_dataä½†æ²¡æ‰¾åˆ°åŒ¹é…ï¼Œå›é€€åˆ°åŸæœ‰çš„åŒ¹é…ç­–ç•¥
        if not candidate_indices:
            # å›é€€åˆ°åŸæœ‰çš„åŒ¹é…ç­–ç•¥
            normalized_target = self._normalize_text(function_point)
            if not normalized_target:
                print(f"{Fore.YELLOW}  âš  åŠŸèƒ½ç‚¹'{function_point}'æ— æ³•å½’ä¸€åŒ–ï¼Œä½¿ç”¨åŸå§‹æ–‡æ¡£ç‰‡æ®µ")
                return requirement_doc[:ExtractionConfig.FALLBACK_SNIPPET_LENGTH] if len(requirement_doc) > ExtractionConfig.FALLBACK_SNIPPET_LENGTH else requirement_doc

            candidate_indices = []

            # 1) ç²¾ç¡®åŒ…å«åŒ¹é…
            for idx, normalized_line in enumerate(self._cached_normalized_lines):
                if normalized_target and normalized_target in normalized_line:
                    candidate_indices.append(idx)

            # 2) è¯ç²’åº¦åŒ¹é…
            if not candidate_indices:
                tokens = [
                    self._normalize_text(token)
                    for token in RE_SPLIT_TOKENS.split(function_point)
                    if token.strip()
                ]
                strong_tokens = [token for token in tokens if len(token) >= 2]
                if strong_tokens:
                    for idx, normalized_line in enumerate(self._cached_normalized_lines):
                        if all(token in normalized_line for token in strong_tokens):
                            candidate_indices.append(idx)

            # 3) æ¨¡ç³ŠåŒ¹é…
            if not candidate_indices and self._cached_normalized_lines:
                ratios = [
                    (difflib.SequenceMatcher(None, normalized_target, normalized_line).ratio(), idx)
                    for idx, normalized_line in enumerate(self._cached_normalized_lines)
                    if normalized_line
                ]
                if ratios:
                    best_ratio, best_idx = max(ratios, key=lambda item: item[0])
                    if best_ratio >= ExtractionConfig.FUZZY_MATCH_THRESHOLD:
                        candidate_indices.append(best_idx)

        if not candidate_indices:
            # å°è¯•æ›´å®½æ¾çš„åŒ¹é…ç­–ç•¥ï¼šä½¿ç”¨åŠŸèƒ½ç‚¹åç§°çš„å•ä¸ªå…³é”®è¯
            print(f"{Fore.YELLOW}  âš  æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•ä½¿ç”¨å…³é”®è¯åŒ¹é…...")
            tokens = [
                self._normalize_text(token)
                for token in RE_SPLIT_TOKENS.split(function_point)
                if token.strip() and len(token.strip()) >= 2
            ]
            # å°è¯•åŒ¹é…å•ä¸ªå…³é”®è¯
            for token in tokens[:3]:  # æœ€å¤šå°è¯•å‰3ä¸ªå…³é”®è¯
                if token:
                    for idx, normalized_line in enumerate(self._cached_normalized_lines):
                        if token in normalized_line:
                            candidate_indices.append(idx)
                    if candidate_indices:
                        print(f"{Fore.GREEN}  âœ“ ä½¿ç”¨å…³é”®è¯'{token}'æ‰¾åˆ° {len(candidate_indices)} ä¸ªåŒ¹é…ä½ç½®")
                        break

            # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨åŸæ–‡æ¡£å‰Nå­—ç¬¦ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ï¼‰
            if not candidate_indices:
                print(f"{Fore.YELLOW}  âš  æœªæ‰¾åˆ°åŒ…å«'{function_point}'çš„å†…å®¹ï¼Œä½¿ç”¨åŸæ–‡æ¡£å‰{ExtractionConfig.FALLBACK_SNIPPET_LENGTH}å­—ç¬¦")
                return requirement_doc[:ExtractionConfig.FALLBACK_SNIPPET_LENGTH] if len(requirement_doc) > ExtractionConfig.FALLBACK_SNIPPET_LENGTH else requirement_doc

        # å¦‚æœåŒ¹é…ä½ç½®å¤ªå¤šï¼Œè¿›è¡Œé¢å¤–ç­›é€‰ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ï¼‰
        if len(candidate_indices) > ExtractionConfig.MAX_MATCH_POSITIONS:
            print(f"{Fore.YELLOW}  âš  æ‰¾åˆ° {len(candidate_indices)} ä¸ªåŒ¹é…ä½ç½®ï¼Œå°è¯•ä¼˜åŒ–ç­›é€‰...")
            # ä½¿ç”¨ç« èŠ‚ä¿¡æ¯è¿›ä¸€æ­¥ç­›é€‰
            if fp_data and fp_data.get("section_hint"):
                section_hint = fp_data.get("section_hint", "").lower()
                filtered_indices = []
                for idx in candidate_indices:
                    # æ£€æŸ¥å‘¨å›´å†…å®¹æ˜¯å¦åŒ…å«ç« èŠ‚çº¿ç´¢
                    start_check = max(0, idx - 5)
                    end_check = min(len(self._cached_doc_lines), idx + 5)
                    context = "\n".join(self._cached_doc_lines[start_check:end_check]).lower()
                    if section_hint in context:
                        filtered_indices.append(idx)
                if filtered_indices:
                    print(f"{Fore.GREEN}  âœ“ ç« èŠ‚ç­›é€‰åå‰©ä½™ {len(filtered_indices)} ä¸ªä½ç½®")
                    candidate_indices = filtered_indices[:ExtractionConfig.MAX_MATCH_POSITIONS]  # é™åˆ¶æœ€å¤šNä¸ª

            # å¦‚æœä»ç„¶å¤ªå¤šï¼Œåªä¿ç•™å‰Nä¸ªï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ï¼‰
            if len(candidate_indices) > ExtractionConfig.MAX_MATCH_POSITIONS:
                candidate_indices = candidate_indices[:ExtractionConfig.MAX_MATCH_POSITIONS]
                print(f"{Fore.YELLOW}  âš  é™åˆ¶åŒ¹é…ä½ç½®æ•°é‡ä¸º{ExtractionConfig.MAX_MATCH_POSITIONS}ä¸ª")

        collected_indices: set[int] = set()
        for idx in candidate_indices:
            start, end = self._locate_section_window(idx)
            collected_indices.update(range(start, end))

        if not collected_indices:
            print(f"{Fore.YELLOW}  âš  æœªèƒ½ç¡®å®šç« èŠ‚èŒƒå›´ï¼Œä½¿ç”¨åŒ¹é…è¡Œé™„è¿‘å†…å®¹")
            idx = candidate_indices[0]
            start = max(0, idx - ExtractionConfig.CONTEXT_BEFORE)
            end = min(len(self._cached_doc_lines), idx + ExtractionConfig.CONTEXT_AFTER)
            collected_indices.update(range(start, end))

        relevant_lines = [self._cached_doc_lines[i] for i in sorted(collected_indices)]
        relevant_section = "\n".join(relevant_lines).strip()

        if len(relevant_section) < ExtractionConfig.MIN_SNIPPET_LENGTH:
            print(f"{Fore.YELLOW}  âš  æå–ç‰‡æ®µä¸è¶³{ExtractionConfig.MIN_SNIPPET_LENGTH}å­—ç¬¦ï¼Œè‡ªåŠ¨æ‰©å±•ä¸Šä¸‹æ–‡")
            idx = candidate_indices[0]
            start = max(0, idx - ExtractionConfig.EXTENDED_CONTEXT_BEFORE)
            end = min(len(self._cached_doc_lines), idx + ExtractionConfig.EXTENDED_CONTEXT_AFTER)
            relevant_section = "\n".join(self._cached_doc_lines[start:end]).strip()

        print(f"{Fore.GREEN}  âœ“ æå–åˆ° {len(relevant_section)} å­—ç¬¦çš„ç›¸å…³å†…å®¹ï¼ˆåŸæ–‡ï¼‰")
        return relevant_section

    def generate_test_cases_for_point(self, requirement_doc: str, function_point: str, fp_data: Optional[Dict] = None) -> Tuple[List[Dict], List[str], str]:
        """
        ä¸ºå•ä¸ªåŠŸèƒ½ç‚¹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹

        Args:
            requirement_doc: éœ€æ±‚æ–‡æ¡£å†…å®¹
            function_point: åŠŸèƒ½ç‚¹åç§°
            fp_data: åŠŸèƒ½ç‚¹æ•°æ®ï¼ˆåŒ…å«å®šä½çº¿ç´¢ï¼‰

        Returns:
            æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
        """
        # æå–ä¸åŠŸèƒ½ç‚¹ç›¸å…³çš„éœ€æ±‚æ–‡æ¡£ç‰‡æ®µ
        doc_snippet = self.extract_relevant_section(requirement_doc, function_point, fp_data)

        prompt = f"""ä½ æ˜¯ä¸€ä½æµ‹è¯•å·¥ç¨‹å¸ˆã€‚è¯·æ ¹æ®éœ€æ±‚æ–‡æ¡£ä¸ºåŠŸèƒ½ç‚¹"{function_point}"ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ã€‚

**é‡è¦ï¼šåªè¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—ï¼**

ã€éœ€æ±‚æ–‡æ¡£ã€‘
{doc_snippet}

ã€åŠŸèƒ½ç‚¹ã€‘
{function_point}

ã€è¦æ±‚ã€‘
1. åªç”Ÿæˆä¸"{function_point}"ç›¸å…³çš„æµ‹è¯•ç”¨ä¾‹
2. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§éœ€æ±‚æ–‡æ¡£ä¸­çš„å†…å®¹ç”Ÿæˆï¼Œä¸èƒ½ç¼–é€ 
3. **æµ‹è¯•ç”¨ä¾‹ä¸­çš„expected_resultå­—æ®µå¿…é¡»é€å­—å¼•ç”¨éœ€æ±‚æ–‡æ¡£ä¸­çš„åŸå¥ï¼Œä¸èƒ½æ”¹å†™ã€æ€»ç»“æˆ–æ„è¯‘**
4. ä¸èƒ½æ·»åŠ éœ€æ±‚æ–‡æ¡£ä¸­æ²¡æœ‰çš„åŠŸèƒ½ã€æŒ‰é’®ã€æ“ä½œ
5. **è¾“å‡ºå¿…é¡»å…¨éƒ¨ä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼Œç¦æ­¢å‡ºç°ç¹ä½“å­—æˆ–ã€Œã€ç­‰ç¹ä½“æ ‡ç‚¹**
6. **ç”¨ä¾‹ä¸­æ¶‰åŠçš„é¡µé¢/æ¨¡å—åç§°å¿…é¡»ä¸éœ€æ±‚æ–‡æ¡£ä¿æŒä¸€è‡´ï¼Œä¸å¾—è‡ªè¡Œæ›´æ¢é¡µé¢ä½ç½®**

ã€ç”Ÿæˆè§„åˆ™ã€‘
- ä»”ç»†é˜…è¯»éœ€æ±‚æ–‡æ¡£ä¸­å…³äº"{function_point}"çš„æ‰€æœ‰æè¿°
- ä¸ºæ¯ä¸ªUIå…ƒç´ ã€äº¤äº’é€»è¾‘ã€ä¸šåŠ¡è§„åˆ™ã€é™åˆ¶æ¡ä»¶ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
- **expected_resultå¿…é¡»ä»éœ€æ±‚æ–‡æ¡£ä¸­ç›´æ¥å¤åˆ¶å®Œæ•´çš„å¥å­ï¼Œä¸èƒ½ä¿®æ”¹ä»»ä½•æ–‡å­—**
- å¦‚æœéœ€æ±‚æ–‡æ¡£å†™"ç‚¹å‡»å…³é—­ç›´æ¥æ¶ˆå¤±"ï¼Œexpected_resultå¿…é¡»å†™"ç‚¹å‡»å…³é—­ç›´æ¥æ¶ˆå¤±"ï¼ˆä¸èƒ½å†™æˆ"ç‚¹å‡»å…³é—­æŒ‰é’®åå¼¹çª—æ¶ˆå¤±"ï¼‰
- ç›´æ¥å¼•ç”¨éœ€æ±‚æ–‡æ¡£ä¸­çš„æŒ‰é’®åç§°ã€å­—æ®µåç§°ã€æ ‡é¢˜æ–‡å­—
- è¾“å‡ºç»“æ„åŒ–ä¸”è¯¦å°½ï¼Œç¡®ä¿è¦†ç›–æ­£å¸¸æµç¨‹ã€è¾¹ç•Œåœºæ™¯ä¸çº¦æŸæ¡ä»¶

ã€ç¤ºä¾‹ã€‘
å¦‚æœéœ€æ±‚æ–‡æ¡£å†™"ä¸»æ ‡é¢˜ï¼šå–œæ¬¢RingConnå—ï¼Ÿæ¥è¯„åˆ†å§ï¼"ï¼Œæµ‹è¯•ç”¨ä¾‹åº”è¯¥å†™ï¼š
{{
  "case_name": "éªŒè¯Bannerä¸»æ ‡é¢˜æ˜¾ç¤º",
  "description": "éªŒè¯Bannerä¸»æ ‡é¢˜æ˜¾ç¤º",
  "preconditions": "ç”¨æˆ·å·²ç™»å½•ï¼Œåœ¨'ä»Šå¤©'é¡µé¢",
  "steps": ["æ‰“å¼€appï¼Œè¿›å…¥'ä»Šå¤©'é¡µé¢", "æŸ¥çœ‹Bannerä¸»æ ‡é¢˜"],
  "expected_result": "ä¸»æ ‡é¢˜ï¼šå–œæ¬¢RingConnå—ï¼Ÿæ¥è¯„åˆ†å§ï¼",
  "priority": "high"
}}

**æ³¨æ„**ï¼šexpected_resultå¿…é¡»ç›´æ¥å¤åˆ¶éœ€æ±‚æ–‡æ¡£ä¸­çš„åŸå¥ï¼Œä¸èƒ½æ·»åŠ "Bannerä¸»æ ‡é¢˜æ˜¾ç¤ºä¸º"è¿™æ ·çš„æè¿°æ€§æ–‡å­—ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
åªè¾“å‡ºJSONï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

{{
  "test_cases": [
    {{
      "case_name": "ç”¨ä¾‹åç§°",
      "description": "ç”¨ä¾‹æè¿°",
      "preconditions": "å‰ç½®æ¡ä»¶",
      "steps": ["æ­¥éª¤1", "æ­¥éª¤2"],
      "expected_result": "é¢„æœŸç»“æœï¼ˆå¿…é¡»é€å­—å¤åˆ¶éœ€æ±‚æ–‡æ¡£ä¸­çš„åŸå¥ï¼Œä¸èƒ½æ”¹å†™ï¼‰",
      "priority": "high"
    }}
  ]
}}

**å…³é”®è¦æ±‚**ï¼š
- expected_resultå­—æ®µå¿…é¡»ä»éœ€æ±‚æ–‡æ¡£ä¸­ç›´æ¥å¤åˆ¶å®Œæ•´çš„åŸå¥
- ä¸èƒ½æ·»åŠ ä»»ä½•æè¿°æ€§æ–‡å­—ï¼ˆå¦‚"æ˜¾ç¤ºä¸º"ã€"åº”è¯¥"ç­‰ï¼‰
- ä¸èƒ½æ”¹å†™ã€æ€»ç»“æˆ–æ„è¯‘åŸæ–‡

**å†æ¬¡å¼ºè°ƒï¼šåªè¾“å‡ºJSONï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—ï¼**"""

        try:
            # æ‰“å°å‘é€ç»™æ¨¡å‹çš„å®Œæ•´Promptï¼ˆç”¨äºè°ƒè¯•ï¼‰
            print(f"{Fore.CYAN}  [å®Œæ•´Prompt]")
            print(f"{Fore.WHITE}  {'='*60}")
            print(f"{Fore.WHITE}  {prompt}")
            print(f"{Fore.WHITE}  {'='*60}\n")

            response_text = self.llm_service.generate(prompt)

            # è§£æJSON
            response_text = response_text.strip()
            if response_text.startswith("```"):
                lines = response_text.split("\n")
                if lines[0].startswith("```"):
                    lines = lines[1:]
                if lines[-1].strip() == "```":
                    lines = lines[:-1]
                response_text = "\n".join(lines)

            start_idx = response_text.find("{")
            end_idx = response_text.rfind("}") + 1
            if start_idx != -1 and end_idx > start_idx:
                response_text = response_text[start_idx:end_idx]

            # æ¸…ç†æ§åˆ¶å­—ç¬¦ï¼ˆJSONä¸å…è®¸çš„æ§åˆ¶å­—ç¬¦ï¼‰
            # ç§»é™¤é™¤äº†æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ã€å›è½¦ç¬¦ä¹‹å¤–çš„æ§åˆ¶å­—ç¬¦
            import string
            control_chars = ''.join(chr(i) for i in range(32) if chr(i) not in '\n\r\t')
            for char in control_chars:
                response_text = response_text.replace(char, '')

            # ä¿®å¤JSONæ ¼å¼é—®é¢˜
            response_text = re.sub(r'"expected[^"]*":', '"expected_result":', response_text)
            response_text = re.sub(r'<\|[^|]+\|>', '', response_text)

            result = json.loads(response_text)
            test_cases = result.get("test_cases", [])
            # å…ˆè¿›è¡Œé™æ€æ ¡éªŒï¼ˆåŒ…æ‹¬æ ¼å¼ä¿®å¤ï¼‰
            warnings = self._run_static_validation(function_point, test_cases, doc_snippet)
            # ç„¶åè¿›è¡Œæ·±åº¦ä¿®å¤ï¼ˆæ›¿æ¢ä¸ºåŸæ–‡ï¼‰ï¼Œé¿å…é‡å¤ä¿®å¤
            repair_logs = self._repair_expected_results(function_point, test_cases, doc_snippet, skip_already_fixed=True)
            if repair_logs:
                warnings.extend(repair_logs)
            return test_cases, warnings, doc_snippet
        except json.JSONDecodeError as e:
            print(f"{Fore.YELLOW}  âš  åŠŸèƒ½ç‚¹ '{function_point}' JSONè§£æå¤±è´¥: {str(e)}")
            print(f"{Fore.YELLOW}  åŸå§‹å“åº”ï¼ˆå‰1000å­—ç¬¦ï¼‰:")
            print(f"{Fore.WHITE}  {response_text[:1000]}")
            return [], [f"JSONè§£æå¤±è´¥: {str(e)}"], doc_snippet
        except Exception as e:
            print(f"{Fore.YELLOW}  âš  åŠŸèƒ½ç‚¹ '{function_point}' ç”Ÿæˆå¤±è´¥: {str(e)}")
            return [], [f"ç”Ÿæˆå¤±è´¥: {str(e)}"], doc_snippet

    def _run_static_validation(self, function_point: str, test_cases: List[Dict], doc_snippet: str) -> List[str]:
        """å¯¹ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹è¿›è¡Œé™æ€æ ¡éªŒï¼Œè¿”å›å‘Šè­¦åˆ—è¡¨"""
        warnings: List[str] = []
        required_fields = {"case_name", "description", "preconditions", "steps", "expected_result", "priority"}
        traditional_punctuation = set("ã€Œã€ã€ã€ï¹ï¹‚ï¹ƒï¹„ï¹™ï¹šï¹›ï¹œï¹ï¹ï¹ƒï¹«ï¹¬ï¹­Â«Â»")

        if not isinstance(test_cases, list):
            warnings.append(f"[{function_point}] æµ‹è¯•ç”¨ä¾‹æ•°æ®æ ¼å¼å¼‚å¸¸ï¼Œéåˆ—è¡¨")
            return warnings

        for idx, case in enumerate(test_cases, 1):
            if not isinstance(case, dict):
                warnings.append(f"[{function_point}] ç¬¬{idx}æ¡ç”¨ä¾‹ä¸æ˜¯å­—å…¸ç±»å‹")
                continue

            missing = required_fields - set(case.keys())
            if missing:
                warnings.append(f"[{function_point}] ç¬¬{idx}æ¡ç”¨ä¾‹ç¼ºå°‘å­—æ®µ: {', '.join(sorted(missing))}")

            # åŸºç¡€å­—æ®µæ ¡éªŒ
            for field in required_fields - {"steps"}:
                value = case.get(field)
                if not isinstance(value, str) or not value.strip():
                    # å°è¯•è‡ªåŠ¨ä¿®å¤ç©ºçš„preconditionså­—æ®µï¼ˆé€šç”¨æ–¹æ³•ï¼‰
                    if field == "preconditions" and (not value or not value.strip()):
                        steps = case.get("steps", [])
                        inferred_preconditions = self._infer_preconditions_from_steps(steps)
                        case["preconditions"] = inferred_preconditions
                        warnings.append(f"[{function_point}] ç¬¬{idx}æ¡ç”¨ä¾‹å­—æ®µ'{field}'å·²è‡ªåŠ¨ä¿®å¤")
                    else:
                        warnings.append(f"[{function_point}] ç¬¬{idx}æ¡ç”¨ä¾‹å­—æ®µ'{field}'ä¸ºç©ºæˆ–ç±»å‹é”™è¯¯")

            steps = case.get("steps")
            if not isinstance(steps, list) or not steps or not all(isinstance(step, str) and step.strip() for step in steps):
                warnings.append(f"[{function_point}] ç¬¬{idx}æ¡ç”¨ä¾‹æ­¥éª¤åˆ—è¡¨ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")

            # æ£€æŸ¥å¹¶ä¿®å¤ç¹ä½“æ ‡ç‚¹
            combined_text = "".join(
                [str(case.get("case_name", "")), case.get("description", ""), case.get("expected_result", ""), "".join(steps or [])]
            )
            if any(char in traditional_punctuation for char in combined_text):
                # è‡ªåŠ¨ä¿®å¤æ‰€æœ‰å­—æ®µä¸­çš„ç¹ä½“æ ‡ç‚¹
                for field in ["case_name", "description", "expected_result"]:
                    if field in case and isinstance(case[field], str):
                        original = case[field]
                        fixed = self._fix_traditional_punctuation(original)
                        if fixed != original:
                            case[field] = fixed
                # ä¿®å¤stepsä¸­çš„ç¹ä½“æ ‡ç‚¹
                if steps and isinstance(steps, list):
                    for i, step in enumerate(steps):
                        if isinstance(step, str):
                            fixed_step = self._fix_traditional_punctuation(step)
                            if fixed_step != step:
                                steps[i] = fixed_step
                warnings.append(f"[{function_point}] ç¬¬{idx}æ¡ç”¨ä¾‹å·²è‡ªåŠ¨ä¿®å¤ç¹ä½“æ ‡ç‚¹")

            # å…³é”®è¯å¼•ç”¨æ£€æŸ¥ï¼ˆæ”¹è¿›ç‰ˆï¼šæ›´å®½æ¾çš„åŒ¹é…ï¼‰
            if doc_snippet:
                expected = case.get("expected_result", "")
                if expected:
                    # æ£€æŸ¥expected_resultæ˜¯å¦åŒ…å«å¤šä¸ªå¥å­è¿åœ¨ä¸€èµ·ï¼ˆå¯èƒ½æ˜¯æ ¼å¼é—®é¢˜ï¼‰
                    # å¦‚æœexpected_resultå¾ˆé•¿ä¸”æ²¡æœ‰æ ‡ç‚¹ç¬¦å·åˆ†éš”ï¼Œå¯èƒ½æ˜¯æ ¼å¼é—®é¢˜
                    if len(expected) > RepairConfig.FORMAT_FIX_MIN_LENGTH and not any(p in expected for p in ["ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", "\n"]):
                        # å°è¯•åœ¨doc_snippetä¸­æŸ¥æ‰¾åŒ…å«expected_resultå…³é”®è¯çš„å¥å­
                        expected_keywords = [kw for kw in RE_EXTRACT_KEYWORDS.split(expected) if len(kw) >= 2][:RepairConfig.FORMAT_FIX_KEYWORD_COUNT + 2]
                        if expected_keywords:
                            # åœ¨æ–‡æ¡£ç‰‡æ®µä¸­æŸ¥æ‰¾åŒ…å«è¿™äº›å…³é”®è¯çš„å¥å­ï¼ˆä½¿ç”¨éƒ¨åˆ†åŒ¹é…ï¼Œæ›´çµæ´»ï¼‰
                            snippet_lines = [line.strip() for line in doc_snippet.splitlines() if line.strip()]
                            matched_sentences = []
                            for line in snippet_lines:
                                # è‡³å°‘åŒ¹é…å‰Nä¸ªå…³é”®è¯ä¸­çš„å¤§éƒ¨åˆ†ï¼ˆæ›´çµæ´»ï¼‰
                                matched_count = sum(1 for kw in expected_keywords[:RepairConfig.FORMAT_FIX_KEYWORD_COUNT] if kw in line)
                                if matched_count >= max(1, RepairConfig.FORMAT_FIX_KEYWORD_COUNT - 1):  # å…è®¸1ä¸ªå…³é”®è¯ä¸åŒ¹é…
                                    matched_sentences.append(line)
                            if matched_sentences:
                                # é€‰æ‹©æœ€ä½³åŒ¹é…ï¼šä¼˜å…ˆé€‰æ‹©é•¿åº¦æœ€æ¥è¿‘çš„å¥å­
                                original_length = len(case["expected_result"])
                                best_match = min(matched_sentences, key=lambda s: abs(len(s) - original_length))
                                case["expected_result"] = best_match
                                # æ ‡è®°å·²ä¿®å¤ï¼Œé¿å…åœ¨_repair_expected_resultsä¸­é‡å¤å¤„ç†
                                case["_format_fixed"] = True
                                warnings.append(f"[{function_point}] ç¬¬{idx}æ¡é¢„æœŸç»“æœå·²è‡ªåŠ¨ä¿®å¤æ ¼å¼é—®é¢˜")

                    # å½’ä¸€åŒ–å¤„ç†ï¼šå»é™¤æ¢è¡Œã€å¤šä½™ç©ºæ ¼ã€ç¹ä½“æ ‡ç‚¹
                    expected_normalized = self._normalize_text(expected.replace("\n", " ").strip())
                    snippet_normalized = self._normalize_text(doc_snippet.replace("\n", " "))

                    # æ£€æŸ¥æ˜¯å¦åœ¨åŸæ–‡ä¸­ï¼ˆä½¿ç”¨å½’ä¸€åŒ–åçš„æ–‡æœ¬ï¼‰
                    if expected_normalized not in snippet_normalized:
                        # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šå»é™¤æ‰€æœ‰ç©ºæ ¼åæ˜¯å¦åŒ¹é…
                        expected_no_space = expected_normalized.replace(" ", "")
                        snippet_no_space = snippet_normalized.replace(" ", "")
                        if expected_no_space not in snippet_no_space:
                            # æœ€åå°è¯•ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®çŸ­è¯­ï¼ˆæ›´å®½æ¾çš„åŒ¹é…ï¼‰
                            key_phrases = [phrase for phrase in RE_SPLIT_PHRASES.split(expected) if len(phrase.strip()) >= RepairConfig.MIN_PHRASE_LENGTH]
                            found_any = False
                            for phrase in key_phrases[:RepairConfig.KEY_PHRASE_COUNT]:  # ä½¿ç”¨é…ç½®çš„æ•°é‡
                                phrase_normalized = self._normalize_text(phrase.strip())
                                # æ£€æŸ¥çŸ­è¯­æ˜¯å¦åœ¨æ–‡æ¡£ä¸­ï¼ˆå…è®¸éƒ¨åˆ†åŒ¹é…ï¼‰
                                if (phrase_normalized in snippet_normalized or
                                    phrase_normalized.replace(" ", "") in snippet_no_space or
                                    any(phrase_normalized in self._normalize_text(line) for line in doc_snippet.splitlines())):
                                    found_any = True
                                    break

                            # å¦‚æœä»ç„¶æ²¡æ‰¾åˆ°ï¼Œå°è¯•æå–å…³é”®è¯è¿›è¡Œæ›´å®½æ¾çš„åŒ¹é…
                            if not found_any:
                                # æå–expected_resultä¸­çš„å…³é”®è¯ï¼ˆè‡³å°‘2ä¸ªå­—ç¬¦ï¼‰
                                expected_keywords = [kw for kw in RE_EXTRACT_KEYWORDS.split(expected) if len(kw) >= 2]
                                if expected_keywords:
                                    # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€åŠçš„å…³é”®è¯åœ¨æ–‡æ¡£ä¸­
                                    matched_keywords = sum(1 for kw in expected_keywords[:5] if self._normalize_text(kw) in snippet_normalized)
                                    if matched_keywords >= max(1, len(expected_keywords[:5]) // 2):
                                        found_any = True

                            if not found_any:
                                warnings.append(f"[{function_point}] ç¬¬{idx}æ¡é¢„æœŸç»“æœæœªåœ¨åŸæ–‡ä¸­æ‰¾åˆ°ï¼Œéœ€äººå·¥ç¡®è®¤")

        return warnings

    def _repair_expected_results(self, function_point: str, test_cases: List[Dict], doc_snippet: str, skip_already_fixed: bool = False) -> List[str]:
        """
        å½“ expected_result ä¸åŸæ–‡ä¸å®Œå…¨åŒ¹é…æ—¶ï¼Œå°è¯•è‡ªåŠ¨çº æ­£ä¸ºæ–‡æ¡£åŸå¥

        Args:
            function_point: åŠŸèƒ½ç‚¹åç§°
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            doc_snippet: æ–‡æ¡£ç‰‡æ®µ
            skip_already_fixed: å¦‚æœä¸ºTrueï¼Œè·³è¿‡å·²ç»åœ¨é™æ€æ ¡éªŒä¸­ä¿®å¤è¿‡çš„ç”¨ä¾‹ï¼ˆé¿å…é‡å¤ä¿®å¤ï¼‰
        """
        repair_logs: List[str] = []

        if not doc_snippet or not isinstance(test_cases, list):
            return repair_logs

        # é¢„å¤„ç†æ–‡æ¡£ç‰‡æ®µ
        normalized_snippet = doc_snippet.replace("\n", "")
        snippet_lines = [line.strip() for line in doc_snippet.splitlines() if line.strip()]

        # è¿›ä¸€æ­¥æ‹†åˆ†æˆå¥å­ï¼ˆåŸºäºä¸­æ–‡æ ‡ç‚¹ï¼‰
        snippet_sentences: List[str] = []
        for line in snippet_lines:
            parts = [part.strip() for part in RE_SENTENCE_SPLIT.split(line) if part.strip()]
            snippet_sentences.extend(parts if parts else [line])

        # å»é‡ï¼Œä¿æŒé¡ºåºï¼Œå¹¶è¿‡æ»¤æ‰ä¸åˆé€‚çš„å€™é€‰
        seen = set()
        unique_candidates: List[str] = []
        for candidate in snippet_lines + snippet_sentences:
            # è¿‡æ»¤æ¡ä»¶ï¼ˆä½¿ç”¨é…ç½®å‚æ•°ï¼‰ï¼š
            # 1. é•¿åº¦è‡³å°‘Nä¸ªå­—ç¬¦ï¼ˆé¿å…æ ‡é¢˜è¡Œï¼‰
            # 2. ä¸æ˜¯çº¯æ•°å­—ç¼–å·è¡Œï¼ˆå¦‚"3. æ ‡é¢˜ï¼š"è¿™ç§æ ‡é¢˜ï¼‰
            # 3. åŒ…å«å®é™…å†…å®¹ï¼ˆä¸æ˜¯åªæœ‰æ ‡ç‚¹ç¬¦å·ï¼‰
            if (len(candidate) >= RepairConfig.MIN_SENTENCE_LENGTH and
                candidate not in seen and
                not RE_TITLE_LINE.match(candidate) and  # è¿‡æ»¤æ ‡é¢˜è¡Œ
                len(RE_EXTRACT_KEYWORDS.sub('', candidate)) >= RepairConfig.MIN_VALID_CHARS):  # è‡³å°‘Nä¸ªæœ‰æ•ˆå­—ç¬¦
                seen.add(candidate)
                unique_candidates.append(candidate)

        for idx, case in enumerate(test_cases, 1):
            if not isinstance(case, dict):
                continue
            expected = case.get("expected_result")
            if not isinstance(expected, str) or not expected.strip():
                continue

            # ä¿®å¤ç¹ä½“æ ‡ç‚¹ï¼šä½¿ç”¨å®Œæ•´çš„æ˜ å°„è¡¨ä¿®å¤æ‰€æœ‰ç¹ä½“æ ‡ç‚¹
            original_expected = expected
            expected = self._fix_traditional_punctuation(expected)
            if expected != original_expected:
                case["expected_result"] = expected
                repair_logs.append(
                    f"[{function_point}] ç¬¬{idx}æ¡é¢„æœŸç»“æœå·²ä¿®å¤ç¹ä½“æ ‡ç‚¹"
                )

            # è‹¥åŸé¢„æœŸç»“æœå·²åœ¨æ–‡æ¡£ä¸­ï¼ˆä½¿ç”¨å½’ä¸€åŒ–åçš„æ–‡æœ¬ï¼‰ï¼Œè·³è¿‡
            expected_normalized = self._normalize_text(expected.replace("\n", " "))
            snippet_normalized = self._normalize_text(normalized_snippet)
            if expected_normalized in snippet_normalized:
                continue

            # å¦‚æœskip_already_fixedä¸ºTrueï¼Œæ£€æŸ¥æ˜¯å¦å·²ç»åœ¨é™æ€æ ¡éªŒä¸­ä¿®å¤è¿‡æ ¼å¼é—®é¢˜
            if skip_already_fixed:
                # æ£€æŸ¥æ˜¯å¦å·²ç»æ ‡è®°ä¸ºæ ¼å¼ä¿®å¤è¿‡
                if case.get("_format_fixed", False):
                    # æ¸…ç†æ ‡è®°
                    case.pop("_format_fixed", None)
                    continue
                # æˆ–è€…æ£€æŸ¥å½’ä¸€åŒ–åçš„æ–‡æœ¬æ˜¯å¦å·²ç»åœ¨æ–‡æ¡£ä¸­ï¼ˆè¯´æ˜å¯èƒ½å·²ç»ä¿®å¤è¿‡ï¼‰
                if expected_normalized in snippet_normalized or expected_normalized.replace(" ", "") in snippet_normalized.replace(" ", ""):
                    continue

            # ç­–ç•¥1ï¼šå»é™¤ç©ºæ ¼åç²¾ç¡®åŒ¹é…ï¼ˆä½¿ç”¨å½’ä¸€åŒ–æ–‡æœ¬ï¼‰
            expected_normalized_for_match = self._normalize_text(expected.replace(" ", ""))
            matched_line: Optional[str] = None

            # é¢„å…ˆæå–å…³é”®è¯ï¼ˆä¾›åç»­ç­–ç•¥ä½¿ç”¨ï¼‰
            expected_normalized = self._normalize_text(expected.replace("\n", " "))
            expected_keywords = [kw for kw in RE_EXTRACT_KEYWORDS.split(expected) if len(kw) >= 2]

            for candidate in unique_candidates:
                candidate_normalized = self._normalize_text(candidate.replace(" ", ""))
                if expected_normalized_for_match == candidate_normalized:
                    matched_line = candidate
                    break

            # ç­–ç•¥2ï¼šç›¸ä¼¼åº¦åŒ¹é…ï¼ˆæé«˜é˜ˆå€¼ï¼Œä¼˜å…ˆåŒ¹é…å®Œæ•´å¥å­ï¼‰
            if not matched_line:
                best_ratio = 0.0
                best_candidate = None

                for candidate in unique_candidates:
                    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨å½’ä¸€åŒ–åçš„æ–‡æœ¬ï¼‰
                    candidate_normalized = self._normalize_text(candidate)
                    ratio = difflib.SequenceMatcher(None, expected_normalized, candidate_normalized).ratio()

                    # å¦‚æœå€™é€‰å¥å­åŒ…å«é¢„æœŸç»“æœä¸­çš„å…³é”®è¯ï¼Œç»™äºˆåŠ åˆ†ï¼ˆä½¿ç”¨é…ç½®å‚æ•°ï¼‰
                    keyword_bonus = 0.0
                    if expected_keywords:
                        matched_keywords = sum(1 for kw in expected_keywords if kw in candidate_normalized)
                        keyword_bonus = (matched_keywords / len(expected_keywords)) * RepairConfig.KEYWORD_BONUS

                    # é•¿åº¦ç›¸ä¼¼åº¦åŠ åˆ†ï¼ˆé•¿åº¦è¶Šæ¥è¿‘ï¼ŒåŠ åˆ†è¶Šå¤šï¼‰
                    length_ratio = min(len(candidate), len(expected)) / max(len(candidate), len(expected)) if max(len(candidate), len(expected)) > 0 else 0
                    length_bonus = length_ratio * 0.05  # æœ€å¤šåŠ 0.05

                    # å¦‚æœå€™é€‰å¥å­åŒ…å«expected_resultä¸­çš„æ ¸å¿ƒå…³é”®è¯ï¼ˆè‡³å°‘2ä¸ªï¼‰ï¼Œé¢å¤–åŠ åˆ†
                    core_keyword_bonus = 0.0
                    if expected_keywords and len(expected_keywords) >= 2:
                        # æå–å‰3ä¸ªæœ€é‡è¦çš„å…³é”®è¯
                        core_keywords = expected_keywords[:3]
                        matched_core = sum(1 for kw in core_keywords if kw in candidate_normalized)
                        if matched_core >= 2:  # è‡³å°‘åŒ¹é…2ä¸ªæ ¸å¿ƒå…³é”®è¯
                            core_keyword_bonus = 0.1  # é¢å¤–åŠ 0.1

                    final_ratio = ratio + keyword_bonus + length_bonus + core_keyword_bonus

                    if final_ratio > best_ratio:
                        best_ratio = final_ratio
                        best_candidate = candidate

                # ä½¿ç”¨é…ç½®çš„ç›¸ä¼¼åº¦é˜ˆå€¼
                if best_candidate and best_ratio >= RepairConfig.SIMILARITY_THRESHOLD:
                    matched_line = best_candidate

            # å¦‚æœæ‰¾åˆ°åŒ¹é…ï¼Œæ›¿æ¢ expected_resultï¼ˆé¿å…é‡å¤è®°å½•ï¼‰
            if matched_line and matched_line != expected:
                # æ£€æŸ¥æ˜¯å¦å·²ç»è®°å½•è¿‡ä¿®å¤æ—¥å¿—ï¼ˆé¿å…é‡å¤ï¼‰
                already_logged = any(
                    f"ç¬¬{idx}æ¡" in log and ("å·²è‡ªåŠ¨æ›¿æ¢" in log or "å·²ä¿®å¤æ ¼å¼" in log)
                    for log in repair_logs
                )
                if not already_logged:
                    case["expected_result"] = matched_line
                    # å¦‚æœæ›¿æ¢åçš„æ–‡æœ¬è¾ƒé•¿ï¼Œæˆªæ–­æ˜¾ç¤ºï¼ˆé¿å…æ—¥å¿—è¿‡é•¿ï¼‰
                    display_text = matched_line if len(matched_line) <= 100 else matched_line[:97] + "..."
                    repair_logs.append(
                        f"[{function_point}] ç¬¬{idx}æ¡é¢„æœŸç»“æœå·²è‡ªåŠ¨æ›¿æ¢ä¸ºåŸæ–‡: {display_text}"
                    )

            # ç­–ç•¥3ï¼šå¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…ï¼ˆæ›´å®½æ¾çš„ç­–ç•¥ï¼‰
            # æ£€æŸ¥expected_resultæ˜¯å¦åŒ…å«æ–‡æ¡£ä¸­çš„å…³é”®çŸ­è¯­
            if not matched_line and expected_keywords:
                # å°è¯•åœ¨æ–‡æ¡£ä¸­æŸ¥æ‰¾åŒ…å«å¤§éƒ¨åˆ†å…³é”®è¯çš„å¥å­
                for candidate in unique_candidates:
                    candidate_normalized = self._normalize_text(candidate)
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«è‡³å°‘ä¸€åŠçš„å…³é”®è¯
                    matched_count = sum(1 for kw in expected_keywords if kw in candidate_normalized)
                    if matched_count >= max(2, len(expected_keywords) // 2):  # è‡³å°‘åŒ¹é…2ä¸ªæˆ–ä¸€åŠçš„å…³é”®è¯
                        # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šå€™é€‰å¥å­æ˜¯å¦åŒ…å«expected_resultä¸­çš„æ ¸å¿ƒæ¦‚å¿µ
                        # æå–expected_resultä¸­çš„æ ¸å¿ƒè¯æ±‡ï¼ˆå»é™¤å¸¸è§è¯ï¼‰
                        common_words = {"çš„", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "æˆ–", "åŠ", "ä¸º", "ä¼š", "å¯ä»¥", "èƒ½å¤Ÿ"}
                        core_words = [kw for kw in expected_keywords if kw not in common_words and len(kw) >= 2]
                        if core_words:
                            matched_core = sum(1 for word in core_words if word in candidate_normalized)
                            if matched_core >= len(core_words) * 0.5:  # è‡³å°‘åŒ¹é…ä¸€åŠçš„æ ¸å¿ƒè¯
                                matched_line = candidate
                                break

                # å¦‚æœæ‰¾åˆ°éƒ¨åˆ†åŒ¹é…ï¼Œæ›¿æ¢
                if matched_line and matched_line != expected:
                    already_logged = any(
                        f"ç¬¬{idx}æ¡" in log and ("å·²è‡ªåŠ¨æ›¿æ¢" in log or "å·²ä¿®å¤æ ¼å¼" in log)
                        for log in repair_logs
                    )
                    if not already_logged:
                        case["expected_result"] = matched_line
                        display_text = matched_line if len(matched_line) <= 100 else matched_line[:97] + "..."
                        repair_logs.append(
                            f"[{function_point}] ç¬¬{idx}æ¡é¢„æœŸç»“æœå·²è‡ªåŠ¨æ›¿æ¢ä¸ºåŸæ–‡ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰: {display_text}"
                        )

        return repair_logs

    def _generate_single_point_wrapper(self, requirement_doc: str, fp_data: Dict, idx: int, total: int) -> Tuple[str, List[Dict], List[str], str]:
        """
        åŒ…è£…æ–¹æ³•ï¼Œç”¨äºå¹¶å‘å¤„ç†å•ä¸ªåŠŸèƒ½ç‚¹

        Args:
            requirement_doc: éœ€æ±‚æ–‡æ¡£å†…å®¹
            fp_data: åŠŸèƒ½ç‚¹æ•°æ®
            idx: å½“å‰ç´¢å¼•
            total: æ€»æ•°é‡

        Returns:
            (function_point_name, test_cases, warnings, doc_snippet)
        """
        function_point_name = fp_data.get("name", "")
        try:
            print(f"{Fore.CYAN}[{idx}/{total}] æ­£åœ¨ä¸ºåŠŸèƒ½ç‚¹ç”Ÿæˆç”¨ä¾‹: {function_point_name}")
            test_cases, warnings, doc_snippet = self.generate_test_cases_for_point(
                requirement_doc, function_point_name, fp_data
            )
            if test_cases:
                print(f"{Fore.GREEN}[{idx}/{total}] âœ“ {function_point_name}: ç”Ÿæˆ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
            else:
                print(f"{Fore.YELLOW}[{idx}/{total}] âš  {function_point_name}: æœªç”Ÿæˆæµ‹è¯•ç”¨ä¾‹")
            if warnings:
                for warn in warnings:
                    print(f"{Fore.YELLOW}    â€¢ {function_point_name}: {warn}")
            return function_point_name, test_cases, warnings, doc_snippet
        except Exception as e:
            print(f"{Fore.RED}[{idx}/{total}] âœ— {function_point_name}: å¤„ç†å¤±è´¥ - {str(e)}")
            return function_point_name, [], [f"å¤„ç†å¤±è´¥: {str(e)}"], ""

    def generate_test_cases(self, requirement_doc: str, limit: Optional[int] = None, max_workers: int = 4) -> Dict:
        """
        ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ - ä¸ºæ¯ä¸ªåŠŸèƒ½ç‚¹åˆ†åˆ«ç”Ÿæˆ

        Args:
            requirement_doc: éœ€æ±‚æ–‡æ¡£å†…å®¹

        Returns:
            åŒ…å«æµ‹è¯•ç”¨ä¾‹çš„å­—å…¸
        """
        print(f"{Fore.YELLOW}æ­£åœ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹...")

        # ç¼“å­˜æ–‡æ¡£ï¼Œé¿å…é‡å¤æ‹†åˆ†
        self._prepare_requirement_cache(requirement_doc)

        # ç¬¬ä¸€æ­¥ï¼šæå–åŠŸèƒ½ç‚¹ï¼ˆå¸¦å®šä½çº¿ç´¢ï¼‰
        function_points_data = self.extract_function_points(requirement_doc)

        if not function_points_data:
            print(f"{Fore.YELLOW}âš  æœªèƒ½æå–åŠŸèƒ½ç‚¹ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ç”Ÿæˆ...")
            # å¦‚æœæ²¡æœ‰æå–åˆ°åŠŸèƒ½ç‚¹ï¼Œä½¿ç”¨åŸæ¥çš„æ–¹å¼
            prompt = self.build_prompt(requirement_doc)
            # æ‰“å°å‘é€ç»™æ¨¡å‹çš„å®Œæ•´Promptï¼ˆç”¨äºè°ƒè¯•ï¼‰
            print(f"{Fore.CYAN}[å®Œæ•´Prompt]")
            print(f"{Fore.WHITE}{'='*60}")
            print(f"{Fore.WHITE}{prompt}")
            print(f"{Fore.WHITE}{'='*60}\n")
            response_text = self.llm_service.generate(prompt)
            # ç»§ç»­åŸæœ‰çš„è§£æé€»è¾‘
            return self._parse_response(response_text)

        # ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªåŠŸèƒ½ç‚¹åˆ†åˆ«ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        all_test_cases = []
        per_point_cases: Dict[str, Dict[str, Any]] = {}
        total_points = len(function_points_data)

        # æµ‹è¯•æ¨¡å¼ï¼šéšæœºé€‰æ‹©Nä¸ªåŠŸèƒ½ç‚¹ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        effective_limit = limit if limit is not None else TEST_FUNCTION_POINTS_LIMIT
        if effective_limit and effective_limit > 0:
            if len(function_points_data) > effective_limit:
                # éšæœºé€‰æ‹©Nä¸ªåŠŸèƒ½ç‚¹
                selected_points = random.sample(function_points_data, effective_limit)
                function_points_data = selected_points
                print(f"{Fore.YELLOW}âš  é™åˆ¶åŠŸèƒ½ç‚¹æ•°é‡ï¼šéšæœºé€‰æ‹© {effective_limit} ä¸ªåŠŸèƒ½ç‚¹è¿›è¡Œç”Ÿæˆ\n")
            else:
                print(f"{Fore.YELLOW}âš  åŠŸèƒ½ç‚¹æ€»æ•°({len(function_points_data)})å°‘äºé™åˆ¶æ•°({effective_limit})ï¼Œå¤„ç†æ‰€æœ‰åŠŸèƒ½ç‚¹\n")

        actual_points = len(function_points_data)
        print(f"\n{Fore.CYAN}ç¬¬äºŒæ­¥ï¼šä¸º {actual_points} ä¸ªåŠŸèƒ½ç‚¹åˆ†åˆ«ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼ˆå¹¶å‘å¤„ç†ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼‰...\n")

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†åŠŸèƒ½ç‚¹
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_fp = {
                executor.submit(self._generate_single_point_wrapper, requirement_doc, fp_data, idx, actual_points): fp_data
                for idx, fp_data in enumerate(function_points_data, 1)
            }

            # æ”¶é›†ç»“æœï¼ˆæŒ‰å®Œæˆé¡ºåºï¼‰
            completed = 0
            for future in as_completed(future_to_fp):
                completed += 1
                try:
                    function_point_name, test_cases, warnings, doc_snippet = future.result()
                    per_point_cases[function_point_name] = {
                        "test_cases": test_cases,
                        "warnings": warnings,
                        "source": doc_snippet
                    }
                    if test_cases:
                        all_test_cases.extend(test_cases)
                    print(f"{Fore.CYAN}  [{completed}/{actual_points}] å·²å®Œæˆ: {function_point_name}\n")
                except Exception as e:
                    fp_data = future_to_fp[future]
                    function_point_name = fp_data.get("name", "")
                    print(f"{Fore.RED}  [{completed}/{actual_points}] âœ— {function_point_name}: å‘ç”Ÿå¼‚å¸¸ - {str(e)}\n")
                    per_point_cases[function_point_name] = {
                        "test_cases": [],
                        "warnings": [f"å¤„ç†å¼‚å¸¸: {str(e)}"],
                        "source": ""
                    }

        result = {
            "test_cases": all_test_cases,
            "by_function_point": per_point_cases,
            "meta": {
                "total_function_points": total_points,
                "processed_function_points": actual_points,
                "limit": effective_limit or 0,
                "total_warnings": sum(len(data.get("warnings", []) or []) for data in per_point_cases.values())
            }
        }
        print(f"{Fore.GREEN}âœ“ æ€»å…±ç”Ÿæˆ {len(all_test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        return result

    def _parse_response(self, response_text: str) -> Dict:
        """
        è§£ææ¨¡å‹å“åº”ï¼ˆåŸæœ‰é€»è¾‘ï¼‰

        Args:
            response_text: æ¨¡å‹å“åº”æ–‡æœ¬

        Returns:
            åŒ…å«æµ‹è¯•ç”¨ä¾‹çš„å­—å…¸
        """

        # è§£æJSONå“åº”
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†ï¼ˆå»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°ï¼‰
            response_text = response_text.strip()
            if response_text.startswith("```"):
                # ç§»é™¤markdownä»£ç å—æ ‡è®°
                lines = response_text.split("\n")
                if lines[0].startswith("```"):
                    lines = lines[1:]
                if lines[-1].strip() == "```":
                    lines = lines[:-1]
                response_text = "\n".join(lines)

            # å°è¯•æ‰¾åˆ°JSONå¯¹è±¡
            start_idx = response_text.find("{")
            end_idx = response_text.rfind("}") + 1
            if start_idx != -1 and end_idx > start_idx:
                response_text = response_text[start_idx:end_idx]

            # æ¸…ç†æ§åˆ¶å­—ç¬¦ï¼ˆJSONä¸å…è®¸çš„æ§åˆ¶å­—ç¬¦ï¼‰
            import string
            control_chars = ''.join(chr(i) for i in range(32) if chr(i) not in '\n\r\t')
            for char in control_chars:
                response_text = response_text.replace(char, '')

            # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
            # 1. ä¿®å¤è¢«æˆªæ–­çš„å­—æ®µåï¼ˆå¦‚ expected<|redacted...|> åº”è¯¥ä¿®å¤ä¸º expected_resultï¼‰
            response_text = re.sub(r'"expected[^"]*":', '"expected_result":', response_text)
            # 2. ä¿®å¤å…¶ä»–å¯èƒ½çš„æˆªæ–­æ ‡è®°
            response_text = re.sub(r'<\|[^|]+\|>', '', response_text)
            # 3. ç§»é™¤æœ«å°¾ä¸å®Œæ•´çš„å­—æ®µ
            lines = response_text.split('\n')
            cleaned_lines = []
            for line in lines:
                # å¦‚æœè¡ŒåŒ…å«ä¸å®Œæ•´çš„å­—æ®µï¼ˆæœ‰å¼•å·å¼€å§‹ä½†æ²¡æœ‰å†’å·ï¼‰ï¼Œè·³è¿‡
                if '":' in line or line.strip() in ['{', '}', '[', ']', ','] or not line.strip():
                    cleaned_lines.append(line)
                elif line.strip().startswith('"') and ':' not in line:
                    # å¯èƒ½æ˜¯è¢«æˆªæ–­çš„å­—æ®µï¼Œå°è¯•ä¿®å¤æˆ–è·³è¿‡
                    if 'expected' in line.lower():
                        cleaned_lines.append('            "expected_result": "",')
                    # å¦åˆ™è·³è¿‡è¿™è¡Œ
                else:
                    cleaned_lines.append(line)
            response_text = '\n'.join(cleaned_lines)

            # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
            # å¦‚æœJSONä¸å®Œæ•´ï¼Œå°è¯•ä¿®å¤
            brace_count = response_text.count('{') - response_text.count('}')
            if brace_count > 0:
                # ç¼ºå°‘å³æ‹¬å·ï¼Œè¡¥å……
                response_text += '\n' + '}' * brace_count
            elif brace_count < 0:
                # å¤šä½™çš„å³æ‹¬å·ï¼Œç§»é™¤æœ€åä¸€ä¸ª
                for _ in range(-brace_count):
                    last_brace = response_text.rfind('}')
                    if last_brace != -1:
                        response_text = response_text[:last_brace] + response_text[last_brace+1:]

            result = json.loads(response_text)

            # éªŒè¯ç»“æœæ ¼å¼
            if "test_cases" not in result:
                raise ValueError("å“åº”ä¸­ç¼ºå°‘'test_cases'å­—æ®µ")

            print(f"{Fore.GREEN}âœ“ æˆåŠŸç”Ÿæˆ {len(result.get('test_cases', []))} ä¸ªæµ‹è¯•ç”¨ä¾‹")
            return result

        except json.JSONDecodeError as e:
            print(f"{Fore.YELLOW}âš  JSONè§£æé‡åˆ°é—®é¢˜ï¼Œå°è¯•ä¿®å¤...")
            # å°è¯•æ›´æ¿€è¿›çš„ä¿®å¤ï¼šæå–æ‰€æœ‰å®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹
            try:
                # ä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
                matches = RE_JSON_CASE.findall(response_text)
                if matches:
                    fixed_cases = []
                    for match in matches:
                        try:
                            case = json.loads(match)
                            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
                            if 'case_name' in case:
                                if 'expected_result' not in case:
                                    case['expected_result'] = ''
                                fixed_cases.append(case)
                        except (json.JSONDecodeError, KeyError, ValueError, TypeError):
                            continue
                    if fixed_cases:
                        result = {"test_cases": fixed_cases}
                        print(f"{Fore.GREEN}âœ“ æˆåŠŸä¿®å¤å¹¶ç”Ÿæˆ {len(fixed_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
                        return result
            except (json.JSONDecodeError, re.error, ValueError):
                pass

            print(f"{Fore.RED}âœ— JSONè§£æå¤±è´¥: {str(e)}")
            print(f"{Fore.YELLOW}åŸå§‹å“åº”å†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰:")
            print(response_text[:500])
            raise ValueError(f"æ— æ³•è§£æLLMè¿”å›çš„JSONæ ¼å¼: {str(e)}")
        except Exception as e:
            print(f"{Fore.RED}âœ— ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
            raise


def read_requirement_file(file_path: str) -> str:
    """è¯»å–éœ€æ±‚æ–‡æ¡£æ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    except Exception as e:
        raise IOError(f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")


def _clean_test_cases(test_cases: List[Dict]) -> List[Dict]:
    """æ¸…ç†æµ‹è¯•ç”¨ä¾‹ä¸­çš„ä¸´æ—¶æ ‡è®°å­—æ®µï¼ˆé€šç”¨æ–¹æ³•ï¼‰"""
    cleaned = []
    for case in test_cases:
        if isinstance(case, dict):
            # åˆ›å»ºå‰¯æœ¬ï¼Œç§»é™¤ä¸´æ—¶æ ‡è®°å­—æ®µ
            cleaned_case = {k: v for k, v in case.items() if not k.startswith("_")}
            cleaned.append(cleaned_case)
        else:
            cleaned.append(case)
    return cleaned

def save_result(result: Dict, output_path: str, split_output: bool = False, output_dir: Optional[str] = None):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    try:
        # æ¸…ç†æµ‹è¯•ç”¨ä¾‹ä¸­çš„ä¸´æ—¶æ ‡è®°
        if "test_cases" in result:
            result["test_cases"] = _clean_test_cases(result["test_cases"])
        if "by_function_point" in result:
            for fp_data in result["by_function_point"].values():
                if "test_cases" in fp_data:
                    fp_data["test_cases"] = _clean_test_cases(fp_data["test_cases"])

        output_path_obj = Path(output_path)
        output_path_obj.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path_obj, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"{Fore.GREEN}âœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path_obj}")
    except Exception as e:
        print(f"{Fore.RED}âœ— ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise

    if split_output:
        by_function_point = result.get("by_function_point", {})
        if not by_function_point:
            print(f"{Fore.YELLOW}âš  æ— åŠŸèƒ½ç‚¹æ˜ç»†å¯æ‹†åˆ†ï¼Œè·³è¿‡æŒ‰åŠŸèƒ½ç‚¹ä¿å­˜")
            return

        target_dir = Path(output_dir) if output_dir else output_path_obj.parent / DEFAULT_OUTPUT_DIR
        target_dir.mkdir(parents=True, exist_ok=True)

        for function_point, data in by_function_point.items():
            slug = slugify_function_point(function_point)
            per_point_path = target_dir / f"{slug}.json"
            payload = {
                "function_point": function_point,
                "test_cases": _clean_test_cases(data.get("test_cases", [])),
                "warnings": data.get("warnings", []),
                "source": data.get("source", "")
            }
            with open(per_point_path, 'w', encoding='utf-8') as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
        print(f"{Fore.GREEN}âœ“ å·²æŒ‰åŠŸèƒ½ç‚¹æ‹†åˆ†ä¿å­˜åˆ°ç›®å½•: {target_dir}")


def print_result(result: Dict):
    """åœ¨æ§åˆ¶å°æ‰“å°ç»“æœ"""
    test_cases = result.get("test_cases", [])
    by_function_point = result.get("by_function_point", {})

    print(f"\n{Fore.CYAN}{'='*60}")
    print(f"{Fore.CYAN}ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹ ({len(test_cases)} ä¸ª)")
    print(f"{Fore.CYAN}{'='*60}\n")

    # æ‰“å°æ•´ä½“å‘Šè­¦
    aggregated_warnings = []
    for function_point, data in by_function_point.items():
        for warning in data.get("warnings", []) or []:
            aggregated_warnings.append((function_point, warning))

    if aggregated_warnings:
        print(f"{Fore.YELLOW}è­¦å‘Šæ±‡æ€»ï¼š")
        for function_point, warning in aggregated_warnings:
            print(f"  {Fore.YELLOW}- [{function_point}] {warning}")
        print()

    for i, case in enumerate(test_cases, 1):
        print(f"{Fore.YELLOW}[ç”¨ä¾‹ {i}] {case.get('case_name', 'N/A')}")
        print(f"{Fore.WHITE}  æè¿°: {case.get('description', 'N/A')}")
        print(f"{Fore.WHITE}  å‰ç½®æ¡ä»¶: {case.get('preconditions', 'N/A')}")
        print(f"{Fore.WHITE}  ä¼˜å…ˆçº§: {case.get('priority', 'N/A')}")
        print(f"{Fore.WHITE}  æµ‹è¯•æ­¥éª¤:")
        for step in case.get('steps', []):
            print(f"{Fore.WHITE}    - {step}")
        print(f"{Fore.WHITE}  é¢„æœŸç»“æœ: {case.get('expected_result', 'N/A')}")
        print()


def list_available_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é¢„è®¾"""
    print(f"\n{Fore.CYAN}{'='*60}")
    print(f"{Fore.CYAN}å¯ç”¨çš„æ¨¡å‹é¢„è®¾")
    print(f"{Fore.CYAN}{'='*60}\n")

    recommended = []
    others = []

    for key, config in MODEL_PRESETS.items():
        if config.get("recommended", False):
            recommended.append((key, config))
        else:
            others.append((key, config))

    if recommended:
        print(f"{Fore.GREEN}æ¨èæ¨¡å‹ï¼š")
        for key, config in recommended:
            print(f"  {Fore.YELLOW}{key:20} {Fore.WHITE}- {config['description']}")
        print()

    if others:
        print(f"{Fore.CYAN}å…¶ä»–æ¨¡å‹ï¼š")
        for key, config in others:
            print(f"  {Fore.YELLOW}{key:20} {Fore.WHITE}- {config['description']}")
        print()

    print(f"{Fore.CYAN}ä½¿ç”¨æ–¹æ³•ï¼š")
    print(f"  {Fore.WHITE}python generate_test_cases.py --model qwen2.5")
    print(f"  {Fore.WHITE}python generate_test_cases.py --model deepseek-coder")
    print(f"  {Fore.WHITE}python generate_test_cases.py --model <æ¨¡å‹åç§°>")
    print()


def main():
    """ä¸»å‡½æ•° - ç®€åŒ–ç‰ˆï¼šç›´æ¥è¯»å–éœ€æ±‚æ–‡ä»¶å¹¶ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="AIæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  python generate_test_cases.py
  python generate_test_cases.py --model qwen2.5
  python generate_test_cases.py --model deepseek-coder --requirement my_requirement.txt
  python generate_test_cases.py --list-models
  python generate_test_cases.py --debug-extraction  # è°ƒè¯•åŠŸèƒ½ç‚¹æå–å’ŒåŸæ–‡åŒ¹é…
        """
    )
    parser.add_argument(
        "--model", "-m",
        type=str,
        help="è¦ä½¿ç”¨çš„æ¨¡å‹é¢„è®¾åç§°ï¼ˆä½¿ç”¨ --list-models æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼‰"
    )
    parser.add_argument(
        "--model-name",
        type=str,
        help="ç›´æ¥æŒ‡å®šæ¨¡å‹åç§°ï¼ˆå¦‚ï¼šqwen2.5:7bï¼‰"
    )
    parser.add_argument(
        "--base-url",
        type=str,
        help=f"LLMæœåŠ¡åœ°å€ï¼ˆé»˜è®¤ï¼š{DEFAULT_CONFIG['llm_base_url']}ï¼‰"
    )
    parser.add_argument(
        "--temperature", "-t",
        type=float,
        help=f"æ¸©åº¦å‚æ•°ï¼ˆé»˜è®¤ï¼š{DEFAULT_CONFIG['temperature']}ï¼‰"
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        help=f"æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤ï¼š{DEFAULT_CONFIG['max_tokens']}ï¼‰"
    )
    parser.add_argument(
        "--requirement", "-r",
        type=str,
        help=f"éœ€æ±‚æ–‡æ¡£æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼š{DEFAULT_REQUIREMENT_FILE}ï¼‰"
    )
    parser.add_argument(
        "--list-models", "-l",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é¢„è®¾"
    )
    parser.add_argument(
        "--test-limit",
        type=int,
        help="é™åˆ¶å‚ä¸ç”Ÿæˆçš„åŠŸèƒ½ç‚¹æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨ä»£ç ä¸­çš„ TEST_FUNCTION_POINTS_LIMIT å€¼ï¼‰"
    )
    parser.add_argument(
        "--split-output",
        action="store_true",
        help="æŒ‰åŠŸèƒ½ç‚¹æ‹†åˆ†ä¿å­˜è¾“å‡ºæ–‡ä»¶"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        help=f"æ‹†åˆ†è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šå½“å‰ç›®å½•/{DEFAULT_OUTPUT_DIR}ï¼‰"
    )
    parser.add_argument(
        "--debug-extraction",
        action="store_true",
        help="åªæ‰§è¡ŒåŠŸèƒ½ç‚¹æå–å’ŒåŸæ–‡åŒ¹é…è°ƒè¯•ï¼Œä¸ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=4,
        help="å¹¶å‘å¤„ç†åŠŸèƒ½ç‚¹çš„æœ€å¤§çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 4ï¼Œå»ºè®®èŒƒå›´: 2-8ï¼‰"
    )

    args = parser.parse_args()

    # å¦‚æœåªæ˜¯åˆ—å‡ºæ¨¡å‹ï¼Œåˆ™æ˜¾ç¤ºåé€€å‡º
    if args.list_models:
        list_available_models()
        sys.exit(0)

    try:
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
        script_dir = Path(__file__).parent
        requirement_file = script_dir / (args.requirement or DEFAULT_REQUIREMENT_FILE)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not requirement_file.exists():
            print(f"{Fore.RED}âœ— é”™è¯¯: éœ€æ±‚æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {requirement_file}")
            print(f"{Fore.YELLOW}æç¤º: è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨")
            sys.exit(1)

        # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹ï¼ˆä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œå‚æ•° > ä»£ç é…ç½® > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼ï¼‰
        model_name = None
        if args.model_name:
            # å‘½ä»¤è¡Œç›´æ¥æŒ‡å®šæ¨¡å‹åç§°ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            model_name = args.model_name
            print(f"{Fore.CYAN}ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æ¨¡å‹: {model_name}")
        elif args.model:
            # å‘½ä»¤è¡Œä½¿ç”¨é¢„è®¾æ¨¡å‹
            if args.model not in MODEL_PRESETS:
                print(f"{Fore.RED}âœ— é”™è¯¯: æœªçŸ¥çš„æ¨¡å‹é¢„è®¾: {args.model}")
                print(f"{Fore.YELLOW}æç¤º: ä½¿ç”¨ --list-models æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹")
                sys.exit(1)
            model_name = MODEL_PRESETS[args.model]["model"]
            print(f"{Fore.CYAN}ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å‹é¢„è®¾: {args.model} -> {model_name}")
        elif SELECTED_MODEL:
            # ä½¿ç”¨ä»£ç ä¸­é…ç½®çš„æ¨¡å‹ï¼ˆåœ¨æ–‡ä»¶é¡¶éƒ¨ä¿®æ”¹ SELECTED_MODEL å˜é‡ï¼‰
            if SELECTED_MODEL in MODEL_PRESETS:
                model_name = MODEL_PRESETS[SELECTED_MODEL]["model"]
                print(f"{Fore.CYAN}ä½¿ç”¨ä»£ç é…ç½®çš„æ¨¡å‹é¢„è®¾: {SELECTED_MODEL} -> {model_name}")
            else:
                # ç›´æ¥ä½¿ç”¨æ¨¡å‹åç§°
                model_name = SELECTED_MODEL
                print(f"{Fore.CYAN}ä½¿ç”¨ä»£ç é…ç½®çš„æ¨¡å‹: {model_name}")
        else:
            # ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤æ¨¡å‹
            model_name = os.getenv("LLM_MODEL", DEFAULT_CONFIG["default_model"])
            print(f"{Fore.CYAN}ä½¿ç”¨é»˜è®¤æ¨¡å‹: {model_name}")

        # è¯»å–éœ€æ±‚æ–‡æ¡£
        print(f"{Fore.CYAN}{'='*60}")
        print(f"{Fore.CYAN}AIæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå·¥å…·")
        print(f"{Fore.CYAN}{'='*60}\n")
        print(f"{Fore.CYAN}è¯»å–éœ€æ±‚æ–‡æ¡£: {requirement_file}")
        requirement_doc = read_requirement_file(str(requirement_file))

        if not requirement_doc.strip():
            print(f"{Fore.RED}âœ— é”™è¯¯: éœ€æ±‚æ–‡æ¡£å†…å®¹ä¸ºç©º")
            sys.exit(1)

        print(f"{Fore.GREEN}âœ“ éœ€æ±‚æ–‡æ¡£é•¿åº¦: {len(requirement_doc)} å­—ç¬¦\n")

        # åˆå§‹åŒ–æœåŠ¡
        base_url = args.base_url or os.getenv("LLM_BASE_URL", DEFAULT_CONFIG["llm_base_url"])
        temperature = args.temperature or float(os.getenv("LLM_TEMPERATURE", DEFAULT_CONFIG["temperature"]))
        max_tokens = args.max_tokens or int(os.getenv("LLM_MAX_TOKENS", DEFAULT_CONFIG["max_tokens"]))

        # åŠŸèƒ½ç‚¹æ•°é‡é™åˆ¶ï¼ˆå‘½ä»¤è¡Œ > ç¯å¢ƒå˜é‡ > é»˜è®¤ï¼‰
        if args.test_limit is not None:
            test_limit = max(args.test_limit, 0)
        else:
            env_limit = os.getenv("LLM_TEST_LIMIT")
            if env_limit and env_limit.isdigit():
                test_limit = int(env_limit)
            else:
                test_limit = None

        llm_service = LLMService(
            base_url=base_url,
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens
        )

        generator = TestCaseGenerator(llm_service)

        # è°ƒè¯•æ¨¡å¼ï¼šåªæ‰§è¡ŒåŠŸèƒ½ç‚¹æå–å’ŒåŸæ–‡åŒ¹é…
        if args.debug_extraction:
            print(f"\n{Fore.CYAN}{'='*60}")
            print(f"{Fore.CYAN}è°ƒè¯•æ¨¡å¼ï¼šåŠŸèƒ½ç‚¹æå–å’ŒåŸæ–‡åŒ¹é…")
            print(f"{Fore.CYAN}{'='*60}\n")

            # æå–åŠŸèƒ½ç‚¹ï¼ˆå¸¦å®šä½çº¿ç´¢ï¼‰
            function_points_data = generator.extract_function_points(requirement_doc)

            if not function_points_data:
                print(f"{Fore.RED}âœ— æœªèƒ½æå–åˆ°åŠŸèƒ½ç‚¹")
                sys.exit(1)

            # æ˜¾ç¤ºåŠŸèƒ½ç‚¹å’Œå®šä½çº¿ç´¢
            print(f"{Fore.GREEN}âœ“ æå–åˆ°çš„åŠŸèƒ½ç‚¹å’Œå®šä½çº¿ç´¢ï¼š")
            for i, fp_data in enumerate(function_points_data, 1):
                print(f"\n{Fore.YELLOW}[åŠŸèƒ½ç‚¹ {i}] {fp_data.get('name', 'N/A')}")
                print(f"  {Fore.CYAN}å…³é”®è¯: {', '.join(fp_data.get('keywords', []))}")
                print(f"  {Fore.CYAN}ç« èŠ‚çº¿ç´¢: {fp_data.get('section_hint', 'æ— ')}")
                print(f"  {Fore.CYAN}ç¡®åˆ‡çŸ­è¯­: {len(fp_data.get('exact_phrases', []))} ä¸ª")

                # å°è¯•æå–åŸæ–‡ç‰‡æ®µ
                try:
                    doc_snippet = generator.extract_relevant_section(requirement_doc, fp_data.get('name', ''), fp_data)
                    print(f"  {Fore.GREEN}åŸæ–‡ç‰‡æ®µ: {len(doc_snippet)} å­—ç¬¦")
                    print(f"  {Fore.WHITE}é¢„è§ˆ: {doc_snippet[:200]}...")
                except Exception as e:
                    print(f"  {Fore.RED}åŸæ–‡æå–å¤±è´¥: {str(e)}")

            print(f"\n{Fore.GREEN}âœ“ è°ƒè¯•å®Œæˆ")
            sys.exit(0)

        # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        result = generator.generate_test_cases(requirement_doc, limit=test_limit, max_workers=args.max_workers)

        # æ‰“å°ç»“æœ
        print_result(result)

        # è‡ªåŠ¨ä¿å­˜åˆ°æ–‡ä»¶
        output_file = script_dir / f"test_cases_{requirement_file.stem}.json"
        save_result(
            result,
            str(output_file),
            split_output=args.split_output,
            output_dir=args.output_dir
        )

        print(f"\n{Fore.GREEN}{'='*60}")
        print(f"{Fore.GREEN}âœ“ å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print(f"{Fore.GREEN}{'='*60}\n")

    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\n{Fore.RED}âœ— é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
