# encoding: utf-8
"""
çŸ¥è¯†åº“æœåŠ¡å±‚ï¼Œå°è£…çŸ¥è¯†åº“ç›¸å…³ä¸šåŠ¡é€»è¾‘ã€‚
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional

from core.engine.knowledge_base import (
    FeishuDocumentLoader,
    RAGEngine,
    VectorStore,
)
from core.logger import log


class KnowledgeBaseService:
    """çŸ¥è¯†åº“æœåŠ¡ï¼Œæä¾›æ–‡æ¡£åŒæ­¥å’Œé—®ç­”åŠŸèƒ½ã€‚"""

    def __init__(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“æœåŠ¡ã€‚"""
        self.document_loader = FeishuDocumentLoader()
        self._rag_engine = None

    @property
    def rag_engine(self) -> RAGEngine:
        """
        è·å–RAGå¼•æ“ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰ã€‚
        
        Returns:
            RAGå¼•æ“å®ä¾‹
            
        Raises:
            ImportError: å¦‚æœç¼ºå°‘å¿…è¦çš„ä¾èµ–
        """
        if self._rag_engine is None:
            try:
                self._rag_engine = RAGEngine()
            except ImportError as e:
                raise ImportError(
                    f"çŸ¥è¯†åº“åŠŸèƒ½ä¸å¯ç”¨ï¼ˆç¼ºå°‘ä¾èµ–ï¼‰: {e}\n"
                    "è¯·å®‰è£…ä¾èµ–: pip install sentence-transformers chromadb"
                ) from e
        return self._rag_engine

    def sync_documents_from_space(self, space_id: str) -> Dict[str, Any]:
        """
        ä»çŸ¥è¯†åº“ç©ºé—´åŒæ­¥æ–‡æ¡£ã€‚

        Args:
            space_id: çŸ¥è¯†åº“ç©ºé—´ID

        Returns:
            åŒæ­¥ç»“æœï¼ŒåŒ…å«åŒæ­¥çš„æ–‡æ¡£æ•°é‡å’ŒçŠ¶æ€
        """
        try:
            log.info(f"å¼€å§‹åŒæ­¥çŸ¥è¯†åº“ç©ºé—´: {space_id}")

            # åŠ è½½æ‰€æœ‰æ–‡æ¡£
            documents = self.document_loader.load_all_documents_from_space(space_id)

            if not documents:
                return {
                    "success": False,
                    "message": "æœªæ‰¾åˆ°æ–‡æ¡£",
                    "document_count": 0,
                }

            # å‡†å¤‡æ–‡æ¡£æ•°æ®
            doc_data = []
            for doc in documents:
                doc_data.append({
                    "id": doc["token"],
                    "content": doc["content"],
                    "metadata": {
                        "title": doc["meta"].get("title", "æœªçŸ¥æ ‡é¢˜"),
                        "url": doc["meta"].get("url", ""),
                        "space_id": space_id,
                        "document_id": doc["meta"].get("document_id", ""),
                    },
                })

            # ç´¢å¼•æ–‡æ¡£
            indexed_count = self.rag_engine.index_documents(doc_data)

            return {
                "success": True,
                "message": "åŒæ­¥æˆåŠŸ",
                "document_count": len(documents),
                "indexed_count": indexed_count,
            }

        except Exception as e:
            log.error(f"åŒæ­¥æ–‡æ¡£å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"åŒæ­¥å¤±è´¥: {str(e)}",
                "document_count": 0,
            }

    def sync_all_spaces(self) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰€æœ‰çŸ¥è¯†åº“ç©ºé—´ã€‚

        Returns:
            åŒæ­¥ç»“æœ
        """
        try:
            # è·å–æ‰€æœ‰çŸ¥è¯†åº“ç©ºé—´
            spaces = self.document_loader.load_wiki_spaces()

            total_documents = 0
            success_count = 0
            failed_spaces = []

            for space in spaces:
                space_id = space.get("space_id", "")
                space_name = space.get("name", "æœªçŸ¥")

                if not space_id:
                    continue

                log.info(f"åŒæ­¥çŸ¥è¯†åº“ç©ºé—´: {space_name} ({space_id})")

                result = self.sync_documents_from_space(space_id)
                if result["success"]:
                    success_count += 1
                    total_documents += result["document_count"]
                else:
                    failed_spaces.append({
                        "space_id": space_id,
                        "name": space_name,
                        "error": result["message"],
                    })

            return {
                "success": True,
                "message": f"åŒæ­¥å®Œæˆï¼šæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {len(failed_spaces)} ä¸ª",
                "total_spaces": len(spaces),
                "success_count": success_count,
                "failed_count": len(failed_spaces),
                "total_documents": total_documents,
                "failed_spaces": failed_spaces,
            }

        except Exception as e:
            error_msg = str(e)
            log.error(f"åŒæ­¥æ‰€æœ‰çŸ¥è¯†åº“å¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æƒé™é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™é‡æ–°æŠ›å‡ºä»¥ä¾¿APIå±‚å¤„ç†
            if "99991672" in error_msg or "æƒé™" in error_msg or "Access denied" in error_msg:
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©APIå±‚è¿”å›403
            
            return {
                "success": False,
                "message": f"åŒæ­¥å¤±è´¥: {error_msg}",
                "total_spaces": 0,
                "success_count": 0,
                "failed_count": 0,
                "total_documents": 0,
            }

    def ask(self, question: str, use_realtime_search: bool = True, space_id: Optional[str] = None) -> Dict[str, Any]:
        """
        å›ç­”é—®é¢˜ã€‚
        
        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        1. å®æ—¶æœç´¢æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼šç›´æ¥ä½¿ç”¨é£ä¹¦APIæœç´¢ï¼Œæ— éœ€å…ˆåŒæ­¥æ–‡æ¡£
        2. å‘é‡æœç´¢æ¨¡å¼ï¼šä½¿ç”¨æœ¬åœ°å‘é‡å­˜å‚¨è¿›è¡Œè¯­ä¹‰æœç´¢ï¼ˆéœ€è¦å…ˆåŒæ­¥æ–‡æ¡£ï¼‰

        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_realtime_search: æ˜¯å¦ä½¿ç”¨å®æ—¶æœç´¢æ¨¡å¼ï¼ˆé»˜è®¤Trueï¼‰
            space_id: æŒ‡å®šæœç´¢çš„çŸ¥è¯†åº“ç©ºé—´IDï¼Œå¦‚æœä¸æä¾›åˆ™æœç´¢æ‰€æœ‰ç©ºé—´

        Returns:
            ç­”æ¡ˆå’Œå¼•ç”¨æ¥æº
        """
        try:
            # æ£€æŸ¥å‘é‡å­˜å‚¨ä¸­æ˜¯å¦æœ‰æ–‡æ¡£
            collection_info = self.get_collection_info()
            has_local_docs = (
                collection_info.get("success") 
                and collection_info.get("info", {}).get("count", 0) > 0
            )
            
            # å¦‚æœå‘é‡å­˜å‚¨ä¸ºç©ºï¼Œè‡ªåŠ¨ä½¿ç”¨å®æ—¶æœç´¢æ¨¡å¼
            if not has_local_docs:
                log.info("å‘é‡å­˜å‚¨ä¸ºç©ºï¼Œä½¿ç”¨å®æ—¶æœç´¢æ¨¡å¼")
                use_realtime_search = True
            
            if use_realtime_search:
                return self._ask_with_realtime_search(question, space_id=space_id)
            else:
                # ä½¿ç”¨å‘é‡æœç´¢æ¨¡å¼ï¼ˆæš‚ä¸æ”¯æŒæŒ‡å®šspace_idï¼Œæœç´¢æ‰€æœ‰æ–‡æ¡£ï¼‰
                if space_id:
                    log.warning("å‘é‡æœç´¢æ¨¡å¼æš‚ä¸æ”¯æŒæŒ‡å®šçŸ¥è¯†åº“ï¼Œå°†æœç´¢æ‰€æœ‰æ–‡æ¡£")
            result = self.rag_engine.qa(question)
            return {
                "success": True,
                "answer": result["answer"],
                "sources": result["sources"],
            }

        except Exception as e:
            log.error(f"å›ç­”é—®é¢˜å¤±è´¥: {e}")
            return {
                "success": False,
                "answer": f"æŠ±æ­‰ï¼Œå¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "sources": [],
            }
    
    def get_wiki_spaces(self) -> Dict[str, Any]:
        """
        è·å–æ‰€æœ‰çŸ¥è¯†åº“ç©ºé—´åˆ—è¡¨ã€‚
        
        Returns:
            çŸ¥è¯†åº“ç©ºé—´åˆ—è¡¨
        """
        try:
            spaces = self.document_loader.load_wiki_spaces()
            space_list = []
            for space in spaces:
                space_list.append({
                    "space_id": space.get("space_id", ""),
                    "name": space.get("name", "æœªçŸ¥"),
                    "description": space.get("description", ""),
                })
            
            return {
                "success": True,
                "spaces": space_list,
                "message": f"æ‰¾åˆ° {len(space_list)} ä¸ªçŸ¥è¯†åº“ç©ºé—´",
            }
        except Exception as e:
            log.error(f"è·å–çŸ¥è¯†åº“ç©ºé—´åˆ—è¡¨å¤±è´¥: {e}")
            error_msg = str(e)
            # æ£€æŸ¥æ˜¯å¦æ˜¯æƒé™é”™è¯¯
            if "99991672" in error_msg or "æƒé™" in error_msg or "Access denied" in error_msg:
                return {
                    "success": False,
                    "spaces": [],
                    "message": f"æƒé™ä¸è¶³: {error_msg}ã€‚è¯·å…ˆè¿›è¡Œé£ä¹¦æˆæƒã€‚",
                }
            return {
                "success": False,
                "spaces": [],
                "message": f"è·å–çŸ¥è¯†åº“ç©ºé—´åˆ—è¡¨å¤±è´¥: {error_msg}",
            }
    
    def _ask_with_realtime_search(self, question: str, space_id: Optional[str] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨å®æ—¶æœç´¢æ¨¡å¼å›ç­”é—®é¢˜ï¼ˆç›´æ¥ä½¿ç”¨é£ä¹¦APIæœç´¢ï¼Œæ— éœ€åŒæ­¥æ–‡æ¡£ï¼‰ã€‚
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. æå–å…³é”®è¯è¿›è¡Œå¤šè½®æœç´¢
        2. ä½¿ç”¨embeddingå¯¹æœç´¢ç»“æœè¿›è¡Œé‡æ’åº
        3. æ™ºèƒ½æå–æ–‡æ¡£ç›¸å…³ç‰‡æ®µ
        4. å¹¶è¡Œæœç´¢å¤šä¸ªç©ºé—´
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            ç­”æ¡ˆå’Œå¼•ç”¨æ¥æº
        """
        try:
            from core.engine.base.llm_service import LLMService
            from core.engine.base.embedding_service import EmbeddingService
            import re
            
            log.info(f"ä½¿ç”¨å®æ—¶æœç´¢æ¨¡å¼å¤„ç†é—®é¢˜: {question}")
            
            # è·å–çŸ¥è¯†ç©ºé—´åˆ—è¡¨
            if space_id:
                # å¦‚æœæŒ‡å®šäº†space_idï¼Œåªæœç´¢è¯¥ç©ºé—´
                log.info(f"æŒ‡å®šæœç´¢çŸ¥è¯†åº“ç©ºé—´: {space_id}")
                # å…ˆè·å–æ‰€æœ‰ç©ºé—´ä»¥éªŒè¯space_idæ˜¯å¦å­˜åœ¨
                all_spaces = self.document_loader.load_wiki_spaces()
                spaces = [s for s in all_spaces if s.get("space_id") == space_id]
                if not spaces:
                    return {
                        "success": False,
                        "answer": f"æœªæ‰¾åˆ°æŒ‡å®šçš„çŸ¥è¯†åº“ç©ºé—´ï¼ˆID: {space_id}ï¼‰ï¼Œè¯·æ£€æŸ¥ç©ºé—´IDæ˜¯å¦æ­£ç¡®",
                        "sources": [],
                    }
                # æ‰¾åˆ°åŒ¹é…çš„ç©ºé—´ï¼Œè®°å½•ç©ºé—´åç§°
                matched_space = spaces[0]
                log.info(f"æ‰¾åˆ°æŒ‡å®šçš„çŸ¥è¯†åº“ç©ºé—´: {matched_space.get('name', 'æœªçŸ¥')} ({space_id})")
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šspace_idï¼Œæœç´¢æ‰€æœ‰ç©ºé—´
                spaces = self.document_loader.load_wiki_spaces()
                if not spaces:
                    return {
                        "success": False,
                        "answer": "æœªæ‰¾åˆ°çŸ¥è¯†åº“ç©ºé—´ï¼Œè¯·æ£€æŸ¥æƒé™é…ç½®",
                        "sources": [],
                    }
                log.info(f"å°†æœç´¢æ‰€æœ‰ {len(spaces)} ä¸ªçŸ¥è¯†åº“ç©ºé—´")
            
            # ã€AIåˆ†æé—®é¢˜ã€‘ä½¿ç”¨LLMåˆ†æé—®é¢˜å¹¶æå–æœç´¢å…³é”®è¯å’Œç­–ç•¥
            search_strategy = self._analyze_question_with_ai(question)
            keywords = search_strategy.get("keywords", [])
            search_queries = search_strategy.get("search_queries", [question])
            related_concepts = search_strategy.get("related_concepts", [])
            
            log.info(f"AIåˆ†æç»“æœ:")
            log.info(f"  å…³é”®è¯: {keywords}")
            log.info(f"  æœç´¢æŸ¥è¯¢: {search_queries}")
            log.info(f"  ç›¸å…³æ¦‚å¿µ: {related_concepts}")
            
            # ä¼˜åŒ–æœç´¢æŸ¥è¯¢ï¼šå»é™¤ç–‘é—®è¯ï¼Œæå–æ ¸å¿ƒå…³é”®è¯
            # é£ä¹¦æœç´¢APIä¸æ”¯æŒåŒ…å«ç–‘é—®è¯çš„å®Œæ•´é—®é¢˜ï¼Œéœ€è¦æå–å…³é”®è¯
            import re
            def clean_query(query: str) -> str:
                """æ¸…ç†æŸ¥è¯¢è¯ï¼Œå»é™¤ç–‘é—®è¯å’Œæ ‡ç‚¹"""
                # å»é™¤å¸¸è§çš„ç–‘é—®è¯
                question_words = ['ä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'æ˜¯ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›', 'å—', 'å‘¢', 'ï¼Ÿ', '?']
                cleaned = query
                for word in question_words:
                    cleaned = cleaned.replace(word, ' ')
                # å»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
                cleaned = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', cleaned)
                cleaned = ' '.join(cleaned.split())
                return cleaned.strip()
            
            # æ„å»ºæœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼šä¼˜å…ˆä½¿ç”¨å…³é”®è¯ï¼Œç„¶åä½¿ç”¨æ¸…ç†åçš„æŸ¥è¯¢
            final_search_queries = []
            
            # 1. ä½¿ç”¨æå–çš„å…³é”®è¯ï¼ˆæœ€é‡è¦ï¼‰
            if keywords:
                # ä½¿ç”¨å‰2ä¸ªæœ€é‡è¦çš„å…³é”®è¯
                for kw in keywords[:2]:
                    if kw and len(kw) >= 2:
                        final_search_queries.append(kw)
            
            # 2. ä½¿ç”¨æ¸…ç†åçš„åŸå§‹é—®é¢˜ï¼ˆå»é™¤ç–‘é—®è¯ï¼‰
            cleaned_question = clean_query(question)
            if cleaned_question and cleaned_question not in final_search_queries:
                final_search_queries.append(cleaned_question)
            
            # 3. å¦‚æœAIç”Ÿæˆçš„æœç´¢æŸ¥è¯¢ä¸­æœ‰å¥½çš„å…³é”®è¯ï¼Œä¹ŸåŠ å…¥
            for query in search_queries[:2]:
                cleaned = clean_query(query)
                if cleaned and cleaned not in final_search_queries and len(cleaned) >= 2:
                    final_search_queries.append(cleaned)
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            seen = set()
            unique_queries = []
            for q in final_search_queries:
                if q.lower() not in seen:
                    seen.add(q.lower())
                    unique_queries.append(q)
            
            search_queries = unique_queries[:3]  # æœ€å¤š3ä¸ªæŸ¥è¯¢
            log.info(f"ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢ï¼ˆå»é™¤ç–‘é—®è¯ï¼‰: {search_queries}")
            
            # åœ¨æ‰€æœ‰ç©ºé—´ä¸­æœç´¢ï¼ˆä½¿ç”¨AIæå–çš„æœç´¢ç­–ç•¥ï¼‰
            all_results = []
            client = self.document_loader.client
            
            import time
            
            # å¦‚æœæŒ‡å®šäº†space_idï¼Œåªæœç´¢è¯¥ç©ºé—´ï¼›å¦åˆ™é™åˆ¶æœç´¢çš„ç©ºé—´æ•°é‡
            if space_id:
                # æŒ‡å®šäº†space_idï¼Œåªæœç´¢è¯¥ç©ºé—´ï¼ˆspaceså·²ç»è¿‡æ»¤è¿‡äº†ï¼‰
                spaces_to_search = spaces
                log.info(f"å°†æœç´¢æŒ‡å®šçš„çŸ¥è¯†åº“ç©ºé—´: {spaces[0].get('name', 'æœªçŸ¥')}")
            else:
                # é™åˆ¶æœç´¢çš„ç©ºé—´æ•°é‡ï¼Œä¼˜å…ˆæœç´¢å‰3ä¸ªç©ºé—´
                spaces_to_search = spaces[:3] if len(spaces) > 3 else spaces
                if len(spaces) > 3:
                    log.info(f"ä¼˜åŒ–ï¼šé™åˆ¶æœç´¢ç©ºé—´æ•°é‡ä¸º {len(spaces_to_search)} ä¸ªï¼Œé¿å…é¢‘ç‡é™åˆ¶")
            
            for space_idx, space_item in enumerate(spaces_to_search):
                current_space_id = space_item.get("space_id", "")
                space_name = space_item.get("name", "æœªçŸ¥")
                
                if not current_space_id:
                    continue
                
                # æ¯ä¸ªç©ºé—´ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼ˆç¬¬ä¸€ä¸ªç©ºé—´ä¸éœ€è¦å»¶è¿Ÿï¼‰
                if space_idx > 0:
                    time.sleep(1.0)  # æ¯ä¸ªç©ºé—´ä¹‹é—´å»¶è¿Ÿ1ç§’
                
                # å°è¯•å¤šä¸ªæœç´¢è¯ï¼ˆæ·»åŠ å»¶è¿Ÿä»¥é¿å…é¢‘ç‡é™åˆ¶ï¼‰
                for idx, query in enumerate(search_queries):
                    try:
                        # æ¯ä¸ªæŸ¥è¯¢ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼ˆç¬¬ä¸€ä¸ªæŸ¥è¯¢ä¸éœ€è¦å»¶è¿Ÿï¼‰
                        if idx > 0:
                            time.sleep(1.0)  # æ¯ä¸ªæŸ¥è¯¢é—´éš”1ç§’ï¼ˆå¢åŠ å»¶è¿Ÿæ—¶é—´ï¼‰
                        
                        log.info(f"æœç´¢çŸ¥è¯†åº“: {space_name} - æŸ¥è¯¢: {query}")
                        search_result = client.search_wiki_nodes(
                            space_id=current_space_id,
                            query=query,
                            limit=20  # å¢åŠ æœç´¢æ•°é‡
                        )
                        
                        if search_result.get("code") == 0:
                            items = search_result.get("data", {}).get("items", [])
                            log.info(f"åœ¨çŸ¥è¯†åº“ {space_name} ä¸­æ‰¾åˆ° {len(items)} ä¸ªæ–‡æ¡£ï¼ˆæŸ¥è¯¢: {query}ï¼‰")
                            for item in items:
                                # æ‰“å°ç¬¬ä¸€ä¸ªitemçš„æ‰€æœ‰å­—æ®µä»¥ä¾¿è°ƒè¯•
                                if len(all_results) == 0:
                                    import json
                                    log.info(f"ğŸ“‹ æœç´¢ç»“æœåŸå§‹æ•°æ®ç»“æ„ï¼ˆç¬¬ä¸€ä¸ªitemï¼‰: {json.dumps(item, indent=2, ensure_ascii=False)}")
                                
                                obj_token = item.get("obj_token", "")
                                # é£ä¹¦æœç´¢APIè¿”å›çš„å­—æ®µå¯èƒ½æ˜¯ node_tokenï¼ˆå­—ç¬¦ä¸²tokenï¼‰ï¼Œè€Œä¸æ˜¯ node_idï¼ˆæ•°å­—IDï¼‰
                                # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„å­—æ®µå
                                node_token = item.get("node_token", "") or item.get("node_id", "")
                                # å¦‚æœ node_token æ˜¯å­—ç¬¦ä¸²ï¼Œè¯´æ˜å®ƒå°±æ˜¯æˆ‘ä»¬è¦ç”¨çš„token
                                # å¦‚æœ node_id æ˜¯æ•°å­—ï¼Œè¯´æ˜å®ƒæ˜¯çœŸæ­£çš„æ•°å­—ID
                                node_id = item.get("node_id", "")
                                # å¦‚æœ node_id æ˜¯å­—ç¬¦ä¸²tokenï¼Œè¯´æ˜å­—æ®µæ˜ å°„æœ‰é—®é¢˜ï¼Œåº”è¯¥ä½¿ç”¨ node_token
                                if node_id and not str(node_id).isdigit():
                                    # node_id æ˜¯å­—ç¬¦ä¸²tokenï¼Œè¯´æ˜å®ƒå®é™…ä¸Šæ˜¯ node_token
                                    node_token = node_id
                                    node_id = ""  # æ¸…ç©ºï¼Œå› ä¸ºæ²¡æœ‰çœŸæ­£çš„æ•°å­—ID
                                
                                title = item.get("title", "æœªçŸ¥æ ‡é¢˜")
                                
                                # å»é‡ï¼ˆåŸºäºobj_tokenï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨node_tokenï¼‰
                                unique_key = obj_token or node_token or node_id
                                if not any(r.get("obj_token") == obj_token or 
                                          r.get("node_id") == node_id or 
                                          r.get("node_token") == node_token 
                                          for r in all_results):
                                    # ä»æœç´¢ç»“æœä¸­æå–URLï¼ˆæœç´¢APIè¿”å›çš„ç»“æœåŒ…å«urlå­—æ®µï¼‰
                                    url = item.get("url", "")
                                    all_results.append({
                                        "title": title,
                                        "obj_token": obj_token,
                                        "node_id": node_id,  # ä¿å­˜node_idï¼ˆæ•°å­—IDï¼Œå¦‚æœæœ‰ï¼‰
                                        "node_token": node_token,  # ä¿å­˜node_tokenï¼ˆå­—ç¬¦ä¸²tokenï¼Œç”¨äºwiki APIï¼‰
                                        "space_id": current_space_id,
                                        "space_name": space_name,
                                        "search_query": query,  # è®°å½•åŒ¹é…çš„æœç´¢è¯
                                        "url": url,  # ä¿å­˜URLï¼ˆæœç´¢APIè¿”å›çš„ï¼‰
                                    })
                    except Exception as e:
                        error_str = str(e)
                        log.warning(f"æœç´¢çŸ¥è¯†ç©ºé—´ {space_name} (æŸ¥è¯¢: {query}) å¤±è´¥: {e}")
                        
                        # å¦‚æœæ˜¯é¢‘ç‡é™åˆ¶é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´å¹¶è·³è¿‡åç»­æŸ¥è¯¢
                        if "frequency limit" in error_str.lower() or "99991400" in error_str:
                            log.warning("æ£€æµ‹åˆ°é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…5ç§’åè·³è¿‡å½“å‰ç©ºé—´...")
                            time.sleep(5)  # ç­‰å¾…5ç§’
                            break  # è·³è¿‡å½“å‰ç©ºé—´çš„å…¶ä»–æŸ¥è¯¢
                        continue
                    
                    # å¦‚æœå·²ç»æ‰¾åˆ°è¶³å¤Ÿçš„ç»“æœï¼Œå¯ä»¥æå‰åœæ­¢
                    if len(all_results) >= 20:
                        log.info(f"å·²æ‰¾åˆ°è¶³å¤Ÿçš„ç»“æœï¼ˆ{len(all_results)}ä¸ªï¼‰ï¼Œæå‰åœæ­¢æœç´¢")
                        break
                
                # å¦‚æœå·²ç»æ‰¾åˆ°è¶³å¤Ÿçš„ç»“æœï¼Œæå‰åœæ­¢æœç´¢æ‰€æœ‰ç©ºé—´
                if len(all_results) >= 20:
                    break
            
            if not all_results:
                return {
                    "success": True,
                    "answer": "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚å»ºè®®ï¼š\n1. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯\n2. æˆ–è€…å…ˆåŒæ­¥æ–‡æ¡£ä»¥è·å¾—æ›´å¥½çš„è¯­ä¹‰æœç´¢æ•ˆæœ",
                    "sources": [],
                }
            
            log.info(f"æ‰¾åˆ° {len(all_results)} ä¸ªå€™é€‰æ–‡æ¡£ï¼Œå¼€å§‹åŠ è½½å†…å®¹å¹¶é‡æ’åº...")
            
            # åŠ è½½æ–‡æ¡£å†…å®¹å¹¶è®¡ç®—ç›¸ä¼¼åº¦
            doc_results = []
            import time
            
            # ä»æœç´¢ç»“æœä¸­æå–URLï¼ˆå¦‚æœæœ‰ï¼‰
            for idx, result in enumerate(all_results[:15]):  # é™åˆ¶åŠ è½½æ•°é‡ä»¥æé«˜æ€§èƒ½
                try:
                    # æ·»åŠ å»¶è¿Ÿä»¥é¿å…é¢‘ç‡é™åˆ¶ï¼ˆæ¯3ä¸ªæ–‡æ¡£é—´éš”0.5ç§’ï¼‰
                    if idx > 0 and idx % 3 == 0:
                        time.sleep(0.5)
                    
                    # å°è¯•è·å–æ–‡æ¡£å†…å®¹ï¼ˆçŸ¥è¯†åº“æœç´¢è¿”å›çš„éƒ½æ˜¯wikièŠ‚ç‚¹ï¼‰
                    doc_content = None
                    doc_meta = None
                    title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                    url = result.get("url", "")  # æœç´¢è¿”å›çš„ç»“æœåŒ…å«URL
                    node_id = result.get("node_id", "")
                    
                    # å¦‚æœæœç´¢ç»“æœä¸­æ²¡æœ‰URLï¼Œå°è¯•ä»node_idæ„å»º
                    if not url and node_id:
                        # ä»node_idæ„å»ºwiki URLï¼ˆæ ¼å¼ï¼šhttps://xxx.feishu.cn/wiki/{node_id}ï¼‰
                        # ä½†æˆ‘ä»¬éœ€è¦çŸ¥é“åŸŸåï¼Œæ‰€ä»¥å…ˆå°è¯•è·å–
                        pass  # æš‚æ—¶è·³è¿‡ï¼Œä½¿ç”¨æœç´¢ç»“æœä¸­çš„URL
                    
                    # å°è¯•è·å–æ–‡æ¡£å†…å®¹ï¼ˆä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨æœç´¢ç»“æœä¸­çš„obj_tokenå’Œobj_typeï¼Œé¿å…ä¸å¿…è¦çš„wiki APIè°ƒç”¨ï¼‰
                    doc_content = None
                    try:
                        node_id = result.get("node_id", "")
                        node_token = result.get("node_token", "")
                        obj_token = result.get("obj_token", "")
                        obj_type = result.get("obj_type", "")
                        
                        # æ‰“å°æœç´¢ç»“æœçš„æ‰€æœ‰å­—æ®µä»¥ä¾¿è°ƒè¯•
                        import json
                        result_keys = list(result.keys())
                        log.info(f"ğŸ“‹ æœç´¢ç»“æœå­—æ®µ: {result_keys}")
                        log.info(f"ğŸ“‹ å…³é”®å­—æ®µå€¼: node_id={node_id[:30] if node_id else 'None'}... (ç±»å‹: {type(node_id).__name__}), node_token={node_token[:30] if node_token else 'None'}..., obj_token={obj_token[:30] if obj_token else 'None'}..., obj_type={obj_type}")
                        
                        # ä¼˜åŒ–ç­–ç•¥ï¼šå¦‚æœæœç´¢ç»“æœä¸­å·²ç»æœ‰obj_tokenå’Œobj_typeï¼Œç›´æ¥ä½¿ç”¨ï¼Œé¿å…ä¸å¿…è¦çš„wiki APIè°ƒç”¨
                        if obj_token:
                            # å¦‚æœobj_typeæ˜¯docxï¼ˆç±»å‹8ï¼‰ï¼Œç›´æ¥ä½¿ç”¨docx APIï¼Œè·³è¿‡wiki API
                            if obj_type == "docx" or obj_type == 8:
                                log.info(f"ğŸ“‹ æ£€æµ‹åˆ°obj_type={obj_type}ï¼ˆdocxï¼‰ï¼Œç›´æ¥ä½¿ç”¨obj_tokenè°ƒç”¨docx APIï¼Œè·³è¿‡wiki API")
                                doc_content = self.document_loader.load_document_content(obj_token, is_wiki_node=False)
                                if doc_content and len(doc_content.strip()) >= 10:
                                    log.info(f"âœ… ç›´æ¥ä½¿ç”¨obj_tokenè·å–docxæ–‡æ¡£å†…å®¹æˆåŠŸï¼Œé•¿åº¦: {len(doc_content)} å­—ç¬¦")
                                else:
                                    log.warning(f"âš ï¸ ä½¿ç”¨obj_tokenè·å–docxæ–‡æ¡£å†…å®¹å¤±è´¥ï¼ˆé•¿åº¦: {len(doc_content) if doc_content else 0}ï¼‰")
                            else:
                                # obj_typeä¸æ˜¯docxï¼Œå…ˆå°è¯•ä½œä¸ºæ™®é€šæ–‡æ¡£ï¼Œå¤±è´¥åå†å°è¯•wiki API
                                log.info(f"ğŸ“‹ æ£€æµ‹åˆ°obj_type={obj_type}ï¼Œå…ˆå°è¯•ä½œä¸ºæ™®é€šæ–‡æ¡£è·å–å†…å®¹")
                                doc_content = self.document_loader.load_document_content(obj_token, is_wiki_node=False)
                                if doc_content and len(doc_content.strip()) >= 10:
                                    log.info(f"âœ… ä½¿ç”¨obj_tokenä½œä¸ºæ™®é€šæ–‡æ¡£æˆåŠŸè·å–å†…å®¹ï¼Œé•¿åº¦: {len(doc_content)} å­—ç¬¦")
                                else:
                                    log.info(f"ğŸ“‹ obj_tokenä½œä¸ºæ™®é€šæ–‡æ¡£å¤±è´¥ï¼Œå°è¯•ä½œä¸ºwikièŠ‚ç‚¹...")
                                    doc_content = self.document_loader.load_document_content(obj_token, is_wiki_node=True)
                                    if doc_content and len(doc_content.strip()) >= 10:
                                        log.info(f"âœ… ä½¿ç”¨obj_tokenä½œä¸ºwikièŠ‚ç‚¹æˆåŠŸè·å–å†…å®¹ï¼Œé•¿åº¦: {len(doc_content)} å­—ç¬¦")
                                    else:
                                        log.warning(f"âš ï¸ ä½¿ç”¨obj_tokenè·å–æ–‡æ¡£å†…å®¹å¤±è´¥ï¼ˆé•¿åº¦: {len(doc_content) if doc_content else 0}ï¼‰")
                        
                        # å¦‚æœæ²¡æœ‰obj_tokenï¼Œæˆ–è€…ä½¿ç”¨obj_tokenå¤±è´¥ï¼Œæ‰å°è¯•ä½¿ç”¨wiki API
                        if not doc_content or len(doc_content.strip()) < 10:
                            # wiki APIéœ€è¦ä½¿ç”¨node_tokenï¼ˆå­—ç¬¦ä¸²tokenï¼‰ï¼Œè€Œä¸æ˜¯node_idï¼ˆæ•°å­—IDï¼‰
                            # æ ¹æ®é£ä¹¦APIæ–‡æ¡£ï¼Œwiki/v2/nodes/{node_token} éœ€è¦ä½¿ç”¨node_tokenï¼ˆå­—ç¬¦ä¸²ï¼‰
                            # å¦‚æœnode_idæ˜¯å­—ç¬¦ä¸²tokenï¼Œè¯´æ˜å­—æ®µæ˜ å°„æœ‰é—®é¢˜ï¼Œåº”è¯¥ä½¿ç”¨node_token
                            if node_id and not str(node_id).isdigit():
                                # node_id æ˜¯å­—ç¬¦ä¸²tokenï¼Œè¯´æ˜å®ƒå®é™…ä¸Šæ˜¯ node_token
                                log.debug(f"node_idå­—æ®µåŒ…å«å­—ç¬¦ä¸²tokenï¼Œä½¿ç”¨node_token")
                                node_token = node_id
                                node_id = ""
                            
                            # ä¼˜å…ˆä½¿ç”¨node_tokenï¼ˆå­—ç¬¦ä¸²tokenï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨node_idï¼ˆæ•°å­—IDï¼‰
                            wiki_node_token = node_token or node_id
                            
                            if wiki_node_token:
                                log.info(f"ğŸ“‹ å°è¯•ä½¿ç”¨wiki APIè·å–èŠ‚ç‚¹ä¿¡æ¯: {wiki_node_token[:30]}...")
                                try:
                                    # å…ˆè·å–wikièŠ‚ç‚¹ä¿¡æ¯ï¼Œå¯èƒ½åŒ…å«obj_token
                                    wiki_result = self.document_loader.client.get_wiki_node_content(wiki_node_token)
                                    if wiki_result.get("code") == 0:
                                        log.info(f"âœ… wiki APIè·å–èŠ‚ç‚¹ä¿¡æ¯æˆåŠŸ")
                                        wiki_data = wiki_result.get("data", {})
                                        wiki_node = wiki_data.get("node", {})
                                        if wiki_node:
                                            # ä»wikièŠ‚ç‚¹ä¸­æå–obj_tokenï¼ˆæ–‡æ¡£çš„å®é™…tokenï¼‰
                                            actual_obj_token = wiki_node.get("obj_token", "")
                                            actual_obj_type = wiki_node.get("obj_type", "")
                                            
                                            log.info(f"ğŸ“‹ ä»wikièŠ‚ç‚¹æå–ä¿¡æ¯ - obj_token: {actual_obj_token[:30] if actual_obj_token else 'None'}..., obj_type: {actual_obj_type}")
                                            
                                            if actual_obj_token:
                                                log.info(f"ğŸ“‹ ä½¿ç”¨ä»wikièŠ‚ç‚¹è·å–çš„obj_tokenè·å–æ–‡æ¡£å†…å®¹: {actual_obj_token[:30]}...")
                                                # ä½¿ç”¨å®é™…çš„obj_tokenè·å–æ–‡æ¡£å†…å®¹
                                                # å¦‚æœobj_typeæ˜¯docxï¼Œè¯´æ˜obj_tokenæ˜¯æ–‡æ¡£tokenï¼Œä¸æ˜¯wikièŠ‚ç‚¹
                                                if actual_obj_type == "docx" or actual_obj_type == 8:
                                                    log.info(f"ğŸ“‹ obj_typeæ˜¯docxï¼Œä½¿ç”¨docs/docx API")
                                                    doc_content = self.document_loader.load_document_content(actual_obj_token, is_wiki_node=False)
                                                else:
                                                    log.info(f"ğŸ“‹ obj_typeæ˜¯{actual_obj_type}ï¼Œå…ˆå°è¯•wiki APIï¼Œå¤±è´¥åå°è¯•docs/docx API")
                                                    doc_content = self.document_loader.load_document_content(actual_obj_token, is_wiki_node=True)
                                                if doc_content and len(doc_content.strip()) >= 10:
                                                    log.info(f"âœ… é€šè¿‡wiki APIæˆåŠŸè·å–æ–‡æ¡£å†…å®¹ï¼Œé•¿åº¦: {len(doc_content)} å­—ç¬¦")
                                                else:
                                                    log.warning(f"âš ï¸ è·å–æ–‡æ¡£å†…å®¹å¤±è´¥æˆ–å†…å®¹ä¸ºç©ºï¼ˆé•¿åº¦: {len(doc_content) if doc_content else 0}ï¼‰")
                                            else:
                                                log.info(f"ğŸ“‹ wikièŠ‚ç‚¹æ²¡æœ‰obj_tokenï¼Œå°è¯•ç›´æ¥åŠ è½½èŠ‚ç‚¹å†…å®¹")
                                                # å¦‚æœwikièŠ‚ç‚¹ç›´æ¥åŒ…å«å†…å®¹ï¼Œå°è¯•ç›´æ¥åŠ è½½
                                                doc_content = self.document_loader.load_document_content(wiki_node_token, is_wiki_node=True)
                                                if doc_content and len(doc_content.strip()) >= 10:
                                                    log.info(f"âœ… é€šè¿‡wiki APIç›´æ¥è·å–èŠ‚ç‚¹å†…å®¹æˆåŠŸï¼Œé•¿åº¦: {len(doc_content)} å­—ç¬¦")
                                                else:
                                                    log.warning(f"âš ï¸ ç›´æ¥åŠ è½½èŠ‚ç‚¹å†…å®¹å¤±è´¥æˆ–å†…å®¹ä¸ºç©ºï¼ˆé•¿åº¦: {len(doc_content) if doc_content else 0}ï¼‰")
                                    else:
                                        error_code = wiki_result.get("code")
                                        error_msg = wiki_result.get("msg", "")
                                        log.debug(f"wiki APIè¿”å›é”™è¯¯: {error_msg} (code: {error_code})")
                                except Exception as e:
                                    error_str = str(e)
                                    if "404" in error_str or "99991679" in error_str:
                                        log.debug(f"wiki APIæƒé™ä¸è¶³æˆ–404: {type(e).__name__}: {str(e)[:200]}")
                                    else:
                                        log.debug(f"wiki APIè·å–èŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {type(e).__name__}: {str(e)[:200]}")
                        if doc_content and len(doc_content.strip()) >= 10:
                            # æˆåŠŸè·å–å†…å®¹ï¼Œå°è¯•è·å–å…ƒä¿¡æ¯
                            try:
                                doc_meta = self.document_loader.load_document_meta(result["obj_token"])
                                if doc_meta:
                                    title = doc_meta.get("title", title)
                                    # å¦‚æœå…ƒä¿¡æ¯ä¸­æœ‰URLï¼Œä¼˜å…ˆä½¿ç”¨
                                    meta_url = doc_meta.get("url", "")
                                    if meta_url:
                                        url = meta_url
                            except Exception:
                                # å…ƒä¿¡æ¯è·å–å¤±è´¥ä¸å½±å“ï¼Œä½¿ç”¨æœç´¢ç»“æœä¸­çš„ä¿¡æ¯
                                pass
                    except Exception as e:
                        # é™é»˜å¤„ç†ï¼Œä¸è¾“å‡ºå¤§é‡é”™è¯¯æ—¥å¿—ï¼ˆè¿™äº›é”™è¯¯æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºæƒé™ä¸è¶³ï¼‰
                        # åªåœ¨DEBUGçº§åˆ«è®°å½•
                        log.debug(f"æ— æ³•è·å–æ–‡æ¡£ {title} çš„å®Œæ•´å†…å®¹ï¼ˆæƒé™é™åˆ¶ï¼‰: {type(e).__name__}")
                        # å¦‚æœæ— æ³•è·å–å†…å®¹ï¼Œç»§ç»­å¤„ç†ï¼Œè‡³å°‘ä¿ç•™æ ‡é¢˜å’ŒURL
                    
                    # å¦‚æœæ— æ³•è·å–æ–‡æ¡£å†…å®¹ï¼Œä½†è‡³å°‘ä¿ç•™æ ‡é¢˜å’ŒURLä½œä¸ºæ¥æº
                    if not doc_content or len(doc_content.strip()) < 10:
                        # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œä½†è‡³å°‘ä¿ç•™æ ‡é¢˜å’ŒURL
                        if title and title != "æœªçŸ¥æ ‡é¢˜":
                            # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå†…å®¹ç‰‡æ®µï¼ˆè‡³å°‘è®©ç”¨æˆ·çŸ¥é“æ‰¾åˆ°äº†ç›¸å…³æ–‡æ¡£ï¼‰
                            doc_results.append({
                                "title": title,
                                "url": url,
                                "content": f"æ–‡æ¡£æ ‡é¢˜ï¼š{title}",
                                "full_content": "",
                                "similarity": 0.5,  # ç»™äºˆä¸­ç­‰ç›¸ä¼¼åº¦ï¼Œå› ä¸ºè‡³å°‘æ ‡é¢˜åŒ¹é…
                                "obj_token": result.get("obj_token", ""),
                                "has_content": False,  # æ ‡è®°ä¸ºæ²¡æœ‰å®Œæ•´å†…å®¹
                            })
                        continue
                    
                    # æå–æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
                    relevant_chunk = self._extract_relevant_chunk(doc_content, question, keywords)
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨embeddingï¼‰- ä½¿ç”¨æå–çš„ç›¸å…³ç‰‡æ®µè®¡ç®—ï¼Œè€Œä¸æ˜¯åŸå§‹å†…å®¹
                    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨relevant_chunkè€Œä¸æ˜¯doc_contentï¼Œå› ä¸ºrelevant_chunkæ˜¯æå–çš„æœ€ç›¸å…³éƒ¨åˆ†
                    similarity = self._calculate_similarity(question, relevant_chunk)
                    
                    doc_results.append({
                        "title": title,
                        "url": url,
                        "content": relevant_chunk,
                        "full_content": doc_content,
                        "similarity": similarity,
                        "obj_token": result["obj_token"],
                        "has_content": True,  # æ ‡è®°ä¸ºæœ‰å®Œæ•´å†…å®¹
                    })
                except Exception as e:
                    log.warning(f"å¤„ç†æ–‡æ¡£ {result.get('title', 'æœªçŸ¥')} å¤±è´¥: {e}")
                    # å³ä½¿å¤„ç†å¤±è´¥ï¼Œä¹Ÿå°è¯•ä¿ç•™æ ‡é¢˜å’ŒURL
                    title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                    url = result.get("url", "")
                    if title and title != "æœªçŸ¥æ ‡é¢˜":
                        doc_results.append({
                            "title": title,
                            "url": url,
                            "content": f"æ–‡æ¡£æ ‡é¢˜ï¼š{title}",
                            "full_content": "",
                            "similarity": 0.3,
                            "obj_token": result.get("obj_token", ""),
                            "has_content": False,
                        })
                    continue
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆä¼˜å…ˆæœ‰å®Œæ•´å†…å®¹çš„æ–‡æ¡£ï¼‰
            doc_results.sort(key=lambda x: (x.get("has_content", False), x["similarity"]), reverse=True)
            
            # åˆ†ç¦»æœ‰å†…å®¹å’Œæ— å†…å®¹çš„æ–‡æ¡£
            results_with_content = [r for r in doc_results if r.get("has_content", True)]
            results_without_content = [r for r in doc_results if not r.get("has_content", True)]
            
            # ä¼˜å…ˆä½¿ç”¨æœ‰å†…å®¹çš„æ–‡æ¡£ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼0.3
            filtered_results = [r for r in results_with_content if r["similarity"] >= 0.3]
            
            # å¦‚æœæ²¡æœ‰é«˜ç›¸ä¼¼åº¦çš„æœ‰å†…å®¹æ–‡æ¡£ï¼Œè‡³å°‘è¿”å›å‰å‡ ä¸ªæœ‰å†…å®¹çš„
            if not filtered_results:
                filtered_results = results_with_content[:3] if results_with_content else []
            
            # å¦‚æœæ²¡æœ‰æœ‰å†…å®¹çš„æ–‡æ¡£ï¼Œä½¿ç”¨æ— å†…å®¹çš„æ–‡æ¡£ï¼ˆè‡³å°‘æ ‡é¢˜åŒ¹é…ï¼‰
            if not filtered_results and results_without_content:
                filtered_results = results_without_content[:5]
                log.info(f"æ— æ³•è·å–æ–‡æ¡£å®Œæ•´å†…å®¹ï¼Œä½¿ç”¨æ–‡æ¡£æ ‡é¢˜ä½œä¸ºæ¥æºï¼ˆ{len(filtered_results)}ä¸ªï¼‰")
            
            # å–å‰5ä¸ªæœ€ç›¸å…³çš„ç»“æœ
            top_results = filtered_results[:5]
            
            # ç»Ÿè®¡æœ‰å†…å®¹å’Œæ— å†…å®¹çš„æ–‡æ¡£æ•°é‡
            content_count = sum(1 for r in top_results if r.get("has_content", True))
            title_only_count = len(top_results) - content_count
            if title_only_count > 0:
                log.warning(f"æ‰¾åˆ° {len(top_results)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œå…¶ä¸­ {title_only_count} ä¸ªæ— æ³•è·å–å®Œæ•´å†…å®¹ï¼ˆå¯èƒ½æƒé™ä¸è¶³ï¼‰")
            
            # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨ç›¸å…³ç‰‡æ®µï¼‰
            context_parts = []
            sources = []
            has_content_results = []
            title_only_results = []
            
            for result in top_results:
                # ä½¿ç”¨æœ‰å®Œæ•´å†…å®¹çš„æ–‡æ¡£æ„å»ºä¸Šä¸‹æ–‡
                if result.get("has_content", True):
                    # ä¼˜å…ˆä½¿ç”¨full_contentï¼ˆå®Œæ•´å†…å®¹ï¼‰ï¼Œå¢åŠ é•¿åº¦é™åˆ¶åˆ°8000å­—ç¬¦
                    # å¦‚æœå®Œæ•´å†…å®¹å¤ªé•¿ï¼ˆè¶…è¿‡8000å­—ç¬¦ï¼‰ï¼Œä½¿ç”¨æå–çš„ç›¸å…³ç‰‡æ®µ
                    full_content = result.get("full_content", "")
                    extracted_chunk = result.get("content", "")
                    
                    if full_content and len(full_content) <= 8000:
                        # ä½¿ç”¨å®Œæ•´å†…å®¹ï¼ˆå¦‚æœé•¿åº¦åˆç†ï¼‰
                        content_to_use = full_content
                    elif full_content and len(full_content) > 8000:
                        # å¦‚æœå®Œæ•´å†…å®¹å¤ªé•¿ï¼Œä½¿ç”¨æå–çš„ç›¸å…³ç‰‡æ®µï¼Œä½†å°½é‡ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡
                        # å°è¯•ä»å®Œæ•´å†…å®¹ä¸­æå–åŒ…å«ç›¸å…³ç‰‡æ®µçš„éƒ¨åˆ†ï¼ˆå‰åå„ä¿ç•™1000å­—ç¬¦ï¼‰
                        if extracted_chunk:
                            # æ‰¾åˆ°æå–ç‰‡æ®µåœ¨å®Œæ•´å†…å®¹ä¸­çš„ä½ç½®
                            chunk_start = full_content.find(extracted_chunk[:100])
                            if chunk_start >= 0:
                                # æå–ç‰‡æ®µå‰åå„1000å­—ç¬¦
                                context_start = max(0, chunk_start - 1000)
                                context_end = min(len(full_content), chunk_start + len(extracted_chunk) + 1000)
                                content_to_use = full_content[context_start:context_end]
                            else:
                                content_to_use = extracted_chunk
                        else:
                            content_to_use = full_content[:8000] + "..."
                    else:
                        # æ²¡æœ‰å®Œæ•´å†…å®¹ï¼Œä½¿ç”¨æå–çš„ç‰‡æ®µ
                        content_to_use = extracted_chunk
                    
                    # ç»“æ„åŒ–ç»„ç»‡ï¼šä¿ç•™æ–‡æ¡£æ ‡é¢˜ï¼Œæ¸…æ™°åˆ†éš”
                    context_parts.append(f"ã€æ–‡æ¡£ï¼š{result['title']}ã€‘\n\n{content_to_use}\n")
                    has_content_results.append(result)
                else:
                    # å³ä½¿æ²¡æœ‰å®Œæ•´å†…å®¹ï¼Œä¹Ÿè®°å½•æ ‡é¢˜ä¿¡æ¯
                    title_only_results.append(result)
                
                # æ‰€æœ‰ç»“æœéƒ½æ·»åŠ åˆ°sourcesï¼ˆåŒ…æ‹¬åªæœ‰æ ‡é¢˜çš„ï¼‰
                sources.append({
                    "title": result["title"],
                    "url": result["url"],
                    "similarity": result["similarity"],
                })
            
            context = "\n\n".join(context_parts)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£å†…å®¹
            has_document_content = len(has_content_results) > 0
            
            # å³ä½¿æ²¡æœ‰å®Œæ•´å†…å®¹ï¼Œä¹Ÿå°è¯•ä½¿ç”¨æ ‡é¢˜ä¿¡æ¯ç”Ÿæˆç­”æ¡ˆ
            if not has_document_content:
                log.warning("æ— æ³•è·å–æ–‡æ¡£å®Œæ•´å†…å®¹ï¼Œå°è¯•åŸºäºæ–‡æ¡£æ ‡é¢˜ç”Ÿæˆç­”æ¡ˆ")
                
                # æ„å»ºåŸºäºæ ‡é¢˜çš„ä¸Šä¸‹æ–‡
                title_context_parts = []
                for result in top_results[:5]:  # ä½¿ç”¨å‰5ä¸ªç»“æœ
                    title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                    url = result.get("url", "")
                    similarity = result.get("similarity", 0)
                    search_query = result.get("search_query", "")
                    
                    # æ„å»ºæ ‡é¢˜ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«æ ‡é¢˜ã€ç›¸ä¼¼åº¦å’ŒåŒ¹é…çš„æœç´¢è¯ï¼‰
                    title_info = f"ã€æ–‡æ¡£ï¼š{title}ã€‘"
                    if search_query:
                        title_info += f"\nåŒ¹é…çš„æœç´¢è¯ï¼š{search_query}"
                    if similarity > 0:
                        title_info += f"\nç›¸å…³æ€§ï¼š{similarity:.2f}"
                    if url:
                        title_info += f"\né“¾æ¥ï¼š{url}"
                    
                    title_context_parts.append(title_info)
                
                title_context = "\n\n".join(title_context_parts)
                
                # ä½¿ç”¨LLMåŸºäºæ ‡é¢˜ä¿¡æ¯ç”Ÿæˆç­”æ¡ˆ
                try:
                    llm_service = LLMService()
                    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼Œä½†å—é™äºæƒé™ï¼Œæˆ‘åªèƒ½è·å–åˆ°ç›¸å…³æ–‡æ¡£çš„æ ‡é¢˜ä¿¡æ¯ï¼Œæ— æ³•è·å–å®Œæ•´å†…å®¹ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æå–çš„å…³é”®è¯ã€‘
{', '.join(keywords) if keywords else 'æ— '}

ã€ç›¸å…³æ¦‚å¿µã€‘
{', '.join(related_concepts) if related_concepts else 'æ— '}

ã€æ‰¾åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼ˆä»…æ ‡é¢˜ï¼‰ã€‘
{title_context}

ã€è¦æ±‚ã€‘
1. åŸºäºæ–‡æ¡£æ ‡é¢˜ï¼Œå°è¯•æ¨æ–­è¿™äº›æ–‡æ¡£å¯èƒ½åŒ…å«å“ªäº›ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯
2. å¦‚æœæ ‡é¢˜æ˜æ˜¾ä¸é—®é¢˜ç›¸å…³ï¼Œå¯ä»¥åŸºäºæ ‡é¢˜è¿›è¡Œåˆç†æ¨æ–­å¹¶ç»™å‡ºç­”æ¡ˆ
3. å¦‚æœæ ‡é¢˜ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜"æ ¹æ®æ–‡æ¡£æ ‡é¢˜ï¼Œæ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³æ–‡æ¡£ï¼Œä½†ç”±äºæƒé™é™åˆ¶æ— æ³•è·å–å®Œæ•´å†…å®¹"
4. åˆ—å‡ºæ‰¾åˆ°çš„ç›¸å…³æ–‡æ¡£æ ‡é¢˜ï¼Œå¹¶å»ºè®®ç”¨æˆ·ç‚¹å‡»é“¾æ¥æŸ¥çœ‹å®Œæ•´å†…å®¹
5. ä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼Œè¯­è¨€ç®€æ´æ˜äº†

ã€ç­”æ¡ˆã€‘
è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£æ ‡é¢˜ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼š
"""
                    answer = llm_service.generate(prompt)
                except Exception as e:
                    log.warning(f"åŸºäºæ ‡é¢˜ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æç¤º")
                    # å›é€€åˆ°ç®€å•çš„æç¤º
                    title_list = [r["title"] for r in top_results if r.get("title")]
                    answer = (
                        f"æ ¹æ®æœç´¢ï¼Œæ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³æ–‡æ¡£ï¼š\n\n"
                        + "\n".join([f"{i+1}. {title}" for i, title in enumerate(title_list)])
                        + "\n\n"
                        + "âš ï¸ æ³¨æ„ï¼šç”±äºæƒé™é™åˆ¶ï¼Œæ— æ³•è·å–æ–‡æ¡£çš„å®Œæ•´å†…å®¹ã€‚\n"
                        + "å»ºè®®ï¼š\n"
                        + "1. ç‚¹å‡»ä¸Šæ–¹æ–‡æ¡£é“¾æ¥æŸ¥çœ‹å®Œæ•´å†…å®¹\n"
                        + "2. æˆ–è€…å…ˆåŒæ­¥æ–‡æ¡£åˆ°æœ¬åœ°å‘é‡åº“ä»¥è·å¾—æ›´å¥½çš„æœç´¢æ•ˆæœ"
                    )
            else:
                # ã€AIåˆ†ææœç´¢ç»“æœã€‘å…ˆè®©AIåˆ†ææœç´¢ç»“æœçš„ç›¸å…³æ€§å’Œå…³é”®ä¿¡æ¯
                analysis_result = self._analyze_search_results_with_ai(question, has_content_results, keywords, related_concepts)
                
                # ã€AIç”Ÿæˆç­”æ¡ˆã€‘ä½¿ç”¨LLMåŸºäºæœç´¢ç»“æœå’ŒAIåˆ†æç”Ÿæˆç­”æ¡ˆ
                llm_service = LLMService()
                prompt = self._build_answer_prompt(question, context, has_content_results, analysis_result, keywords)
                
                answer = llm_service.generate(prompt)
            
            return {
                "success": True,
                "answer": answer.strip(),
                "sources": sources,
            }
            
        except Exception as e:
            log.error(f"å®æ—¶æœç´¢æ¨¡å¼å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "answer": f"å®æ—¶æœç´¢å¤±è´¥: {str(e)}",
                "sources": [],
            }
    
    def _analyze_question_with_ai(self, question: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨AIåˆ†æé—®é¢˜å¹¶æå–æœç´¢å…³é”®è¯å’Œç­–ç•¥ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            åŒ…å«å…³é”®è¯ã€æœç´¢æŸ¥è¯¢å’Œç›¸å…³æ¦‚å¿µçš„å­—å…¸
        """
        try:
            from core.engine.base.llm_service import LLMService
            import json
            import re
            
            llm_service = LLMService()
            
            prompt = f"""åˆ†æä»¥ä¸‹é—®é¢˜ï¼Œæå–ç”¨äºæœç´¢çŸ¥è¯†åº“çš„å…³é”®è¯å’ŒæŸ¥è¯¢ç­–ç•¥ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åˆ†æï¼š
1. é—®é¢˜çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
2. éœ€è¦æœç´¢å“ªäº›å…³é”®è¯ï¼Ÿï¼ˆæå–2-5ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼‰
3. æœ‰å“ªäº›åŒä¹‰è¯æˆ–ç›¸å…³æ¦‚å¿µï¼Ÿ
4. å¯ä»¥å°è¯•å“ªäº›ä¸åŒçš„æœç´¢æŸ¥è¯¢ï¼Ÿï¼ˆç”Ÿæˆ3-5ä¸ªä¸åŒçš„æœç´¢æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åŸé—®é¢˜çš„ä¸åŒè¡¨è¾¾æ–¹å¼ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
    "search_queries": ["æœç´¢æŸ¥è¯¢1", "æœç´¢æŸ¥è¯¢2", "æœç´¢æŸ¥è¯¢3"],
    "related_concepts": ["ç›¸å…³æ¦‚å¿µ1", "ç›¸å…³æ¦‚å¿µ2"]
}}

è¦æ±‚ï¼š
- keywordsï¼šæå–çš„æ ¸å¿ƒå…³é”®è¯ï¼Œå»é™¤ç–‘é—®è¯ï¼ˆä»€ä¹ˆã€å¦‚ä½•ã€æ€ä¹ˆç­‰ï¼‰
- search_queriesï¼šå¤šä¸ªæœç´¢æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åŸé—®é¢˜çš„ä¸åŒè¡¨è¾¾æ–¹å¼ã€ç®€åŒ–ç‰ˆæœ¬ã€å…³é”®è¯ç»„åˆç­‰
- related_conceptsï¼šç›¸å…³æ¦‚å¿µæˆ–åŒä¹‰è¯

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
"""
            
            log.info("ä½¿ç”¨AIåˆ†æé—®é¢˜å¹¶æå–æœç´¢ç­–ç•¥...")
            response = llm_service.generate(prompt)
            
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            json_match = re.search(r'\{[^{}]*"keywords"[^{}]*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•è§£ææ•´ä¸ªå“åº”
                json_str = response.strip()
                # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                json_str = re.sub(r'```json\s*', '', json_str)
                json_str = re.sub(r'```\s*', '', json_str)
                json_str = json_str.strip()
            
            try:
                result = json.loads(json_str)
                
                # éªŒè¯å’Œæ¸…ç†ç»“æœ
                keywords = result.get("keywords", [])
                search_queries = result.get("search_queries", [])
                related_concepts = result.get("related_concepts", [])
                
                # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæœç´¢æŸ¥è¯¢
                if not search_queries:
                    search_queries = [question]
                else:
                    # ç¡®ä¿åŸé—®é¢˜åœ¨æœç´¢æŸ¥è¯¢ä¸­
                    if question not in search_queries:
                        search_queries.insert(0, question)
                
                # é™åˆ¶æ•°é‡
                keywords = keywords[:5]
                search_queries = search_queries[:5]
                related_concepts = related_concepts[:3]
                
                return {
                    "keywords": keywords,
                    "search_queries": search_queries,
                    "related_concepts": related_concepts,
                }
            except json.JSONDecodeError as e:
                log.warning(f"AIè¿”å›çš„JSONè§£æå¤±è´¥: {e}ï¼Œå“åº”: {response[:200]}")
                # å›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®è¯
                return self._fallback_extract_keywords(question)
                
        except Exception as e:
            log.warning(f"AIåˆ†æé—®é¢˜å¤±è´¥: {e}ï¼Œå›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼æå–")
            # å›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®è¯
            return self._fallback_extract_keywords(question)
    
    def _fallback_extract_keywords(self, question: str) -> Dict[str, Any]:
        """
        å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®è¯ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            åŒ…å«å…³é”®è¯å’Œæœç´¢æŸ¥è¯¢çš„å­—å…¸
        """
        keywords = self._extract_keywords(question)
        search_queries = [question] + keywords[:2]
        
        return {
            "keywords": keywords,
            "search_queries": search_queries,
            "related_concepts": [],
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        ä»é—®é¢˜ä¸­æå–å…³é”®è¯ã€‚
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        import re
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™ç©ºæ ¼
        text_clean = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
        
        keywords = []
        
        # æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2-4ä¸ªå­—ç¬¦ï¼Œé¿å…æå–æ•´ä¸ªé—®é¢˜ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', text_clean)
        keywords.extend(chinese_words)
        
        # æå–è‹±æ–‡å•è¯ï¼ˆ3ä¸ªå­—ç¬¦ä»¥ä¸Šï¼‰
        english_words = re.findall(r'\b[a-zA-Z]{3,}\b', text_clean)
        keywords.extend(english_words)
        
        # è¿‡æ»¤å¸¸è§åœç”¨è¯å’Œç–‘é—®è¯
        stop_words = {
            'ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›', 'è¿™ä¸ª', 'é‚£ä¸ª', 
            'æ˜¯', 'çš„', 'äº†', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä¸º',
            'æ˜¯ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›',
            'the', 'is', 'are', 'a', 'an', 'and', 'or', 'what', 'how', 'why'
        }
        keywords = [kw for kw in keywords if kw not in stop_words and len(kw) >= 2]
        
        # è¿›ä¸€æ­¥è¿‡æ»¤ï¼šå¦‚æœå…³é”®è¯åŒ…å«åœç”¨è¯ï¼Œå°è¯•æå–æ ¸å¿ƒéƒ¨åˆ†
        filtered_keywords = []
        for kw in keywords:
            # ç§»é™¤å¸¸è§çš„ç–‘é—®è¯å‰ç¼€/åç¼€
            kw_clean = kw
            for stop in ['ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'æ˜¯', 'çš„']:
                if kw_clean.startswith(stop):
                    kw_clean = kw_clean[len(stop):]
                if kw_clean.endswith(stop):
                    kw_clean = kw_clean[:-len(stop)]
            if kw_clean and len(kw_clean) >= 2 and kw_clean not in stop_words:
                filtered_keywords.append(kw_clean)
        
        keywords = filtered_keywords if filtered_keywords else keywords
        
        # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
        seen = set()
        unique_keywords = []
        for kw in keywords:
            kw_lower = kw.lower()
            if kw_lower not in seen:
                seen.add(kw_lower)
                unique_keywords.append(kw)
        
        # å¦‚æœæå–çš„å…³é”®è¯å¤ªå°‘ï¼Œå°è¯•æå–2-3å­—çš„çŸ­è¯­
        if len(unique_keywords) < 2:
            # æå–2-3å­—çš„ä¸­æ–‡çŸ­è¯­
            phrases = re.findall(r'[\u4e00-\u9fff]{2,3}', text)
            for phrase in phrases:
                if phrase not in stop_words and phrase not in seen:
                    seen.add(phrase.lower())
                    unique_keywords.append(phrase)
                    if len(unique_keywords) >= 3:
                        break
        
        return unique_keywords[:5]  # æœ€å¤šè¿”å›5ä¸ªå…³é”®è¯
    
    def _extract_relevant_chunk(self, content: str, question: str, keywords: List[str], chunk_size: int = 4000) -> str:
        """
        ä»æ–‡æ¡£ä¸­æå–ä¸é—®é¢˜æœ€ç›¸å…³çš„ç‰‡æ®µã€‚
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            question: ç”¨æˆ·é—®é¢˜
            keywords: å…³é”®è¯åˆ—è¡¨
            chunk_size: ç‰‡æ®µå¤§å°ï¼ˆå¢åŠ åˆ°2000ä»¥æä¾›æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
            
        Returns:
            ç›¸å…³ç‰‡æ®µ
        """
        import re
        
        if not content:
            return ""
        
        # å¦‚æœæ–‡æ¡£è¾ƒçŸ­ï¼Œç›´æ¥è¿”å›
        if len(content) <= chunk_size:
            return content
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n+', content)
        
        # è®¡ç®—æ¯ä¸ªæ®µè½çš„ç›¸å…³æ€§åˆ†æ•°
        scored_paragraphs = []
        for para in paragraphs:
            if not para.strip():
                continue
            
            score = 0
            para_lower = para.lower()
            question_lower = question.lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜ä¸­çš„å…³é”®è¯
            for keyword in keywords:
                if keyword.lower() in para_lower:
                    score += 2
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜ä¸­çš„å®Œæ•´çŸ­è¯­
            if question_lower in para_lower:
                score += 5
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜ä¸­çš„éƒ¨åˆ†è¯æ±‡
            question_words = question_lower.split()
            for word in question_words:
                if len(word) >= 2 and word in para_lower:
                    score += 1
            
            if score > 0:
                scored_paragraphs.append((score, para))
        
        # æŒ‰åˆ†æ•°æ’åº
        scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
        
        # é€‰æ‹©æœ€ç›¸å…³çš„æ®µè½ç»„åˆï¼ˆå¢åŠ åˆ°æœ€å¤š15ä¸ªæ®µè½ï¼Œæä¾›æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
        # ä½¿ç”¨embeddingè®¡ç®—æ®µè½ç›¸ä¼¼åº¦ï¼Œè€Œä¸æ˜¯ç®€å•çš„å…³é”®è¯åŒ¹é…
        selected_text = ""
        selected_count = 0
        max_paragraphs = 15  # å¢åŠ æ®µè½æ•°é‡
        
        # å¦‚æœæ®µè½æ•°é‡è¾ƒå¤šï¼Œå°è¯•ä½¿ç”¨embeddingè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ›´å‡†ç¡®ï¼‰
        if len(scored_paragraphs) > 5:
            try:
                from core.engine.base.embedding_service import EmbeddingService
                import numpy as np
                
                embedding_service = EmbeddingService()
                question_vector = np.array(embedding_service.embed_text(question))
                
                # é‡æ–°è®¡ç®—æ¯ä¸ªæ®µè½çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆç»“åˆå…³é”®è¯åŒ¹é…å’Œè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
                enhanced_scored_paragraphs = []
                for score, para in scored_paragraphs[:20]:  # åªå¤„ç†å‰20ä¸ªæ®µè½ä»¥æé«˜æ€§èƒ½
                    # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
                    para_vector = np.array(embedding_service.embed_text(para[:500]))
                    semantic_score = np.dot(question_vector, para_vector) / (
                        np.linalg.norm(question_vector) * np.linalg.norm(para_vector) + 1e-8
                    )
                    # ç»“åˆå…³é”®è¯åŒ¹é…åˆ†æ•°å’Œè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°
                    combined_score = score * 0.4 + semantic_score * 100 * 0.6
                    enhanced_scored_paragraphs.append((combined_score, para))
                
                # é‡æ–°æ’åº
                enhanced_scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
                scored_paragraphs = enhanced_scored_paragraphs
            except Exception as e:
                log.debug(f"ä½¿ç”¨embeddingè®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯åŒ¹é…: {e}")
                # å¦‚æœå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæ¥çš„å…³é”®è¯åŒ¹é…ç»“æœ
        
        # é€‰æ‹©æ®µè½æ—¶ï¼Œä¿ç•™ä¸Šä¸‹æ–‡çª—å£ï¼ˆæ¯ä¸ªç›¸å…³æ®µè½å‰åå„ä¿ç•™ä¸€ä¸ªæ®µè½ï¼‰
        selected_indices = set()
        
        # æ„å»ºæ®µè½åˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼‰
        para_to_index = {}
        for idx, orig_para in enumerate(paragraphs):
            if orig_para.strip():
                para_key = orig_para.strip()[:100]  # ä½¿ç”¨å‰100å­—ç¬¦ä½œä¸ºkey
                if para_key not in para_to_index:
                    para_to_index[para_key] = []
                para_to_index[para_key].append(idx)
        
        # é€‰æ‹©æœ€ç›¸å…³çš„æ®µè½åŠå…¶ä¸Šä¸‹æ–‡
        for score, para in scored_paragraphs[:max_paragraphs]:
            # æ‰¾åˆ°è¿™ä¸ªæ®µè½åœ¨åŸæ–‡ä¸­çš„ç´¢å¼•
            para_key = para.strip()[:100]
            para_indices = para_to_index.get(para_key, [])
            
            if para_indices:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„ç´¢å¼•
                para_index = para_indices[0]
                # é€‰æ‹©å½“å‰æ®µè½åŠå…¶å‰åå„ä¸€ä¸ªæ®µè½ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰
                for ctx_idx in range(max(0, para_index - 1), min(len(paragraphs), para_index + 2)):
                    selected_indices.add(ctx_idx)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                para_stripped = para.strip()
                for idx, orig_para in enumerate(paragraphs):
                    if orig_para.strip() and para_stripped[:50] in orig_para.strip():
                        para_index = idx
                        # é€‰æ‹©å½“å‰æ®µè½åŠå…¶å‰åå„ä¸€ä¸ªæ®µè½
                        for ctx_idx in range(max(0, para_index - 1), min(len(paragraphs), para_index + 2)):
                            selected_indices.add(ctx_idx)
                        break
        
        # æŒ‰é¡ºåºæå–é€‰ä¸­çš„æ®µè½
        for idx in sorted(selected_indices):
            para = paragraphs[idx]
            if not para.strip():
                continue
            
            if len(selected_text) + len(para) <= chunk_size:
                selected_text += para + "\n\n"
            else:
                # å¦‚æœè¶…è¿‡é•¿åº¦é™åˆ¶ï¼Œå°è¯•æˆªå–éƒ¨åˆ†
                remaining = chunk_size - len(selected_text)
                if remaining > 200:  # è‡³å°‘ä¿ç•™200å­—ç¬¦
                    selected_text += para[:remaining] + "..."
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ®µè½ï¼Œè¿”å›å¼€å¤´éƒ¨åˆ†ï¼ˆå¢åŠ é•¿åº¦ï¼‰
        if not selected_text:
            # è¿”å›æ›´å¤šå†…å®¹ï¼ŒåŒ…æ‹¬æ–‡æ¡£å¼€å¤´
            selected_text = content[:chunk_size] + "..."
        
        return selected_text.strip()
    
    def _calculate_similarity(self, question: str, content: str) -> float:
        """
        è®¡ç®—é—®é¢˜å’Œæ–‡æ¡£å†…å®¹çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨embeddingï¼‰ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            content: æ–‡æ¡£å†…å®¹
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        try:
            from core.engine.base.embedding_service import EmbeddingService
            import numpy as np
            
            # åˆå§‹åŒ–embeddingæœåŠ¡
            embedding_service = EmbeddingService()
            
            # å‘é‡åŒ–é—®é¢˜å’Œå†…å®¹
            # æ³¨æ„ï¼šcontentåº”è¯¥æ˜¯å·²ç»æå–çš„ç›¸å…³ç‰‡æ®µï¼Œä¸éœ€è¦å†æˆªå–å‰500å­—ç¬¦
            # å¦‚æœcontentå¤ªé•¿ï¼ˆè¶…è¿‡2000å­—ç¬¦ï¼‰ï¼Œæˆªå–å‰2000å­—ç¬¦ä»¥æé«˜æ€§èƒ½
            content_to_embed = content[:2000] if len(content) > 2000 else content
            question_vector = np.array(embedding_service.embed_text(question))
            content_vector = np.array(embedding_service.embed_text(content_to_embed))
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(question_vector, content_vector)
            norm_q = np.linalg.norm(question_vector)
            norm_c = np.linalg.norm(content_vector)
            
            if norm_q == 0 or norm_c == 0:
                return 0.0
            
            similarity = dot_product / (norm_q * norm_c)
            
            # ç¡®ä¿ç›¸ä¼¼åº¦åœ¨0-1èŒƒå›´å†…
            return max(0.0, min(1.0, float(similarity)))
            
        except Exception as e:
            log.warning(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼ŒåŸºäºå…³é”®è¯åŒ¹é…è¿”å›ä¸€ä¸ªä¼°è®¡å€¼
            keywords = self._extract_keywords(question)
            content_lower = content.lower()
            match_count = sum(1 for kw in keywords if kw.lower() in content_lower)
            return min(0.8, 0.3 + match_count * 0.1)  # åŸºç¡€åˆ†æ•°0.3ï¼Œæ¯ä¸ªå…³é”®è¯åŒ¹é…+0.1
    
    def _analyze_search_results_with_ai(
        self, 
        question: str, 
        search_results: List[Dict[str, Any]], 
        keywords: List[str],
        related_concepts: List[str]
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨AIåˆ†ææœç´¢ç»“æœçš„ç›¸å…³æ€§å’Œå…³é”®ä¿¡æ¯ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            keywords: æå–çš„å…³é”®è¯
            related_concepts: ç›¸å…³æ¦‚å¿µ
            
        Returns:
            åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
            - relevance_summary: ç›¸å…³æ€§æ€»ç»“
            - key_points: å…³é”®ä¿¡æ¯ç‚¹
            - answer_strategy: ç­”æ¡ˆç”Ÿæˆç­–ç•¥
        """
        try:
            from core.engine.base.llm_service import LLMService
            import json
            import re
            
            llm_service = LLMService()
            
            # æ„å»ºæœç´¢ç»“æœæ‘˜è¦ï¼ˆåªåŒ…å«æ ‡é¢˜å’Œç›¸ä¼¼åº¦ï¼Œé¿å…tokenè¿‡å¤šï¼‰
            results_summary = []
            for i, result in enumerate(search_results[:5], 1):
                results_summary.append({
                    "åºå·": i,
                    "æ ‡é¢˜": result.get("title", "æœªçŸ¥"),
                    "ç›¸ä¼¼åº¦": f"{result.get('similarity', 0):.2f}",
                    "å†…å®¹æ‘˜è¦": result.get("content", "")[:200] + "..." if len(result.get("content", "")) > 200 else result.get("content", "")
                })
            
            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œéœ€è¦åˆ†ææœç´¢ç»“æœä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æå–çš„å…³é”®è¯ã€‘
{', '.join(keywords) if keywords else 'æ— '}

ã€ç›¸å…³æ¦‚å¿µã€‘
{', '.join(related_concepts) if related_concepts else 'æ— '}

ã€æœç´¢ç»“æœã€‘
{json.dumps(results_summary, ensure_ascii=False, indent=2)}

è¯·åˆ†æï¼š
1. è¿™äº›æœç´¢ç»“æœä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§å¦‚ä½•ï¼Ÿ
2. å“ªäº›ç»“æœæœ€ç›¸å…³ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
3. ä»è¿™äº›ç»“æœä¸­å¯ä»¥æå–å“ªäº›å…³é”®ä¿¡æ¯ç‚¹ï¼Ÿ
4. åº”è¯¥å¦‚ä½•ç»„ç»‡ç­”æ¡ˆï¼Ÿï¼ˆç›´æ¥å›ç­”ã€åˆ†ç‚¹è¯´æ˜ã€å¯¹æ¯”è¯´æ˜ç­‰ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "relevance_summary": "ç›¸å…³æ€§æ€»ç»“ï¼ˆ1-2å¥è¯ï¼‰",
    "key_points": ["å…³é”®ä¿¡æ¯ç‚¹1", "å…³é”®ä¿¡æ¯ç‚¹2", "å…³é”®ä¿¡æ¯ç‚¹3"],
    "answer_strategy": "ç­”æ¡ˆç”Ÿæˆç­–ç•¥ï¼ˆå¦‚ï¼šç›´æ¥å›ç­”ã€åˆ†ç‚¹è¯´æ˜ã€å¯¹æ¯”è¯´æ˜ç­‰ï¼‰",
    "most_relevant_results": [1, 2]  // æœ€ç›¸å…³çš„ç»“æœåºå·åˆ—è¡¨
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
"""
            
            log.info("ä½¿ç”¨AIåˆ†ææœç´¢ç»“æœ...")
            response = llm_service.generate(prompt)
            
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            json_match = re.search(r'\{[^{}]*"relevance_summary"[^{}]*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                json_str = response.strip()
                json_str = re.sub(r'```json\s*', '', json_str)
                json_str = re.sub(r'```\s*', '', json_str)
                json_str = json_str.strip()
            
            try:
                result = json.loads(json_str)
                return result
            except json.JSONDecodeError as e:
                log.warning(f"AIåˆ†æç»“æœJSONè§£æå¤±è´¥: {e}ï¼Œå“åº”: {response[:200]}")
                return {
                    "relevance_summary": "æœç´¢ç»“æœä¸é—®é¢˜ç›¸å…³",
                    "key_points": [],
                    "answer_strategy": "ç›´æ¥å›ç­”",
                    "most_relevant_results": [1, 2, 3],
                }
                
        except Exception as e:
            log.warning(f"AIåˆ†ææœç´¢ç»“æœå¤±è´¥: {e}")
            return {
                "relevance_summary": "æœç´¢ç»“æœä¸é—®é¢˜ç›¸å…³",
                "key_points": [],
                "answer_strategy": "ç›´æ¥å›ç­”",
                "most_relevant_results": [1, 2, 3],
            }
    
    def _build_answer_prompt(
        self,
        question: str,
        context: str,
        search_results: List[Dict[str, Any]],
        analysis_result: Dict[str, Any],
        keywords: List[str]
    ) -> str:
        """
        æ„å»ºç­”æ¡ˆç”Ÿæˆçš„Promptï¼Œè®©AIæ›´å¥½åœ°åˆ©ç”¨æœç´¢ç»“æœã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            analysis_result: AIåˆ†æç»“æœ
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            å®Œæ•´çš„Prompt
        """
        # æ„å»ºæœç´¢ç»“æœä¿¡æ¯
        results_info = []
        for i, result in enumerate(search_results, 1):
            similarity = result.get("similarity", 0)
            title = result.get("title", "æœªçŸ¥")
            results_info.append(f"{i}. {title} (ç›¸ä¼¼åº¦: {similarity:.2f})")
        
        results_summary = "\n".join(results_info)
        
        # è·å–AIåˆ†æçš„å…³é”®ä¿¡æ¯
        relevance_summary = analysis_result.get("relevance_summary", "")
        key_points = analysis_result.get("key_points", [])
        answer_strategy = analysis_result.get("answer_strategy", "ç›´æ¥å›ç­”")
        most_relevant = analysis_result.get("most_relevant_results", [])
        
        # æ„å»ºå…³é”®ä¿¡æ¯ç‚¹
        key_points_text = ""
        if key_points:
            key_points_text = "\n".join([f"- {point}" for point in key_points[:5]])
        
        # æ„å»ºæœ€ç›¸å…³ç»“æœæç¤º
        most_relevant_text = ""
        if most_relevant:
            most_relevant_titles = [
                search_results[i-1].get("title", "") 
                for i in most_relevant 
                if 1 <= i <= len(search_results)
            ]
            if most_relevant_titles:
                most_relevant_text = f"\nã€æœ€ç›¸å…³æ–‡æ¡£ã€‘ä¼˜å…ˆå‚è€ƒä»¥ä¸‹æ–‡æ¡£ï¼š{', '.join(most_relevant_titles)}"
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„AIçŸ¥è¯†åº“åŠ©æ‰‹ï¼Œæ“…é•¿æ·±å…¥åˆ†ææ–‡æ¡£å†…å®¹å¹¶ç”Ÿæˆé«˜è´¨é‡ã€ç»“æ„åŒ–çš„ç­”æ¡ˆã€‚

ã€ä»»åŠ¡ã€‘
åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæ·±å…¥åˆ†æå¹¶å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ä½ éœ€è¦ï¼š
1. **æ·±å…¥ç†è§£**ï¼šä»”ç»†é˜…è¯»æ–‡æ¡£å†…å®¹ï¼Œç†è§£ä¸Šä¸‹æ–‡å’Œç»†èŠ‚
2. **æå–å…³é”®ä¿¡æ¯**ï¼šè¯†åˆ«ä¸é—®é¢˜ç›¸å…³çš„æ ¸å¿ƒä¿¡æ¯ã€å…³é”®æ­¥éª¤ã€é‡è¦æ¦‚å¿µ
3. **ç»¼åˆåˆ†æ**ï¼šå¦‚æœæ¶‰åŠå¤šä¸ªæ–‡æ¡£ï¼Œè¦ç»¼åˆä¸åŒæ–‡æ¡£çš„ä¿¡æ¯ï¼Œå½¢æˆå®Œæ•´çš„ç­”æ¡ˆ
4. **ç»“æ„åŒ–ç»„ç»‡**ï¼šæŒ‰ç…§é€»è¾‘é¡ºåºç»„ç»‡ç­”æ¡ˆï¼Œä½¿ç”¨æ¸…æ™°çš„æ®µè½å’Œåˆ†ç‚¹è¯´æ˜
5. **æ·±å…¥é˜è¿°**ï¼šä¸ä»…è¦å¼•ç”¨æ–‡æ¡£å†…å®¹ï¼Œè¿˜è¦è¿›è¡Œè§£é‡Šã€åˆ†æå’Œæ€»ç»“

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æå–çš„å…³é”®è¯ã€‘
{', '.join(keywords) if keywords else 'æ— '}

ã€æœç´¢ç»“æœåˆ†æã€‘
{relevance_summary if relevance_summary else 'æœç´¢ç»“æœä¸é—®é¢˜ç›¸å…³'}

ã€å…³é”®ä¿¡æ¯ç‚¹ã€‘
{key_points_text if key_points_text else 'éœ€è¦ä»æ–‡æ¡£ä¸­æå–'}

ã€ç­”æ¡ˆç”Ÿæˆç­–ç•¥ã€‘
{answer_strategy}{most_relevant_text}

ã€æ–‡æ¡£å†…å®¹ã€‘
{context}

ã€æœç´¢ç»“æœåˆ—è¡¨ã€‘
{results_summary}

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. **æ·±åº¦åˆ†æ**ï¼š
   - ä¸è¦ç®€å•å¼•ç”¨æ–‡æ¡£ä¸­çš„ä¸€ä¸¤å¥è¯
   - è¦æ·±å…¥ç†è§£æ–‡æ¡£å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶è¿›è¡Œè§£é‡Š
   - å¦‚æœæ–‡æ¡£æåˆ°æŸä¸ªåŠŸèƒ½æˆ–æ¦‚å¿µï¼Œè¦è¯¦ç»†è¯´æ˜å…¶ä½œç”¨ã€ä½¿ç”¨æ–¹æ³•ã€æ³¨æ„äº‹é¡¹ç­‰

2. **å®Œæ•´æ€§**ï¼š
   - å¦‚æœæ–‡æ¡£ä¸­æœ‰å¤šä¸ªç›¸å…³ä¿¡æ¯ç‚¹ï¼Œè¦å…¨éƒ¨æå–å¹¶ç»¼åˆå›ç­”
   - ä¸è¦é—æ¼é‡è¦çš„ç»†èŠ‚ã€æ­¥éª¤ã€æ¡ä»¶ã€é™åˆ¶ç­‰
   - å¦‚æœæ¶‰åŠå¤šä¸ªæ–¹é¢ï¼Œè¦å…¨é¢è¦†ç›–

3. **ç»“æ„åŒ–ç»„ç»‡**ï¼š
   - ä½¿ç”¨æ¸…æ™°çš„æ®µè½ç»“æ„
   - å¯¹äºå¤æ‚é—®é¢˜ï¼Œä½¿ç”¨åˆ†ç‚¹è¯´æ˜ï¼ˆ1. 2. 3.ï¼‰æˆ–åˆ†ç±»è¯´æ˜
   - æŒ‰ç…§é€»è¾‘é¡ºåºç»„ç»‡ï¼šæ¦‚è¿° â†’ è¯¦ç»†è¯´æ˜ â†’ æ€»ç»“

4. **å¯è¯»æ€§å’Œä¸“ä¸šæ€§**ï¼š
   - ä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼Œè¯­è¨€æµç•…è‡ªç„¶
   - ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œä½†ç¡®ä¿æ˜“äºç†è§£
   - é¿å…å†—ä½™å’Œé‡å¤
   - é€‚å½“ä½¿ç”¨è¿‡æ¸¡è¯ï¼Œä½¿ç­”æ¡ˆè¿è´¯

5. **å¼•ç”¨å’Œæ ‡æ³¨**ï¼š
   - åœ¨ç­”æ¡ˆå¼€å¤´æˆ–å…³é”®éƒ¨åˆ†æåŠæ–‡æ¡£æ¥æºï¼ˆå¦‚"æ ¹æ®ã€ŠXXXæ–‡æ¡£ã€‹..."ï¼‰
   - å¦‚æœä¿¡æ¯æ¥è‡ªå¤šä¸ªæ–‡æ¡£ï¼Œå¯ä»¥åˆ†åˆ«æ ‡æ³¨

ã€ç­”æ¡ˆç»“æ„å»ºè®®ã€‘
- **å¼€å¤´**ï¼šç®€è¦è¯´æ˜æ‰¾åˆ°äº†å“ªäº›ç›¸å…³ä¿¡æ¯ï¼ˆå¯æåŠæ–‡æ¡£åç§°ï¼‰
- **ä¸»ä½“**ï¼šè¯¦ç»†å›ç­”é—®é¢˜çš„å„ä¸ªæ–¹é¢ï¼Œä½¿ç”¨åˆ†ç‚¹æˆ–åˆ†æ®µè¯´æ˜
- **ç»“å°¾**ï¼šå¦‚æœ‰å¿…è¦ï¼Œè¿›è¡Œæ€»ç»“æˆ–è¡¥å……è¯´æ˜

ã€æ³¨æ„äº‹é¡¹ã€‘
- å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›´æ¥å›ç­”é—®é¢˜çš„ä¿¡æ¯ï¼Œå¯ä»¥åŸºäºç›¸å…³å†…å®¹è¿›è¡Œåˆç†æ¨æ–­ï¼Œä½†è¦è¯´æ˜è¿™æ˜¯åŸºäºæ–‡æ¡£çš„æ¨æ–­
- å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜ä¸å®Œå…¨åŒ¹é…ï¼Œè¯´æ˜æ–‡æ¡£ä¸­æ‰¾åˆ°äº†å“ªäº›ç›¸å…³ä¿¡æ¯ï¼Œå¹¶è§£é‡Šè¿™äº›ä¿¡æ¯å¦‚ä½•å¸®åŠ©å›ç­”é—®é¢˜
- å¦‚æœå¤šä¸ªæ–‡æ¡£æœ‰å†²çªä¿¡æ¯ï¼Œè¦å¯¹æ¯”è¯´æ˜å¹¶æŒ‡å‡ºå·®å¼‚
- å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

ã€ç­”æ¡ˆã€‘
è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£å†…å®¹ï¼Œæ·±å…¥åˆ†æå¹¶å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¦æ±‚ç­”æ¡ˆå®Œæ•´ã€æ·±å…¥ã€æœ‰æ¡ç†ï¼š
"""
        
        return prompt

    def get_collection_info(self) -> Dict[str, Any]:
        """
        è·å–å‘é‡å­˜å‚¨ä¿¡æ¯ã€‚

        Returns:
            é›†åˆä¿¡æ¯
        """
        try:
            info = self.rag_engine.vector_store.get_collection_info()
            return {
                "success": True,
                "info": info,
            }
        except Exception as e:
            log.error(f"è·å–é›†åˆä¿¡æ¯å¤±è´¥: {e}")
            return {
                "success": False,
                "info": {},
            }

