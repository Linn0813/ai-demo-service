# encoding: utf-8
"""
çŸ¥è¯†åº“æœåŠ¡å±‚ï¼Œå°è£…çŸ¥è¯†åº“ç›¸å…³ä¸šåŠ¡é€»è¾‘ã€‚
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional
from pathlib import Path
from datetime import datetime
import json

from infrastructure.external.feishu.loader import FeishuDocumentLoader
from infrastructure.vector_store.chroma import VectorStore
from domain.knowledge_base.rag import RAGEngine
from shared.logger import log

class KnowledgeBaseService:
    """çŸ¥è¯†åº“æœåŠ¡ï¼Œæä¾›æ–‡æ¡£åŒæ­¥å’Œé—®ç­”åŠŸèƒ½ã€‚"""

    def __init__(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“æœåŠ¡ã€‚"""
        self.document_loader = FeishuDocumentLoader()
        self._rag_engine = None
        self._web_search_service = None
        # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
        project_root = Path(__file__).parent.parent.parent
        self.results_dir = project_root / 'data' / 'query_results'
        self.results_dir.mkdir(parents=True, exist_ok=True)

    @property
    def rag_engine(self) -> RAGEngine:
        """
        è·å–RAGå¼•æ“ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰ã€‚
        
        Returns:
            RAGå¼•æ“å®ä¾‹
            
        Raises:
            ImportError: å¦‚æœç¼ºå°‘å¿…è¦çš„ä¾èµ–
        """
        if self._rag_engine is None:
            try:
                self._rag_engine = RAGEngine()
            except ImportError as e:
                raise ImportError(
                    f"çŸ¥è¯†åº“åŠŸèƒ½ä¸å¯ç”¨ï¼ˆç¼ºå°‘ä¾èµ–ï¼‰: {e}\n"
                    "è¯·å®‰è£…ä¾èµ–: pip install sentence-transformers chromadb"
                ) from e
        return self._rag_engine

    def sync_documents_from_space(self, space_id: str, incremental: bool = True) -> Dict[str, Any]:
        """
        ä»çŸ¥è¯†åº“ç©ºé—´åŒæ­¥æ–‡æ¡£ï¼ˆæ”¯æŒå¢é‡åŒæ­¥ï¼‰ã€‚

        Args:
            space_id: çŸ¥è¯†åº“ç©ºé—´ID
            incremental: æ˜¯å¦ä½¿ç”¨å¢é‡åŒæ­¥ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            åŒæ­¥ç»“æœï¼ŒåŒ…å«åŒæ­¥çš„æ–‡æ¡£æ•°é‡å’ŒçŠ¶æ€
        """
        try:
            log.info(f"å¼€å§‹åŒæ­¥çŸ¥è¯†åº“ç©ºé—´: {space_id} (å¢é‡æ¨¡å¼: {incremental})")

            # åŠ è½½æ‰€æœ‰æ–‡æ¡£
            documents = self.document_loader.load_all_documents_from_space(space_id)

            if not documents:
                return {
                    "success": False,
                    "message": "æœªæ‰¾åˆ°æ–‡æ¡£",
                    "document_count": 0,
                    "new_count": 0,
                    "updated_count": 0,
                    "skipped_count": 0,
                }

            # å¢é‡åŒæ­¥ï¼šè·å–å·²æœ‰æ–‡æ¡£çš„æ›´æ–°æ—¶é—´
            existing_docs = {}
            if incremental:
                existing_docs = self.rag_engine.vector_store.get_documents_by_space(space_id)
                log.info(f"å‘é‡åº“ä¸­å·²æœ‰ {len(existing_docs)} ä¸ªæ–‡æ¡£")
            
            # è¾…åŠ©å‡½æ•°ï¼šæ¯”è¾ƒæ›´æ–°æ—¶é—´
            def compare_update_time(time1: Any, time2: Any) -> int:
                """
                æ¯”è¾ƒä¸¤ä¸ªæ›´æ–°æ—¶é—´ï¼Œè¿”å›ï¼š
                -1: time1 < time2
                 0: time1 == time2
                 1: time1 > time2
                """
                if not time1 or not time2:
                    return 0  # å¦‚æœä»»ä¸€æ—¶é—´ä¸ºç©ºï¼Œè®¤ä¸ºç›¸ç­‰ï¼ˆéœ€è¦åŒæ­¥ï¼‰
                
                # è½¬æ¢ä¸ºæ•´æ•°æ—¶é—´æˆ³è¿›è¡Œæ¯”è¾ƒ
                try:
                    t1 = int(time1) if isinstance(time1, (int, str)) else 0
                    t2 = int(time2) if isinstance(time2, (int, str)) else 0
                    if t1 < t2:
                        return -1
                    elif t1 > t2:
                        return 1
                    else:
                        return 0
                except (ValueError, TypeError):
                    # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè®¤ä¸ºéœ€è¦åŒæ­¥
                    return 0

            # å‡†å¤‡æ–‡æ¡£æ•°æ®ï¼ˆåªåŒæ­¥æ–°å¢æˆ–æ›´æ–°çš„æ–‡æ¡£ï¼‰
            doc_data = []
            new_count = 0
            updated_count = 0
            skipped_count = 0
            current_doc_tokens = set()

            for doc in documents:
                doc_token = doc["token"]
                current_doc_tokens.add(doc_token)
                doc_update_time = doc["meta"].get("update_time")
                
                # å¢é‡åŒæ­¥ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                if incremental and doc_token in existing_docs:
                    existing_update_time = existing_docs[doc_token].get("update_time")
                    # æ¯”è¾ƒæ›´æ–°æ—¶é—´
                    cmp_result = compare_update_time(doc_update_time, existing_update_time)
                    if cmp_result <= 0:  # æ–‡æ¡£æœªæ›´æ–°æˆ–æ—¶é—´ç›¸åŒ
                        skipped_count += 1
                        log.debug(f"è·³è¿‡æœªæ›´æ–°çš„æ–‡æ¡£: {doc['meta'].get('title', 'æœªçŸ¥')} (æ›´æ–°æ—¶é—´: {doc_update_time})")
                        continue
                    updated_count += 1
                    log.debug(f"æ–‡æ¡£å·²æ›´æ–°: {doc['meta'].get('title', 'æœªçŸ¥')} (æ—§: {existing_update_time}, æ–°: {doc_update_time})")
                else:
                    new_count += 1

                doc_data.append({
                    "id": doc_token,
                    "content": doc["content"],
                    "metadata": {
                        "title": doc["meta"].get("title", "æœªçŸ¥æ ‡é¢˜"),
                        "url": doc["meta"].get("url", ""),
                        "space_id": space_id,
                        "document_id": doc["meta"].get("document_id", ""),
                        "update_time": doc_update_time,  # æ·»åŠ æ›´æ–°æ—¶é—´
                    },
                })

            # åˆ é™¤å·²ä¸å­˜åœ¨çš„æ–‡æ¡£ï¼ˆå¢é‡åŒæ­¥æ—¶ï¼‰
            deleted_count = 0
            if incremental and existing_docs:
                deleted_tokens = set(existing_docs.keys()) - current_doc_tokens
                if deleted_tokens:
                    log.info(f"å‘ç° {len(deleted_tokens)} ä¸ªå·²åˆ é™¤çš„æ–‡æ¡£ï¼Œå‡†å¤‡æ¸…ç†...")
                    for deleted_token in deleted_tokens:
                        # åˆ é™¤è¯¥æ–‡æ¡£çš„æ‰€æœ‰chunkï¼ˆchunk_idæ ¼å¼ï¼š{token}_chunk_{idx}ï¼‰
                        try:
                            # æŸ¥è¯¢è¯¥æ–‡æ¡£çš„æ‰€æœ‰chunk
                            all_docs = self.rag_engine.vector_store._collection.get(
                                where={"space_id": space_id}
                            )
                            chunk_ids_to_delete = [
                                doc_id for doc_id in all_docs.get("ids", [])
                                if doc_id.startswith(f"{deleted_token}_chunk_") or doc_id == deleted_token
                            ]
                            if chunk_ids_to_delete:
                                self.rag_engine.vector_store.delete(ids=chunk_ids_to_delete)
                                deleted_count += 1
                                log.info(f"å·²åˆ é™¤æ–‡æ¡£: {deleted_token}")
                        except Exception as e:
                            log.warning(f"åˆ é™¤æ–‡æ¡£å¤±è´¥ {deleted_token}: {e}")

            # å¦‚æœæœ‰éœ€è¦åŒæ­¥çš„æ–‡æ¡£ï¼Œå…ˆåˆ é™¤æ—§ç‰ˆæœ¬å†ç´¢å¼•æ–°ç‰ˆæœ¬
            if doc_data:
                # å…ˆåˆ é™¤éœ€è¦æ›´æ–°çš„æ–‡æ¡£çš„æ—§ç‰ˆæœ¬
                if incremental:
                    tokens_to_update = {doc["id"] for doc in doc_data}
                    for token in tokens_to_update:
                        try:
                            all_docs = self.rag_engine.vector_store._collection.get(
                                where={"space_id": space_id}
                            )
                            chunk_ids_to_delete = [
                                doc_id for doc_id in all_docs.get("ids", [])
                                if doc_id.startswith(f"{token}_chunk_") or doc_id == token
                            ]
                            if chunk_ids_to_delete:
                                self.rag_engine.vector_store.delete(ids=chunk_ids_to_delete)
                        except Exception as e:
                            log.warning(f"åˆ é™¤æ—§ç‰ˆæœ¬å¤±è´¥ {token}: {e}")

                # ç´¢å¼•æ–‡æ¡£
                indexed_count = self.rag_engine.index_documents(doc_data)
            else:
                indexed_count = 0
                log.info("æ²¡æœ‰éœ€è¦åŒæ­¥çš„æ–‡æ¡£")

            return {
                "success": True,
                "message": "åŒæ­¥æˆåŠŸ",
                "document_count": len(documents),
                "new_count": new_count,
                "updated_count": updated_count,
                "skipped_count": skipped_count,
                "deleted_count": deleted_count,
                "indexed_count": indexed_count,
            }

        except Exception as e:
            log.error(f"åŒæ­¥æ–‡æ¡£å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"åŒæ­¥å¤±è´¥: {str(e)}",
                "document_count": 0,
                "new_count": 0,
                "updated_count": 0,
                "skipped_count": 0,
                "deleted_count": 0,
            }

    def sync_all_spaces(self, incremental: bool = True) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰€æœ‰çŸ¥è¯†åº“ç©ºé—´ã€‚

        Args:
            incremental: æ˜¯å¦ä½¿ç”¨å¢é‡åŒæ­¥ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            åŒæ­¥ç»“æœ
        """
        try:
            # è·å–æ‰€æœ‰çŸ¥è¯†åº“ç©ºé—´
            spaces = self.document_loader.load_wiki_spaces()

            total_documents = 0
            total_new = 0
            total_updated = 0
            total_skipped = 0
            total_deleted = 0
            success_count = 0
            failed_spaces = []

            for space in spaces:
                space_id = space.get("space_id", "")
                space_name = space.get("name", "æœªçŸ¥")

                if not space_id:
                    continue

                log.info(f"åŒæ­¥çŸ¥è¯†åº“ç©ºé—´: {space_name} ({space_id})")

                result = self.sync_documents_from_space(space_id, incremental=incremental)
                if result["success"]:
                    success_count += 1
                    total_documents += result["document_count"]
                    total_new += result.get("new_count", 0)
                    total_updated += result.get("updated_count", 0)
                    total_skipped += result.get("skipped_count", 0)
                    total_deleted += result.get("deleted_count", 0)
                else:
                    failed_spaces.append({
                        "space_id": space_id,
                        "name": space_name,
                        "error": result["message"],
                    })

            sync_mode = "å¢é‡" if incremental else "å…¨é‡"
            return {
                "success": True,
                "message": f"åŒæ­¥å®Œæˆï¼ˆ{sync_mode}æ¨¡å¼ï¼‰ï¼šæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {len(failed_spaces)} ä¸ª",
                "total_spaces": len(spaces),
                "success_count": success_count,
                "failed_count": len(failed_spaces),
                "total_documents": total_documents,
                "new_count": total_new,
                "updated_count": total_updated,
                "skipped_count": total_skipped,
                "deleted_count": total_deleted,
                "failed_spaces": failed_spaces,
            }

        except Exception as e:
            error_msg = str(e)
            log.error(f"åŒæ­¥æ‰€æœ‰çŸ¥è¯†åº“å¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æƒé™é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™é‡æ–°æŠ›å‡ºä»¥ä¾¿APIå±‚å¤„ç†
            is_auth_error = (
                "99991672" in error_msg or 
                "99991663" in error_msg or 
                "99991664" in error_msg or 
                "99991679" in error_msg or
                "æƒé™" in error_msg or 
                "Access denied" in error_msg or
                "unauthorized" in error_msg.lower() or
                "forbidden" in error_msg.lower()
            )
            if is_auth_error:
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©APIå±‚è¿”å›403
            
            return {
                "success": False,
                "message": f"åŒæ­¥å¤±è´¥: {error_msg}",
                "total_spaces": 0,
                "success_count": 0,
                "failed_count": 0,
                "total_documents": 0,
            }

    @property
    def web_search_service(self):
        """è·å–ç½‘ç»œæœç´¢æœåŠ¡ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self._web_search_service is None:
            try:
                from infrastructure.external.web_search import WebSearchService
                self._web_search_service = WebSearchService()
            except Exception as e:
                log.warning(f"ç½‘ç»œæœç´¢æœåŠ¡ä¸å¯ç”¨: {e}")
                self._web_search_service = None
        return self._web_search_service

    def ask(self, question: str, use_realtime_search: bool = True, space_id: Optional[str] = None, use_web_search: bool = False) -> Dict[str, Any]:
        """
        å›ç­”é—®é¢˜ã€‚
        
        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        1. å®æ—¶æœç´¢æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼šç›´æ¥ä½¿ç”¨é£ä¹¦APIæœç´¢ï¼Œæ— éœ€å…ˆåŒæ­¥æ–‡æ¡£
        2. å‘é‡æœç´¢æ¨¡å¼ï¼šä½¿ç”¨æœ¬åœ°å‘é‡å­˜å‚¨è¿›è¡Œè¯­ä¹‰æœç´¢ï¼ˆéœ€è¦å…ˆåŒæ­¥æ–‡æ¡£ï¼‰

        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_realtime_search: æ˜¯å¦ä½¿ç”¨å®æ—¶æœç´¢æ¨¡å¼ï¼ˆé»˜è®¤Trueï¼‰
            space_id: æŒ‡å®šæœç´¢çš„çŸ¥è¯†åº“ç©ºé—´IDï¼Œå¦‚æœä¸æä¾›åˆ™æœç´¢æ‰€æœ‰ç©ºé—´
            use_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢ï¼ˆé»˜è®¤Falseï¼‰ã€‚å½“çŸ¥è¯†åº“ç»“æœä¸ç†æƒ³æ—¶ï¼Œä¼šä½¿ç”¨ç½‘ç»œæœç´¢è¡¥å……

        Returns:
            ç­”æ¡ˆå’Œå¼•ç”¨æ¥æº
        """
        try:
            # æ£€æŸ¥å‘é‡å­˜å‚¨ä¸­æ˜¯å¦æœ‰æ–‡æ¡£
            collection_info = self.get_collection_info()
            has_local_docs = (
                collection_info.get("success") 
                and collection_info.get("info", {}).get("count", 0) > 0
            )
            
            # å¦‚æœå‘é‡å­˜å‚¨ä¸ºç©ºï¼Œè‡ªåŠ¨ä½¿ç”¨å®æ—¶æœç´¢æ¨¡å¼
            if not has_local_docs:
                log.info("å‘é‡å­˜å‚¨ä¸ºç©ºï¼Œä½¿ç”¨å®æ—¶æœç´¢æ¨¡å¼")
                use_realtime_search = True
            
            if use_realtime_search:
                kb_result = self._ask_with_realtime_search(question, space_id=space_id)
                
                # è®¡ç®—æœ€é«˜ç›¸ä¼¼åº¦ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
                sources = kb_result.get("sources", [])
                max_similarity = max([s.get("similarity", 0) for s in sources]) if sources else 0.0
                
                # åˆ¤æ–­æ˜¯å¦å»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢
                suggest_web_search = self._should_use_web_search(question, kb_result)
                
                # å¦‚æœå¯ç”¨äº†ç½‘ç»œæœç´¢ï¼Œä¸”çŸ¥è¯†åº“ç»“æœä¸ç†æƒ³ï¼Œå°è¯•ç½‘ç»œæœç´¢
                if use_web_search and suggest_web_search:
                    log.info("ğŸŒ çŸ¥è¯†åº“ç»“æœä¸ç†æƒ³ï¼Œå°è¯•ä½¿ç”¨ç½‘ç»œæœç´¢è¡¥å……...")
                    web_result = self._search_web_and_merge(question, kb_result)
                    return web_result
                
                # å¦‚æœæœªå¯ç”¨ç½‘ç»œæœç´¢ï¼Œä½†å»ºè®®ä½¿ç”¨ï¼Œåœ¨ç»“æœä¸­æ·»åŠ å»ºè®®ä¿¡æ¯
                if not use_web_search:
                    kb_result["suggest_web_search"] = suggest_web_search
                    kb_result["max_similarity"] = max_similarity
                    if suggest_web_search:
                        log.info(f"ğŸ’¡ å»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}ï¼‰")
                
                return kb_result
            else:
                # ä½¿ç”¨å‘é‡æœç´¢æ¨¡å¼ï¼ˆæš‚ä¸æ”¯æŒæŒ‡å®šspace_idï¼Œæœç´¢æ‰€æœ‰æ–‡æ¡£ï¼‰
                if space_id:
                    log.warning("å‘é‡æœç´¢æ¨¡å¼æš‚ä¸æ”¯æŒæŒ‡å®šçŸ¥è¯†åº“ï¼Œå°†æœç´¢æ‰€æœ‰æ–‡æ¡£")
            result = self.rag_engine.qa(question)
            return {
                "success": True,
                "answer": result["answer"],
                "sources": result["sources"],
            }

        except Exception as e:
            log.error(f"å›ç­”é—®é¢˜å¤±è´¥: {e}")
            return {
                "success": False,
                "answer": f"æŠ±æ­‰ï¼Œå¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "sources": [],
            }
    
    def get_wiki_spaces(self) -> Dict[str, Any]:
        """
        è·å–æ‰€æœ‰çŸ¥è¯†åº“ç©ºé—´åˆ—è¡¨ã€‚
        
        Returns:
            çŸ¥è¯†åº“ç©ºé—´åˆ—è¡¨
        """
        try:
            spaces = self.document_loader.load_wiki_spaces()
            space_list = []
            for space in spaces:
                space_list.append({
                    "space_id": space.get("space_id", ""),
                    "name": space.get("name", "æœªçŸ¥"),
                    "description": space.get("description", ""),
                })
            
            return {
                "success": True,
                "spaces": space_list,
                "message": f"æ‰¾åˆ° {len(space_list)} ä¸ªçŸ¥è¯†åº“ç©ºé—´",
            }
        except Exception as e:
            log.error(f"è·å–çŸ¥è¯†åº“ç©ºé—´åˆ—è¡¨å¤±è´¥: {e}")
            error_msg = str(e)
            # æ£€æŸ¥æ˜¯å¦æ˜¯æƒé™é”™è¯¯ï¼ˆåŒ…æ‹¬å„ç§æƒé™é”™è¯¯ç ï¼‰
            is_auth_error = (
                "99991672" in error_msg or 
                "99991663" in error_msg or 
                "99991664" in error_msg or 
                "99991679" in error_msg or
                "æƒé™" in error_msg or 
                "Access denied" in error_msg or
                "unauthorized" in error_msg.lower() or
                "forbidden" in error_msg.lower()
            )
            if is_auth_error:
                return {
                    "success": False,
                    "spaces": [],
                    "message": f"æƒé™ä¸è¶³: {error_msg}ã€‚è¯·å…ˆè¿›è¡Œé£ä¹¦æˆæƒã€‚",
                }
            return {
                "success": False,
                "spaces": [],
                "message": f"è·å–çŸ¥è¯†åº“ç©ºé—´åˆ—è¡¨å¤±è´¥: {error_msg}",
            }
    
    def _save_query_result(self, question: str, step: str, data: Dict[str, Any], query_timestamp: Optional[str] = None):
        """ä¿å­˜æŸ¥è¯¢ç»“æœåˆ°æ–‡ä»¶"""
        try:
            # å¦‚æœæä¾›äº†query_timestampï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ç”Ÿæˆæ–°çš„
            if query_timestamp is None:
                query_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            filename = f"query_{query_timestamp}.json"
            filepath = self.results_dir / filename
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿½åŠ æ•°æ®ï¼›å¦åˆ™åˆ›å»ºæ–°æ–‡ä»¶
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
            else:
                result_data = {
                    "question": question,
                    "timestamp": query_timestamp,
                    "steps": {}
                }
            
            result_data["steps"][step] = {
                "time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "data": data
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            
            log.info(f"ğŸ’¾ æŸ¥è¯¢ç»“æœå·²ä¿å­˜åˆ°: {filepath} (æ­¥éª¤: {step})")
            return query_timestamp  # è¿”å›æ—¶é—´æˆ³ï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨
        except Exception as e:
            log.warning(f"ä¿å­˜æŸ¥è¯¢ç»“æœå¤±è´¥: {e}")
            return None
    
    def _ask_with_realtime_search(self, question: str, space_id: Optional[str] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨å®æ—¶æœç´¢æ¨¡å¼å›ç­”é—®é¢˜ï¼ˆç›´æ¥ä½¿ç”¨é£ä¹¦APIæœç´¢ï¼Œæ— éœ€åŒæ­¥æ–‡æ¡£ï¼‰ã€‚
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. æå–å…³é”®è¯è¿›è¡Œå¤šè½®æœç´¢
        2. ä½¿ç”¨embeddingå¯¹æœç´¢ç»“æœè¿›è¡Œé‡æ’åº
        3. æ™ºèƒ½æå–æ–‡æ¡£ç›¸å…³ç‰‡æ®µ
        4. å¹¶è¡Œæœç´¢å¤šä¸ªç©ºé—´
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            ç­”æ¡ˆå’Œå¼•ç”¨æ¥æº
        """
        try:
            from infrastructure.llm.service import LLMService
            from infrastructure.embedding.service import EmbeddingService
            import re
            
            log.info("="*80)
            log.info(f"ğŸ” ä½¿ç”¨å®æ—¶æœç´¢æ¨¡å¼å¤„ç†é—®é¢˜: {question}")
            log.info("="*80)
            
            # ç”ŸæˆæŸ¥è¯¢æ—¶é—´æˆ³ï¼ˆæ‰€æœ‰æ­¥éª¤ä½¿ç”¨åŒä¸€ä¸ªæ—¶é—´æˆ³ï¼‰
            query_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ä¿å­˜é—®é¢˜
            self._save_query_result(question, "question", {"question": question, "space_id": space_id}, query_timestamp)
            
            # è·å–çŸ¥è¯†ç©ºé—´åˆ—è¡¨
            if space_id:
                # å¦‚æœæŒ‡å®šäº†space_idï¼Œåªæœç´¢è¯¥ç©ºé—´
                log.info(f"æŒ‡å®šæœç´¢çŸ¥è¯†åº“ç©ºé—´: {space_id}")
                # å…ˆè·å–æ‰€æœ‰ç©ºé—´ä»¥éªŒè¯space_idæ˜¯å¦å­˜åœ¨
                all_spaces = self.document_loader.load_wiki_spaces()
                spaces = [s for s in all_spaces if s.get("space_id") == space_id]
                if not spaces:
                    return {
                        "success": False,
                        "answer": f"æœªæ‰¾åˆ°æŒ‡å®šçš„çŸ¥è¯†åº“ç©ºé—´ï¼ˆID: {space_id}ï¼‰ï¼Œè¯·æ£€æŸ¥ç©ºé—´IDæ˜¯å¦æ­£ç¡®",
                        "sources": [],
                    }
                # æ‰¾åˆ°åŒ¹é…çš„ç©ºé—´ï¼Œè®°å½•ç©ºé—´åç§°
                matched_space = spaces[0]
                log.info(f"æ‰¾åˆ°æŒ‡å®šçš„çŸ¥è¯†åº“ç©ºé—´: {matched_space.get('name', 'æœªçŸ¥')} ({space_id})")
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šspace_idï¼Œæœç´¢æ‰€æœ‰ç©ºé—´
                spaces = self.document_loader.load_wiki_spaces()
                if not spaces:
                    return {
                        "success": False,
                        "answer": "æœªæ‰¾åˆ°çŸ¥è¯†åº“ç©ºé—´ï¼Œè¯·æ£€æŸ¥æƒé™é…ç½®",
                        "sources": [],
                    }
                log.info(f"å°†æœç´¢æ‰€æœ‰ {len(spaces)} ä¸ªçŸ¥è¯†åº“ç©ºé—´")
            
            # ã€é—®é¢˜ç±»å‹è¯†åˆ«ã€‘æ£€æµ‹é—®é¢˜ç±»å‹
            question_type_info = self._detect_question_type(question)
            question_type = question_type_info.get("type", "content_qa")
            type_confidence = question_type_info.get("confidence", 0.5)
            subtype = question_type_info.get("subtype", "normal")
            
            log.info(f"ğŸ“‹ é—®é¢˜ç±»å‹è¯†åˆ«:")
            log.info(f"  ç±»å‹: {question_type} ({subtype})")
            log.info(f"  ç½®ä¿¡åº¦: {type_confidence:.2f}")
            
            # ã€AIåˆ†æé—®é¢˜ã€‘ä½¿ç”¨LLMåˆ†æé—®é¢˜å¹¶æå–æœç´¢å…³é”®è¯å’Œç­–ç•¥
            search_strategy = self._analyze_question_with_ai(question)
            keywords = search_strategy.get("keywords", [])
            search_queries = search_strategy.get("search_queries", [question])
            related_concepts = search_strategy.get("related_concepts", [])
            
            # å¦‚æœæ˜¯æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢ï¼Œä¼˜å…ˆä½¿ç”¨æ£€æµ‹åˆ°çš„å…³é”®è¯
            if question_type == "document_list" and question_type_info.get("keywords"):
                keywords = list(set(keywords + question_type_info["keywords"]))
            
            log.info(f"ğŸ“Š AIåˆ†æç»“æœ:")
            log.info(f"  å…³é”®è¯: {keywords}")
            log.info(f"  æœç´¢æŸ¥è¯¢: {search_queries}")
            log.info(f"  ç›¸å…³æ¦‚å¿µ: {related_concepts}")
            
            # ä¿å­˜AIåˆ†æç»“æœ
            self._save_query_result(question, "ai_analysis", {
                "keywords": keywords,
                "search_queries": search_queries,
                "related_concepts": related_concepts
            }, query_timestamp)
            
            # ä¼˜åŒ–æœç´¢æŸ¥è¯¢ï¼šå»é™¤ç–‘é—®è¯ï¼Œæå–æ ¸å¿ƒå…³é”®è¯
            # é£ä¹¦æœç´¢APIä¸æ”¯æŒåŒ…å«ç–‘é—®è¯çš„å®Œæ•´é—®é¢˜ï¼Œéœ€è¦æå–å…³é”®è¯
            import re
            def clean_query(query: str) -> str:
                """æ¸…ç†æŸ¥è¯¢è¯ï¼Œå»é™¤ç–‘é—®è¯å’Œæ ‡ç‚¹"""
                # å»é™¤å¸¸è§çš„ç–‘é—®è¯
                question_words = ['ä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'æ˜¯ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›', 'å—', 'å‘¢', 'ï¼Ÿ', '?']
                cleaned = query
                for word in question_words:
                    cleaned = cleaned.replace(word, ' ')
                # å»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
                cleaned = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', cleaned)
                cleaned = ' '.join(cleaned.split())
                return cleaned.strip()
            
            # æ„å»ºæœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼šä¼˜å…ˆä½¿ç”¨å…³é”®è¯ï¼Œç„¶åä½¿ç”¨æ¸…ç†åçš„æŸ¥è¯¢
            final_search_queries = []
            
            # 1. ä½¿ç”¨æå–çš„å…³é”®è¯ï¼ˆæœ€é‡è¦ï¼‰
            if keywords:
                # ä½¿ç”¨å‰2ä¸ªæœ€é‡è¦çš„å…³é”®è¯
                for kw in keywords[:2]:
                    if kw and len(kw) >= 2:
                        final_search_queries.append(kw)
            
            # 2. ä½¿ç”¨æ¸…ç†åçš„åŸå§‹é—®é¢˜ï¼ˆå»é™¤ç–‘é—®è¯ï¼‰
            cleaned_question = clean_query(question)
            if cleaned_question and cleaned_question not in final_search_queries:
                final_search_queries.append(cleaned_question)
            
            # 3. å¦‚æœAIç”Ÿæˆçš„æœç´¢æŸ¥è¯¢ä¸­æœ‰å¥½çš„å…³é”®è¯ï¼Œä¹ŸåŠ å…¥
            for query in search_queries[:2]:
                cleaned = clean_query(query)
                if cleaned and cleaned not in final_search_queries and len(cleaned) >= 2:
                    final_search_queries.append(cleaned)
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            seen = set()
            unique_queries = []
            for q in final_search_queries:
                if q.lower() not in seen:
                    seen.add(q.lower())
                    unique_queries.append(q)
            
            search_queries = unique_queries[:3]  # æœ€å¤š3ä¸ªæŸ¥è¯¢
            log.info(f"ğŸ” ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢ï¼ˆå»é™¤ç–‘é—®è¯ï¼‰: {search_queries}")
            
            # ä¿å­˜ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢
            self._save_query_result(question, "search_queries", {
                "final_queries": search_queries,
                "original_queries": search_strategy.get("search_queries", [])
            }, query_timestamp)
            
            # åœ¨æ‰€æœ‰ç©ºé—´ä¸­æœç´¢ï¼ˆä½¿ç”¨AIæå–çš„æœç´¢ç­–ç•¥ï¼‰
            all_results = []
            client = self.document_loader.client
            
            import time
            
            # å¦‚æœæŒ‡å®šäº†space_idï¼Œåªæœç´¢è¯¥ç©ºé—´ï¼›å¦åˆ™é™åˆ¶æœç´¢çš„ç©ºé—´æ•°é‡
            if space_id:
                # æŒ‡å®šäº†space_idï¼Œåªæœç´¢è¯¥ç©ºé—´ï¼ˆspaceså·²ç»è¿‡æ»¤è¿‡äº†ï¼‰
                spaces_to_search = spaces
                log.info(f"å°†æœç´¢æŒ‡å®šçš„çŸ¥è¯†åº“ç©ºé—´: {spaces[0].get('name', 'æœªçŸ¥')}")
            else:
                # é™åˆ¶æœç´¢çš„ç©ºé—´æ•°é‡ï¼Œä¼˜å…ˆæœç´¢å‰3ä¸ªç©ºé—´
                spaces_to_search = spaces[:3] if len(spaces) > 3 else spaces
                if len(spaces) > 3:
                    log.info(f"ä¼˜åŒ–ï¼šé™åˆ¶æœç´¢ç©ºé—´æ•°é‡ä¸º {len(spaces_to_search)} ä¸ªï¼Œé¿å…é¢‘ç‡é™åˆ¶")
            
            for space_idx, space_item in enumerate(spaces_to_search):
                current_space_id = space_item.get("space_id", "")
                space_name = space_item.get("name", "æœªçŸ¥")
                
                if not current_space_id:
                    continue
                
                # æ¯ä¸ªç©ºé—´ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼ˆç¬¬ä¸€ä¸ªç©ºé—´ä¸éœ€è¦å»¶è¿Ÿï¼‰
                if space_idx > 0:
                    time.sleep(1.0)  # æ¯ä¸ªç©ºé—´ä¹‹é—´å»¶è¿Ÿ1ç§’
                
                # å°è¯•å¤šä¸ªæœç´¢è¯ï¼ˆæ·»åŠ å»¶è¿Ÿä»¥é¿å…é¢‘ç‡é™åˆ¶ï¼‰
                for idx, query in enumerate(search_queries):
                    try:
                        # æ¯ä¸ªæŸ¥è¯¢ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼ˆç¬¬ä¸€ä¸ªæŸ¥è¯¢ä¸éœ€è¦å»¶è¿Ÿï¼‰
                        if idx > 0:
                            time.sleep(1.0)  # æ¯ä¸ªæŸ¥è¯¢é—´éš”1ç§’ï¼ˆå¢åŠ å»¶è¿Ÿæ—¶é—´ï¼‰
                        
                        log.info(f"æœç´¢çŸ¥è¯†åº“: {space_name} - æŸ¥è¯¢: {query}")
                        search_result = client.search_wiki_nodes(
                            space_id=current_space_id,
                            query=query,
                            limit=20  # å¢åŠ æœç´¢æ•°é‡
                        )
                        
                        if search_result.get("code") == 0:
                            items = search_result.get("data", {}).get("items", [])
                            log.info(f"åœ¨çŸ¥è¯†åº“ {space_name} ä¸­æ‰¾åˆ° {len(items)} ä¸ªæ–‡æ¡£ï¼ˆæŸ¥è¯¢: {query}ï¼‰")
                            for item in items:
                                # æ‰“å°ç¬¬ä¸€ä¸ªitemçš„æ‰€æœ‰å­—æ®µä»¥ä¾¿è°ƒè¯•
                                if len(all_results) == 0:
                                    import json
                                    log.info(f"ğŸ“‹ æœç´¢ç»“æœåŸå§‹æ•°æ®ç»“æ„ï¼ˆç¬¬ä¸€ä¸ªitemï¼‰: {json.dumps(item, indent=2, ensure_ascii=False)}")
                                
                                obj_token = item.get("obj_token", "")
                                # é£ä¹¦æœç´¢APIè¿”å›çš„å­—æ®µå¯èƒ½æ˜¯ node_tokenï¼ˆå­—ç¬¦ä¸²tokenï¼‰ï¼Œè€Œä¸æ˜¯ node_idï¼ˆæ•°å­—IDï¼‰
                                # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„å­—æ®µå
                                node_token = item.get("node_token", "") or item.get("node_id", "")
                                # å¦‚æœ node_token æ˜¯å­—ç¬¦ä¸²ï¼Œè¯´æ˜å®ƒå°±æ˜¯æˆ‘ä»¬è¦ç”¨çš„token
                                # å¦‚æœ node_id æ˜¯æ•°å­—ï¼Œè¯´æ˜å®ƒæ˜¯çœŸæ­£çš„æ•°å­—ID
                                node_id = item.get("node_id", "")
                                # å¦‚æœ node_id æ˜¯å­—ç¬¦ä¸²tokenï¼Œè¯´æ˜å­—æ®µæ˜ å°„æœ‰é—®é¢˜ï¼Œåº”è¯¥ä½¿ç”¨ node_token
                                if node_id and not str(node_id).isdigit():
                                    # node_id æ˜¯å­—ç¬¦ä¸²tokenï¼Œè¯´æ˜å®ƒå®é™…ä¸Šæ˜¯ node_token
                                    node_token = node_id
                                    node_id = ""  # æ¸…ç©ºï¼Œå› ä¸ºæ²¡æœ‰çœŸæ­£çš„æ•°å­—ID
                                
                                title = item.get("title", "æœªçŸ¥æ ‡é¢˜")
                                
                                # å»é‡ï¼ˆåŸºäºobj_tokenï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨node_tokenï¼‰
                                unique_key = obj_token or node_token or node_id
                                if not any(r.get("obj_token") == obj_token or 
                                          r.get("node_id") == node_id or 
                                          r.get("node_token") == node_token 
                                          for r in all_results):
                                    # ä»æœç´¢ç»“æœä¸­æå–URLï¼ˆæœç´¢APIè¿”å›çš„ç»“æœåŒ…å«urlå­—æ®µï¼‰
                                    url = item.get("url", "")
                                    all_results.append({
                                        "title": title,
                                        "obj_token": obj_token,
                                        "node_id": node_id,  # ä¿å­˜node_idï¼ˆæ•°å­—IDï¼Œå¦‚æœæœ‰ï¼‰
                                        "node_token": node_token,  # ä¿å­˜node_tokenï¼ˆå­—ç¬¦ä¸²tokenï¼Œç”¨äºwiki APIï¼‰
                                        "space_id": current_space_id,
                                        "space_name": space_name,
                                        "search_query": query,  # è®°å½•åŒ¹é…çš„æœç´¢è¯
                                        "url": url,  # ä¿å­˜URLï¼ˆæœç´¢APIè¿”å›çš„ï¼‰
                                    })
                    except Exception as e:
                        error_str = str(e)
                        log.warning(f"æœç´¢çŸ¥è¯†ç©ºé—´ {space_name} (æŸ¥è¯¢: {query}) å¤±è´¥: {e}")
                        
                        # å¦‚æœæ˜¯é¢‘ç‡é™åˆ¶é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´å¹¶è·³è¿‡åç»­æŸ¥è¯¢
                        if "frequency limit" in error_str.lower() or "99991400" in error_str:
                            log.warning("æ£€æµ‹åˆ°é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…5ç§’åè·³è¿‡å½“å‰ç©ºé—´...")
                            time.sleep(5)  # ç­‰å¾…5ç§’
                            break  # è·³è¿‡å½“å‰ç©ºé—´çš„å…¶ä»–æŸ¥è¯¢
                        continue
                    
                    # å¦‚æœå·²ç»æ‰¾åˆ°è¶³å¤Ÿçš„ç»“æœï¼Œå¯ä»¥æå‰åœæ­¢
                    if len(all_results) >= 20:
                        log.info(f"å·²æ‰¾åˆ°è¶³å¤Ÿçš„ç»“æœï¼ˆ{len(all_results)}ä¸ªï¼‰ï¼Œæå‰åœæ­¢æœç´¢")
                        break
                
                # å¦‚æœå·²ç»æ‰¾åˆ°è¶³å¤Ÿçš„ç»“æœï¼Œæå‰åœæ­¢æœç´¢æ‰€æœ‰ç©ºé—´
                if len(all_results) >= 20:
                    break
            
            if not all_results:
                return {
                    "success": True,
                    "answer": "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚å»ºè®®ï¼š\n1. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯\n2. æˆ–è€…å…ˆåŒæ­¥æ–‡æ¡£ä»¥è·å¾—æ›´å¥½çš„è¯­ä¹‰æœç´¢æ•ˆæœ",
                    "sources": [],
                }
            
            log.info(f"ğŸ“š æ‰¾åˆ° {len(all_results)} ä¸ªå€™é€‰æ–‡æ¡£ï¼Œå¼€å§‹åŠ è½½å†…å®¹å¹¶é‡æ’åº...")
            
            # ä¿å­˜æœç´¢ç»“æœ
            self._save_query_result(question, "search_results", {
                "total_count": len(all_results),
                "documents": [
                    {
                        "title": r.get("title", "æœªçŸ¥"),
                        "url": r.get("url", ""),
                        "space_name": r.get("space_name", ""),
                        "search_query": r.get("search_query", "")
                    }
                    for r in all_results[:20]  # åªä¿å­˜å‰20ä¸ª
                ]
            }, query_timestamp)
            
            # åŠ è½½æ–‡æ¡£å†…å®¹å¹¶è®¡ç®—ç›¸ä¼¼åº¦
            doc_results = []
            import time
            
            # ä»æœç´¢ç»“æœä¸­æå–URLï¼ˆå¦‚æœæœ‰ï¼‰
            log.info(f"ğŸ“‹ å‡†å¤‡åŠ è½½ {len(all_results)} ä¸ªæ–‡æ¡£çš„å†…å®¹ï¼ˆé™åˆ¶åŠ è½½å‰15ä¸ªï¼‰...")
            for idx, result in enumerate(all_results[:15]):  # é™åˆ¶åŠ è½½æ•°é‡ä»¥æé«˜æ€§èƒ½
                log.info(f"ğŸ“‹ [{idx+1}/{min(len(all_results), 15)}] å¤„ç†æ–‡æ¡£: {result.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                try:
                    # æ·»åŠ å»¶è¿Ÿä»¥é¿å…é¢‘ç‡é™åˆ¶ï¼ˆæ¯3ä¸ªæ–‡æ¡£é—´éš”0.5ç§’ï¼‰
                    if idx > 0 and idx % 3 == 0:
                        time.sleep(0.5)
                    
                    # å°è¯•è·å–æ–‡æ¡£å†…å®¹ï¼ˆçŸ¥è¯†åº“æœç´¢è¿”å›çš„éƒ½æ˜¯wikièŠ‚ç‚¹ï¼‰
                    doc_content = None
                    doc_meta = None
                    title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                    url = result.get("url", "")  # æœç´¢è¿”å›çš„ç»“æœåŒ…å«URL
                    node_id = result.get("node_id", "")
                    
                    # å¦‚æœæœç´¢ç»“æœä¸­æ²¡æœ‰URLï¼Œå°è¯•ä»node_idæ„å»º
                    if not url and node_id:
                        # ä»node_idæ„å»ºwiki URLï¼ˆæ ¼å¼ï¼šhttps://xxx.feishu.cn/wiki/{node_id}ï¼‰
                        # ä½†æˆ‘ä»¬éœ€è¦çŸ¥é“åŸŸåï¼Œæ‰€ä»¥å…ˆå°è¯•è·å–
                        pass  # æš‚æ—¶è·³è¿‡ï¼Œä½¿ç”¨æœç´¢ç»“æœä¸­çš„URL
                    
                    # å°è¯•è·å–æ–‡æ¡£å†…å®¹ï¼ˆä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨æœç´¢ç»“æœä¸­çš„obj_tokenå’Œobj_typeï¼Œé¿å…ä¸å¿…è¦çš„wiki APIè°ƒç”¨ï¼‰
                    doc_content = None
                    try:
                        node_id = result.get("node_id", "")
                        node_token = result.get("node_token", "")
                        obj_token = result.get("obj_token", "")
                        obj_type = result.get("obj_type", "")
                        
                        # æ‰“å°æœç´¢ç»“æœçš„æ‰€æœ‰å­—æ®µä»¥ä¾¿è°ƒè¯•
                        import json
                        result_keys = list(result.keys())
                        log.info(f"ğŸ“‹ æœç´¢ç»“æœå­—æ®µ: {result_keys}")
                        log.info(f"ğŸ“‹ å…³é”®å­—æ®µå€¼: node_id={node_id[:30] if node_id else 'None'}... (ç±»å‹: {type(node_id).__name__}), node_token={node_token[:30] if node_token else 'None'}..., obj_token={obj_token[:30] if obj_token else 'None'}..., obj_type={obj_type}")
                        
                        # ä¼˜åŒ–ç­–ç•¥ï¼šå¦‚æœæœç´¢ç»“æœä¸­å·²ç»æœ‰obj_tokenå’Œobj_typeï¼Œç›´æ¥ä½¿ç”¨ï¼Œé¿å…ä¸å¿…è¦çš„wiki APIè°ƒç”¨
                        if obj_token:
                            # å¦‚æœobj_typeæ˜¯docxï¼ˆç±»å‹8ï¼‰ï¼Œç›´æ¥ä½¿ç”¨docx APIï¼Œè·³è¿‡wiki API
                            if obj_type == "docx" or obj_type == 8:
                                log.info(f"ğŸ“‹ æ£€æµ‹åˆ°obj_type={obj_type}ï¼ˆdocxï¼‰ï¼Œç›´æ¥ä½¿ç”¨obj_tokenè°ƒç”¨docx APIï¼Œè·³è¿‡wiki API")
                                doc_content = self.document_loader.load_document_content(obj_token, is_wiki_node=False)
                                if doc_content and len(doc_content.strip()) >= 10:
                                    log.info(f"âœ… ç›´æ¥ä½¿ç”¨obj_tokenè·å–docxæ–‡æ¡£å†…å®¹æˆåŠŸï¼Œé•¿åº¦: {len(doc_content)} å­—ç¬¦")
                                else:
                                    log.warning(f"âš ï¸ ä½¿ç”¨obj_tokenè·å–docxæ–‡æ¡£å†…å®¹å¤±è´¥ï¼ˆé•¿åº¦: {len(doc_content) if doc_content else 0}ï¼‰")
                            else:
                                # obj_typeä¸æ˜¯docxï¼Œå…ˆå°è¯•ä½œä¸ºæ™®é€šæ–‡æ¡£ï¼Œå¤±è´¥åå†å°è¯•wiki API
                                log.info(f"ğŸ“‹ æ£€æµ‹åˆ°obj_type={obj_type}ï¼Œå…ˆå°è¯•ä½œä¸ºæ™®é€šæ–‡æ¡£è·å–å†…å®¹")
                                doc_content = self.document_loader.load_document_content(obj_token, is_wiki_node=False)
                                if doc_content and len(doc_content.strip()) >= 10:
                                    log.info(f"âœ… ä½¿ç”¨obj_tokenä½œä¸ºæ™®é€šæ–‡æ¡£æˆåŠŸè·å–å†…å®¹ï¼Œé•¿åº¦: {len(doc_content)} å­—ç¬¦")
                                else:
                                    log.info(f"ğŸ“‹ obj_tokenä½œä¸ºæ™®é€šæ–‡æ¡£å¤±è´¥ï¼Œå°è¯•ä½œä¸ºwikièŠ‚ç‚¹...")
                                    doc_content = self.document_loader.load_document_content(obj_token, is_wiki_node=True)
                                    if doc_content and len(doc_content.strip()) >= 10:
                                        log.info(f"âœ… ä½¿ç”¨obj_tokenä½œä¸ºwikièŠ‚ç‚¹æˆåŠŸè·å–å†…å®¹ï¼Œé•¿åº¦: {len(doc_content)} å­—ç¬¦")
                                    else:
                                        log.warning(f"âš ï¸ ä½¿ç”¨obj_tokenè·å–æ–‡æ¡£å†…å®¹å¤±è´¥ï¼ˆé•¿åº¦: {len(doc_content) if doc_content else 0}ï¼‰")
                        
                        # å¦‚æœæ²¡æœ‰obj_tokenï¼Œæˆ–è€…ä½¿ç”¨obj_tokenå¤±è´¥ï¼Œæ‰å°è¯•ä½¿ç”¨wiki API
                        if not doc_content or len(doc_content.strip()) < 10:
                            # wiki APIéœ€è¦ä½¿ç”¨node_tokenï¼ˆå­—ç¬¦ä¸²tokenï¼‰ï¼Œè€Œä¸æ˜¯node_idï¼ˆæ•°å­—IDï¼‰
                            # æ ¹æ®é£ä¹¦APIæ–‡æ¡£ï¼Œwiki/v2/nodes/{node_token} éœ€è¦ä½¿ç”¨node_tokenï¼ˆå­—ç¬¦ä¸²ï¼‰
                            # å¦‚æœnode_idæ˜¯å­—ç¬¦ä¸²tokenï¼Œè¯´æ˜å­—æ®µæ˜ å°„æœ‰é—®é¢˜ï¼Œåº”è¯¥ä½¿ç”¨node_token
                            if node_id and not str(node_id).isdigit():
                                # node_id æ˜¯å­—ç¬¦ä¸²tokenï¼Œè¯´æ˜å®ƒå®é™…ä¸Šæ˜¯ node_token
                                log.debug(f"node_idå­—æ®µåŒ…å«å­—ç¬¦ä¸²tokenï¼Œä½¿ç”¨node_token")
                                node_token = node_id
                                node_id = ""
                            
                            # ä¼˜å…ˆä½¿ç”¨node_tokenï¼ˆå­—ç¬¦ä¸²tokenï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨node_idï¼ˆæ•°å­—IDï¼‰
                            wiki_node_token = node_token or node_id
                            
                            if wiki_node_token:
                                log.info(f"ğŸ“‹ å°è¯•ä½¿ç”¨wiki APIè·å–èŠ‚ç‚¹ä¿¡æ¯: {wiki_node_token[:30]}...")
                                try:
                                    # å…ˆè·å–wikièŠ‚ç‚¹ä¿¡æ¯ï¼Œå¯èƒ½åŒ…å«obj_token
                                    wiki_result = self.document_loader.client.get_wiki_node_content(wiki_node_token)
                                    if wiki_result.get("code") == 0:
                                        log.info(f"âœ… wiki APIè·å–èŠ‚ç‚¹ä¿¡æ¯æˆåŠŸ")
                                        wiki_data = wiki_result.get("data", {})
                                        wiki_node = wiki_data.get("node", {})
                                        if wiki_node:
                                            # ä»wikièŠ‚ç‚¹ä¸­æå–obj_tokenï¼ˆæ–‡æ¡£çš„å®é™…tokenï¼‰
                                            actual_obj_token = wiki_node.get("obj_token", "")
                                            actual_obj_type = wiki_node.get("obj_type", "")
                                            
                                            log.info(f"ğŸ“‹ ä»wikièŠ‚ç‚¹æå–ä¿¡æ¯ - obj_token: {actual_obj_token[:30] if actual_obj_token else 'None'}..., obj_type: {actual_obj_type}")
                                            
                                            if actual_obj_token:
                                                log.info(f"ğŸ“‹ ä½¿ç”¨ä»wikièŠ‚ç‚¹è·å–çš„obj_tokenè·å–æ–‡æ¡£å†…å®¹: {actual_obj_token[:30]}...")
                                                # ä½¿ç”¨å®é™…çš„obj_tokenè·å–æ–‡æ¡£å†…å®¹
                                                # å¦‚æœobj_typeæ˜¯docxï¼Œè¯´æ˜obj_tokenæ˜¯æ–‡æ¡£tokenï¼Œä¸æ˜¯wikièŠ‚ç‚¹
                                                if actual_obj_type == "docx" or actual_obj_type == 8:
                                                    log.info(f"ğŸ“‹ obj_typeæ˜¯docxï¼Œä½¿ç”¨docs/docx API")
                                                    doc_content = self.document_loader.load_document_content(actual_obj_token, is_wiki_node=False)
                                                else:
                                                    log.info(f"ğŸ“‹ obj_typeæ˜¯{actual_obj_type}ï¼Œå…ˆå°è¯•wiki APIï¼Œå¤±è´¥åå°è¯•docs/docx API")
                                                    doc_content = self.document_loader.load_document_content(actual_obj_token, is_wiki_node=True)
                                                if doc_content and len(doc_content.strip()) >= 10:
                                                    log.info(f"âœ… é€šè¿‡wiki APIæˆåŠŸè·å–æ–‡æ¡£å†…å®¹ï¼Œé•¿åº¦: {len(doc_content)} å­—ç¬¦")
                                                else:
                                                    log.warning(f"âš ï¸ è·å–æ–‡æ¡£å†…å®¹å¤±è´¥æˆ–å†…å®¹ä¸ºç©ºï¼ˆé•¿åº¦: {len(doc_content) if doc_content else 0}ï¼‰")
                                            else:
                                                log.info(f"ğŸ“‹ wikièŠ‚ç‚¹æ²¡æœ‰obj_tokenï¼Œå°è¯•ç›´æ¥åŠ è½½èŠ‚ç‚¹å†…å®¹")
                                                # å¦‚æœwikièŠ‚ç‚¹ç›´æ¥åŒ…å«å†…å®¹ï¼Œå°è¯•ç›´æ¥åŠ è½½
                                                doc_content = self.document_loader.load_document_content(wiki_node_token, is_wiki_node=True)
                                                if doc_content and len(doc_content.strip()) >= 10:
                                                    log.info(f"âœ… é€šè¿‡wiki APIç›´æ¥è·å–èŠ‚ç‚¹å†…å®¹æˆåŠŸï¼Œé•¿åº¦: {len(doc_content)} å­—ç¬¦")
                                                else:
                                                    log.warning(f"âš ï¸ ç›´æ¥åŠ è½½èŠ‚ç‚¹å†…å®¹å¤±è´¥æˆ–å†…å®¹ä¸ºç©ºï¼ˆé•¿åº¦: {len(doc_content) if doc_content else 0}ï¼‰")
                                    else:
                                        error_code = wiki_result.get("code")
                                        error_msg = wiki_result.get("msg", "")
                                        log.debug(f"wiki APIè¿”å›é”™è¯¯: {error_msg} (code: {error_code})")
                                except Exception as e:
                                    error_str = str(e)
                                    if "404" in error_str or "99991679" in error_str:
                                        log.debug(f"wiki APIæƒé™ä¸è¶³æˆ–404: {type(e).__name__}: {str(e)[:200]}")
                                    else:
                                        log.debug(f"wiki APIè·å–èŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {type(e).__name__}: {str(e)[:200]}")
                        if doc_content and len(doc_content.strip()) >= 10:
                            # æˆåŠŸè·å–å†…å®¹ï¼Œå°è¯•è·å–å…ƒä¿¡æ¯
                            try:
                                doc_meta = self.document_loader.load_document_meta(result["obj_token"])
                                if doc_meta:
                                    title = doc_meta.get("title", title)
                                    # å¦‚æœå…ƒä¿¡æ¯ä¸­æœ‰URLï¼Œä¼˜å…ˆä½¿ç”¨
                                    meta_url = doc_meta.get("url", "")
                                    if meta_url:
                                        url = meta_url
                            except Exception:
                                # å…ƒä¿¡æ¯è·å–å¤±è´¥ä¸å½±å“ï¼Œä½¿ç”¨æœç´¢ç»“æœä¸­çš„ä¿¡æ¯
                                pass
                    except Exception as e:
                        # é™é»˜å¤„ç†ï¼Œä¸è¾“å‡ºå¤§é‡é”™è¯¯æ—¥å¿—ï¼ˆè¿™äº›é”™è¯¯æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºæƒé™ä¸è¶³ï¼‰
                        # åªåœ¨DEBUGçº§åˆ«è®°å½•
                        log.debug(f"æ— æ³•è·å–æ–‡æ¡£ {title} çš„å®Œæ•´å†…å®¹ï¼ˆæƒé™é™åˆ¶ï¼‰: {type(e).__name__}")
                        # å¦‚æœæ— æ³•è·å–å†…å®¹ï¼Œç»§ç»­å¤„ç†ï¼Œè‡³å°‘ä¿ç•™æ ‡é¢˜å’ŒURL
                    
                    # å¦‚æœæ— æ³•è·å–æ–‡æ¡£å†…å®¹ï¼Œä½†è‡³å°‘ä¿ç•™æ ‡é¢˜å’ŒURLä½œä¸ºæ¥æº
                    if not doc_content or len(doc_content.strip()) < 10:
                        # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œä½†è‡³å°‘ä¿ç•™æ ‡é¢˜å’ŒURL
                        if title and title != "æœªçŸ¥æ ‡é¢˜":
                            # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå†…å®¹ç‰‡æ®µï¼ˆè‡³å°‘è®©ç”¨æˆ·çŸ¥é“æ‰¾åˆ°äº†ç›¸å…³æ–‡æ¡£ï¼‰
                            log.info(f"âš ï¸ æ–‡æ¡£ {title} æ— æ³•è·å–å®Œæ•´å†…å®¹ï¼Œä½†ä¿ç•™æ ‡é¢˜å’ŒURL")
                            doc_results.append({
                                "title": title,
                                "url": url,
                                "content": f"æ–‡æ¡£æ ‡é¢˜ï¼š{title}",
                                "full_content": "",
                                "similarity": 0.5,  # ç»™äºˆä¸­ç­‰ç›¸ä¼¼åº¦ï¼Œå› ä¸ºè‡³å°‘æ ‡é¢˜åŒ¹é…
                                "obj_token": result.get("obj_token", ""),
                                "has_content": False,  # æ ‡è®°ä¸ºæ²¡æœ‰å®Œæ•´å†…å®¹
                            })
                        else:
                            log.warning(f"âš ï¸ æ–‡æ¡£æ ‡é¢˜ä¸ºç©ºï¼Œè·³è¿‡: obj_token={result.get('obj_token', '')[:30]}...")
                        continue
                    
                    # æå–æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
                    relevant_chunk = self._extract_relevant_chunk(doc_content, question, keywords)
                    
                    # éªŒè¯æå–çš„ç‰‡æ®µæ˜¯å¦æœ‰æ•ˆ
                    if not relevant_chunk or not relevant_chunk.strip():
                        log.warning(f"æ–‡æ¡£ {title} æå–çš„ç›¸å…³ç‰‡æ®µä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹å†…å®¹è®¡ç®—ç›¸ä¼¼åº¦")
                        relevant_chunk = doc_content[:1000] if doc_content else ""  # ä½¿ç”¨å‰1000å­—ç¬¦ä½œä¸ºå›é€€
                    
                    if not relevant_chunk or not relevant_chunk.strip():
                        log.warning(f"æ–‡æ¡£ {title} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ç›¸ä¼¼åº¦è®¡ç®—")
                        similarity = 0.0
                    else:
                        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨embeddingï¼‰- ä½¿ç”¨æå–çš„ç›¸å…³ç‰‡æ®µè®¡ç®—ï¼Œè€Œä¸æ˜¯åŸå§‹å†…å®¹
                        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨relevant_chunkè€Œä¸æ˜¯doc_contentï¼Œå› ä¸ºrelevant_chunkæ˜¯æå–çš„æœ€ç›¸å…³éƒ¨åˆ†
                        similarity = self._calculate_similarity(question, relevant_chunk)
                        log.debug(f"æ–‡æ¡£ {title} ç›¸ä¼¼åº¦: {similarity:.3f} (ç‰‡æ®µé•¿åº¦: {len(relevant_chunk)})")
                    
                    doc_results.append({
                        "title": title,
                        "url": url,
                        "content": relevant_chunk,
                        "full_content": doc_content,
                        "similarity": similarity,
                        "obj_token": result["obj_token"],
                        "has_content": True,  # æ ‡è®°ä¸ºæœ‰å®Œæ•´å†…å®¹
                    })
                except Exception as e:
                    log.warning(f"âŒ å¤„ç†æ–‡æ¡£ {result.get('title', 'æœªçŸ¥')} å¤±è´¥: {e}")
                    # å³ä½¿å¤„ç†å¤±è´¥ï¼Œä¹Ÿå°è¯•ä¿ç•™æ ‡é¢˜å’ŒURL
                    title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                    url = result.get("url", "")
                    if title and title != "æœªçŸ¥æ ‡é¢˜":
                        log.info(f"âš ï¸ æ–‡æ¡£ {title} å¤„ç†å¤±è´¥ï¼Œä½†ä¿ç•™æ ‡é¢˜å’ŒURL")
                        doc_results.append({
                            "title": title,
                            "url": url,
                            "content": f"æ–‡æ¡£æ ‡é¢˜ï¼š{title}",
                            "full_content": "",
                            "similarity": 0.3,
                            "obj_token": result.get("obj_token", ""),
                            "has_content": False,
                        })
                    else:
                        log.warning(f"âš ï¸ æ–‡æ¡£å¤„ç†å¤±è´¥ä¸”æ ‡é¢˜ä¸ºç©ºï¼Œå®Œå…¨è·³è¿‡: obj_token={result.get('obj_token', '')[:30]}...")
                    continue
            
            log.info(f"ğŸ“Š å†…å®¹åŠ è½½å®Œæˆï¼šå…±å¤„ç† {len(doc_results)} ä¸ªæ–‡æ¡£ç»“æœ")
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆä¼˜å…ˆæœ‰å®Œæ•´å†…å®¹çš„æ–‡æ¡£ï¼‰
            doc_results.sort(key=lambda x: (x.get("has_content", False), x["similarity"]), reverse=True)
            
            # åˆ†ç¦»æœ‰å†…å®¹å’Œæ— å†…å®¹çš„æ–‡æ¡£
            results_with_content = [r for r in doc_results if r.get("has_content", True)]
            results_without_content = [r for r in doc_results if not r.get("has_content", True)]
            
            # æ ¹æ®é—®é¢˜ç±»å‹è®¾ç½®ä¸åŒçš„ç›¸ä¼¼åº¦é˜ˆå€¼
            if question_type == "document_list":
                # æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢ï¼šä½¿ç”¨æ›´ä½çš„é˜ˆå€¼ï¼Œè¿”å›æ›´å¤šæ–‡æ¡£
                MIN_SIMILARITY_THRESHOLD = 0.2
                MAX_RESULTS = 30  # è¿”å›æ›´å¤šæ–‡æ¡£
                log.info(f"ğŸ“‹ æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢æ¨¡å¼ï¼šé˜ˆå€¼={MIN_SIMILARITY_THRESHOLD}, æœ€å¤§ç»“æœæ•°={MAX_RESULTS}")
            else:
                # å†…å®¹é—®ç­”ï¼šä½¿ç”¨è¾ƒé«˜çš„é˜ˆå€¼ï¼Œç¡®ä¿ç›¸å…³æ€§
                MIN_SIMILARITY_THRESHOLD = 0.5
                MAX_RESULTS = 5  # åªè¿”å›æœ€ç›¸å…³çš„å‡ ä¸ªæ–‡æ¡£
                log.info(f"ğŸ’¬ å†…å®¹é—®ç­”æ¨¡å¼ï¼šé˜ˆå€¼={MIN_SIMILARITY_THRESHOLD}, æœ€å¤§ç»“æœæ•°={MAX_RESULTS}")
            
            filtered_results = [r for r in results_with_content if r["similarity"] >= MIN_SIMILARITY_THRESHOLD]
            
            # è®°å½•ç›¸ä¼¼åº¦ä¿¡æ¯ç”¨äºè°ƒè¯•
            if results_with_content:
                max_sim = max([r["similarity"] for r in results_with_content])
                avg_sim = sum([r["similarity"] for r in results_with_content]) / len(results_with_content)
                log.info(f"ğŸ“Š æ–‡æ¡£ç›¸ä¼¼åº¦ç»Ÿè®¡: æœ€é«˜={max_sim:.3f}, å¹³å‡={avg_sim:.3f}, é˜ˆå€¼={MIN_SIMILARITY_THRESHOLD}")
                log.info(f"âœ… è¾¾åˆ°é˜ˆå€¼ï¼ˆ>={MIN_SIMILARITY_THRESHOLD}ï¼‰çš„æ–‡æ¡£æ•°: {len(filtered_results)}/{len(results_with_content)}")
                
                # æ‰“å°å‰10ä¸ªæ–‡æ¡£çš„ç›¸ä¼¼åº¦
                log.info(f"ğŸ“‹ æ–‡æ¡£ç›¸ä¼¼åº¦åˆ—è¡¨ï¼ˆå‰10ä¸ªï¼‰:")
                for i, doc in enumerate(results_with_content[:10], 1):
                    sim = doc.get("similarity", 0.0)
                    status = "âœ…" if sim >= MIN_SIMILARITY_THRESHOLD else "âŒ"
                    log.info(f"   {status} {i}. {doc.get('title', 'æœªçŸ¥')}: {sim:.3f}")
            
            # ä¿å­˜ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ
            max_sim = max([r["similarity"] for r in results_with_content]) if results_with_content else 0.0
            avg_sim = sum([r["similarity"] for r in results_with_content]) / len(results_with_content) if results_with_content else 0.0
            self._save_query_result(question, "similarity_calculation", {
                "total_docs": len(doc_results),
                "with_content": len(results_with_content),
                "without_content": len(results_without_content),
                "filtered_count": len(filtered_results),
                "threshold": MIN_SIMILARITY_THRESHOLD,
                "max_similarity": max_sim,
                "avg_similarity": avg_sim,
                "documents": [
                    {
                        "title": r.get("title", "æœªçŸ¥"),
                        "similarity": r.get("similarity", 0.0),
                        "has_content": r.get("has_content", False),
                        "url": r.get("url", "")
                    }
                    for r in results_with_content[:15]  # ä¿å­˜å‰15ä¸ª
                ]
            }, query_timestamp)
            
            # å¦‚æœæ˜¯æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢ï¼Œå³ä½¿æ²¡æœ‰è¾¾åˆ°é˜ˆå€¼ä¹Ÿè¿”å›æ–‡æ¡£åˆ—è¡¨
            if question_type == "document_list":
                # æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢ï¼šåˆå¹¶æœ‰å†…å®¹å’Œæ— å†…å®¹çš„æ–‡æ¡£
                # å¯¹äºæ— å†…å®¹çš„æ–‡æ¡£ï¼Œç»™äºˆé»˜è®¤ç›¸ä¼¼åº¦0.3ï¼ˆå› ä¸ºè‡³å°‘æ ‡é¢˜åŒ¹é…ï¼‰
                all_documents = []
                
                # æ·»åŠ æœ‰å†…å®¹çš„æ–‡æ¡£ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
                for doc in sorted(results_with_content, key=lambda x: x["similarity"], reverse=True):
                    all_documents.append(doc)
                
                # æ·»åŠ æ— å†…å®¹çš„æ–‡æ¡£ï¼ˆè‡³å°‘æ˜¾ç¤ºæ ‡é¢˜å’ŒURLï¼‰
                for doc in results_without_content:
                    # ç¡®ä¿æ— å†…å®¹æ–‡æ¡£æœ‰ç›¸ä¼¼åº¦å€¼ï¼ˆå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼0.3ï¼‰
                    if "similarity" not in doc or doc.get("similarity", 0) == 0:
                        doc["similarity"] = 0.3
                    all_documents.append(doc)
                
                # é™åˆ¶è¿”å›æ•°é‡
                document_list_results = all_documents[:MAX_RESULTS]
                
                log.info(f"ğŸ“‹ æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢ï¼šæ‰¾åˆ° {len(document_list_results)} ä¸ªæ–‡æ¡£ï¼ˆæœ‰å†…å®¹: {len(results_with_content)}, æ— å†…å®¹: {len(results_without_content)}ï¼‰")
                
                if document_list_results:
                    # æ ¼å¼åŒ–æ–‡æ¡£åˆ—è¡¨
                    answer_text = self._format_document_list(document_list_results, question, subtype)
                    
                    return {
                        "success": True,
                        "answer": answer_text,
                        "sources": [{"title": r["title"], "url": r["url"], "similarity": r.get("similarity", 0.3)} 
                                   for r in document_list_results],
                        "question_type": "document_list",
                        "max_similarity": max([r.get("similarity", 0.3) for r in document_list_results]) if document_list_results else 0.0,
                    }
                else:
                    # æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£
                    return {
                        "success": False,
                        "answer": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚\n\nå»ºè®®ï¼š\n1. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æœç´¢\n2. æˆ–è€…æ£€æŸ¥çŸ¥è¯†åº“ä¸­æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£",
                        "sources": [],
                        "question_type": "document_list",
                        "max_similarity": 0.0,
                    }
            
            # ğŸ”´ å†…å®¹é—®ç­”æ¨¡å¼ï¼šå¦‚æœæ²¡æœ‰è¾¾åˆ°é˜ˆå€¼çš„æ–‡æ¡£ï¼Œæ˜ç¡®æ‹’ç»ï¼Œä¸å†å¼ºåˆ¶è¿”å›
            if not filtered_results:
                log.warning(f"æœªæ‰¾åˆ°ç›¸ä¼¼åº¦>={MIN_SIMILARITY_THRESHOLD}çš„ç›¸å…³æ–‡æ¡£")
                if results_with_content:
                    # è®°å½•æœ€é«˜ç›¸ä¼¼åº¦ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£ä¸ºä»€ä¹ˆæ‹’ç»
                    max_sim = max([r["similarity"] for r in results_with_content])
                    top_titles = [r["title"] for r in sorted(results_with_content, key=lambda x: x["similarity"], reverse=True)[:3]]
            
                    # åˆ¤æ–­æ˜¯å¦å»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢
                    suggest_web = self._should_use_web_search(question, {
                        "success": False,
                        "sources": [{"similarity": max_sim}]
                    })
                    
                    answer_text = (
                        f"æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜é«˜åº¦ç›¸å…³çš„æ–‡æ¡£ã€‚\n\n"
                        f"æ‰¾åˆ°çš„æ–‡æ¡£æœ€é«˜ç›¸ä¼¼åº¦ä¸º {max_sim:.3f}ï¼Œä½äºé˜ˆå€¼ {MIN_SIMILARITY_THRESHOLD}ã€‚\n\n"
                        f"æ‰¾åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼š\n" + "\n".join([f"- {title}" for title in top_titles]) + "\n\n"
                    )
                    
                    if suggest_web:
                        answer_text += (
                            f"ğŸ’¡ å»ºè®®ï¼š\n"
                            f"1. å¯ä»¥å°è¯•ä½¿ç”¨ç½‘ç»œæœç´¢è·å–æ›´å¤šä¿¡æ¯\n"
                            f"2. æˆ–è€…å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®\n"
                            f"3. æˆ–è€…æ£€æŸ¥çŸ¥è¯†åº“ä¸­æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£"
                        )
                    else:
                        answer_text += (
                            f"å»ºè®®ï¼š\n"
                            f"1. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®\n"
                            f"2. æˆ–è€…æ£€æŸ¥çŸ¥è¯†åº“ä¸­æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£"
                        )
                    
                    return {
                        "success": False,
                        "answer": answer_text,
                        "sources": [{"title": r["title"], "url": r["url"], "similarity": r["similarity"]} 
                                   for r in sorted(results_with_content, key=lambda x: x["similarity"], reverse=True)[:3]],
                        "suggest_web_search": suggest_web,
                        "max_similarity": max_sim,
                        "question_type": "content_qa",
                    }
                else:
                    # å¦‚æœæ²¡æœ‰æœ‰å†…å®¹çš„æ–‡æ¡£ï¼Œä¹Ÿä¸ä½¿ç”¨æ— å†…å®¹çš„æ–‡æ¡£ï¼ˆé¿å…è¯¯å¯¼ï¼‰
                    # åˆ¤æ–­æ˜¯å¦å»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢
                    suggest_web = self._should_use_web_search(question, {
                        "success": False,
                        "sources": []
                    })
                    
                    answer_text = (
                        "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ã€‚\n\n"
                    )
                    
                    if suggest_web:
                        answer_text += (
                            "ğŸ’¡ å»ºè®®ï¼š\n"
                            "1. å¯ä»¥å°è¯•ä½¿ç”¨ç½‘ç»œæœç´¢è·å–æ›´å¤šä¿¡æ¯\n"
                            "2. æˆ–è€…å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®\n"
                            "3. æˆ–è€…æ£€æŸ¥çŸ¥è¯†åº“ä¸­æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£"
                        )
                    else:
                        answer_text += (
                            "å»ºè®®ï¼š\n"
                            "1. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®\n"
                            "2. æˆ–è€…æ£€æŸ¥çŸ¥è¯†åº“ä¸­æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£"
                        )
                    
                    return {
                        "success": False,
                        "answer": answer_text,
                        "sources": [],
                        "suggest_web_search": suggest_web,
                        "max_similarity": 0.0,
                    }
            
            # å¦‚æœæ²¡æœ‰æœ‰å†…å®¹çš„æ–‡æ¡£ï¼Œä¹Ÿä¸ä½¿ç”¨æ— å†…å®¹çš„æ–‡æ¡£ï¼ˆé¿å…è¯¯å¯¼ï¼‰
            # ç§»é™¤åŸæ¥çš„é€»è¾‘ï¼šif not filtered_results and results_without_content
            
            # æ ¹æ®é—®é¢˜ç±»å‹å–ä¸åŒæ•°é‡çš„ç»“æœ
            top_results = filtered_results[:MAX_RESULTS]
            
            # ç»Ÿè®¡æœ‰å†…å®¹å’Œæ— å†…å®¹çš„æ–‡æ¡£æ•°é‡
            content_count = sum(1 for r in top_results if r.get("has_content", True))
            title_only_count = len(top_results) - content_count
            if title_only_count > 0:
                log.warning(f"æ‰¾åˆ° {len(top_results)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œå…¶ä¸­ {title_only_count} ä¸ªæ— æ³•è·å–å®Œæ•´å†…å®¹ï¼ˆå¯èƒ½æƒé™ä¸è¶³ï¼‰")
            
            # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨ç›¸å…³ç‰‡æ®µï¼‰
            context_parts = []
            sources = []
            has_content_results = []
            title_only_results = []
            
            for result in top_results:
                # ä½¿ç”¨æœ‰å®Œæ•´å†…å®¹çš„æ–‡æ¡£æ„å»ºä¸Šä¸‹æ–‡
                if result.get("has_content", True):
                    # ä¼˜å…ˆä½¿ç”¨full_contentï¼ˆå®Œæ•´å†…å®¹ï¼‰ï¼Œå¢åŠ é•¿åº¦é™åˆ¶åˆ°8000å­—ç¬¦
                    # å¦‚æœå®Œæ•´å†…å®¹å¤ªé•¿ï¼ˆè¶…è¿‡8000å­—ç¬¦ï¼‰ï¼Œä½¿ç”¨æå–çš„ç›¸å…³ç‰‡æ®µ
                    full_content = result.get("full_content", "")
                    extracted_chunk = result.get("content", "")
                    
                    if full_content and len(full_content) <= 8000:
                        # ä½¿ç”¨å®Œæ•´å†…å®¹ï¼ˆå¦‚æœé•¿åº¦åˆç†ï¼‰
                        content_to_use = full_content
                    elif full_content and len(full_content) > 8000:
                        # å¦‚æœå®Œæ•´å†…å®¹å¤ªé•¿ï¼Œä½¿ç”¨æå–çš„ç›¸å…³ç‰‡æ®µï¼Œä½†å°½é‡ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡
                        # å°è¯•ä»å®Œæ•´å†…å®¹ä¸­æå–åŒ…å«ç›¸å…³ç‰‡æ®µçš„éƒ¨åˆ†ï¼ˆå‰åå„ä¿ç•™1000å­—ç¬¦ï¼‰
                        if extracted_chunk:
                            # æ‰¾åˆ°æå–ç‰‡æ®µåœ¨å®Œæ•´å†…å®¹ä¸­çš„ä½ç½®
                            chunk_start = full_content.find(extracted_chunk[:100])
                            if chunk_start >= 0:
                                # æå–ç‰‡æ®µå‰åå„1000å­—ç¬¦
                                context_start = max(0, chunk_start - 1000)
                                context_end = min(len(full_content), chunk_start + len(extracted_chunk) + 1000)
                                content_to_use = full_content[context_start:context_end]
                            else:
                                content_to_use = extracted_chunk
                        else:
                            content_to_use = full_content[:8000] + "..."
                    else:
                        # æ²¡æœ‰å®Œæ•´å†…å®¹ï¼Œä½¿ç”¨æå–çš„ç‰‡æ®µ
                        content_to_use = extracted_chunk
                    
                    # ç»“æ„åŒ–ç»„ç»‡ï¼šä¿ç•™æ–‡æ¡£æ ‡é¢˜ï¼Œæ¸…æ™°åˆ†éš”
                    context_parts.append(f"ã€æ–‡æ¡£ï¼š{result['title']}ã€‘\n\n{content_to_use}\n")
                    has_content_results.append(result)
                else:
                    # å³ä½¿æ²¡æœ‰å®Œæ•´å†…å®¹ï¼Œä¹Ÿè®°å½•æ ‡é¢˜ä¿¡æ¯
                    title_only_results.append(result)
                
                # æ‰€æœ‰ç»“æœéƒ½æ·»åŠ åˆ°sourcesï¼ˆåŒ…æ‹¬åªæœ‰æ ‡é¢˜çš„ï¼‰
                sources.append({
                    "title": result["title"],
                    "url": result["url"],
                    "similarity": result["similarity"],
                })
            
            context = "\n\n".join(context_parts)
            
            # å¦‚æœæ˜¯æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢æ¨¡å¼ï¼Œä¸”æ‰¾åˆ°äº†æ–‡æ¡£ï¼Œç›´æ¥è¿”å›æ–‡æ¡£åˆ—è¡¨
            if question_type == "document_list" and top_results:
                log.info(f"ğŸ“‹ æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢æ¨¡å¼ï¼šè¿”å› {len(top_results)} ä¸ªæ–‡æ¡£")
                answer_text = self._format_document_list(top_results, question, subtype)
                return {
                    "success": True,
                    "answer": answer_text,
                    "sources": [{"title": r["title"], "url": r["url"], "similarity": r["similarity"]} 
                               for r in top_results],
                    "question_type": "document_list",
                    "max_similarity": max([r["similarity"] for r in top_results]) if top_results else 0.0,
                }
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£å†…å®¹
            has_document_content = len(has_content_results) > 0
            
            # å³ä½¿æ²¡æœ‰å®Œæ•´å†…å®¹ï¼Œä¹Ÿå°è¯•ä½¿ç”¨æ ‡é¢˜ä¿¡æ¯ç”Ÿæˆç­”æ¡ˆ
            if not has_document_content:
                log.warning("æ— æ³•è·å–æ–‡æ¡£å®Œæ•´å†…å®¹ï¼Œå°è¯•åŸºäºæ–‡æ¡£æ ‡é¢˜ç”Ÿæˆç­”æ¡ˆ")
                
                # æ„å»ºåŸºäºæ ‡é¢˜çš„ä¸Šä¸‹æ–‡
                title_context_parts = []
                for result in top_results[:5]:  # ä½¿ç”¨å‰5ä¸ªç»“æœ
                    title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                    url = result.get("url", "")
                    similarity = result.get("similarity", 0)
                    search_query = result.get("search_query", "")
                    
                    # æ„å»ºæ ‡é¢˜ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«æ ‡é¢˜ã€ç›¸ä¼¼åº¦å’ŒåŒ¹é…çš„æœç´¢è¯ï¼‰
                    title_info = f"ã€æ–‡æ¡£ï¼š{title}ã€‘"
                    if search_query:
                        title_info += f"\nåŒ¹é…çš„æœç´¢è¯ï¼š{search_query}"
                    if similarity > 0:
                        title_info += f"\nç›¸å…³æ€§ï¼š{similarity:.2f}"
                    if url:
                        title_info += f"\né“¾æ¥ï¼š{url}"
                    
                    title_context_parts.append(title_info)
                
                title_context = "\n\n".join(title_context_parts)
                
                # ä½¿ç”¨LLMåŸºäºæ ‡é¢˜ä¿¡æ¯ç”Ÿæˆç­”æ¡ˆ
                try:
                    llm_service = LLMService()
                    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼Œä½†å—é™äºæƒé™ï¼Œæˆ‘åªèƒ½è·å–åˆ°ç›¸å…³æ–‡æ¡£çš„æ ‡é¢˜ä¿¡æ¯ï¼Œæ— æ³•è·å–å®Œæ•´å†…å®¹ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æå–çš„å…³é”®è¯ã€‘
{', '.join(keywords) if keywords else 'æ— '}

ã€ç›¸å…³æ¦‚å¿µã€‘
{', '.join(related_concepts) if related_concepts else 'æ— '}

ã€æ‰¾åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼ˆä»…æ ‡é¢˜ï¼‰ã€‘
{title_context}

ã€è¦æ±‚ã€‘
1. åŸºäºæ–‡æ¡£æ ‡é¢˜ï¼Œå°è¯•æ¨æ–­è¿™äº›æ–‡æ¡£å¯èƒ½åŒ…å«å“ªäº›ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯
2. å¦‚æœæ ‡é¢˜æ˜æ˜¾ä¸é—®é¢˜ç›¸å…³ï¼Œå¯ä»¥åŸºäºæ ‡é¢˜è¿›è¡Œåˆç†æ¨æ–­å¹¶ç»™å‡ºç­”æ¡ˆ
3. å¦‚æœæ ‡é¢˜ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜"æ ¹æ®æ–‡æ¡£æ ‡é¢˜ï¼Œæ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³æ–‡æ¡£ï¼Œä½†ç”±äºæƒé™é™åˆ¶æ— æ³•è·å–å®Œæ•´å†…å®¹"
4. åˆ—å‡ºæ‰¾åˆ°çš„ç›¸å…³æ–‡æ¡£æ ‡é¢˜ï¼Œå¹¶å»ºè®®ç”¨æˆ·ç‚¹å‡»é“¾æ¥æŸ¥çœ‹å®Œæ•´å†…å®¹
5. ä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼Œè¯­è¨€ç®€æ´æ˜äº†

ã€ç­”æ¡ˆã€‘
è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£æ ‡é¢˜ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼š
"""
                    answer = llm_service.generate(prompt)
                except Exception as e:
                    log.warning(f"åŸºäºæ ‡é¢˜ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æç¤º")
                    # å›é€€åˆ°ç®€å•çš„æç¤º
                    title_list = [r["title"] for r in top_results if r.get("title")]
                    answer = (
                        f"æ ¹æ®æœç´¢ï¼Œæ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³æ–‡æ¡£ï¼š\n\n"
                        + "\n".join([f"{i+1}. {title}" for i, title in enumerate(title_list)])
                        + "\n\n"
                        + "âš ï¸ æ³¨æ„ï¼šç”±äºæƒé™é™åˆ¶ï¼Œæ— æ³•è·å–æ–‡æ¡£çš„å®Œæ•´å†…å®¹ã€‚\n"
                        + "å»ºè®®ï¼š\n"
                        + "1. ç‚¹å‡»ä¸Šæ–¹æ–‡æ¡£é“¾æ¥æŸ¥çœ‹å®Œæ•´å†…å®¹\n"
                        + "2. æˆ–è€…å…ˆåŒæ­¥æ–‡æ¡£åˆ°æœ¬åœ°å‘é‡åº“ä»¥è·å¾—æ›´å¥½çš„æœç´¢æ•ˆæœ"
                    )
            else:
                # ã€AIåˆ†ææœç´¢ç»“æœã€‘å…ˆè®©AIåˆ†ææœç´¢ç»“æœçš„ç›¸å…³æ€§å’Œå…³é”®ä¿¡æ¯
                analysis_result = self._analyze_search_results_with_ai(question, has_content_results, keywords, related_concepts)
                
                # ã€AIç”Ÿæˆç­”æ¡ˆã€‘ä½¿ç”¨LLMåŸºäºæœç´¢ç»“æœå’ŒAIåˆ†æç”Ÿæˆç­”æ¡ˆ
                llm_service = LLMService()
                prompt = self._build_answer_prompt(question, context, has_content_results, analysis_result, keywords)
                
                answer = llm_service.generate(prompt)
            
                # ğŸ”´ æ–°å¢ï¼šéªŒè¯ç­”æ¡ˆç›¸å…³æ€§
                answer_relevance = self._verify_answer_relevance(question, answer, has_content_results)
                if not answer_relevance.get("is_relevant", True):
                    log.warning(f"ç­”æ¡ˆç›¸å…³æ€§éªŒè¯å¤±è´¥: {answer_relevance.get('reason', 'æœªçŸ¥åŸå› ')}")
                    # å¦‚æœç­”æ¡ˆä¸ç›¸å…³ï¼Œè¿”å›æç¤ºä¿¡æ¯
                    return {
                        "success": False,
                        "answer": (
                            f"æŠ±æ­‰ï¼Œæ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ— æ³•ç”Ÿæˆä¸æ‚¨çš„é—®é¢˜é«˜åº¦ç›¸å…³çš„ç­”æ¡ˆã€‚\n\n"
                            f"æ‰¾åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼š\n" + "\n".join([f"- {s['title']}" for s in sources[:3]]) + "\n\n"
                            f"å»ºè®®ï¼š\n"
                            f"1. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®\n"
                            f"2. æˆ–è€…æ£€æŸ¥çŸ¥è¯†åº“ä¸­æ˜¯å¦æœ‰æ›´ç›¸å…³çš„æ–‡æ¡£"
                        ),
                        "sources": sources,
                        "question_type": "content_qa",
                    }
            
            # è®¡ç®—æœ€é«˜ç›¸ä¼¼åº¦ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦å»ºè®®ç½‘ç»œæœç´¢
            sources_with_similarity = [s for s in sources if s.get("similarity", 0) > 0]
            max_similarity = max([s.get("similarity", 0) for s in sources_with_similarity]) if sources_with_similarity else 0.0
            
            # åˆ¤æ–­æ˜¯å¦å»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢ï¼ˆå³ä½¿æœ‰ç­”æ¡ˆï¼Œå¦‚æœç›¸ä¼¼åº¦è¾ƒä½ï¼Œä¹Ÿå»ºè®®ç½‘ç»œæœç´¢ï¼‰
            suggest_web = False
            if max_similarity > 0 and max_similarity < 0.6:
                # å¦‚æœç›¸ä¼¼åº¦åœ¨0.5-0.6ä¹‹é—´ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯é€šç”¨æ¦‚å¿µé—®é¢˜
                if self._is_general_concept_question(question):
                    suggest_web = True
            
            result = {
                "success": True,
                "answer": answer.strip(),
                "sources": sources,
                "suggest_web_search": suggest_web,
                "max_similarity": max_similarity,
                "question_type": question_type,
            }
            
            # ä¿å­˜æœ€ç»ˆç»“æœå¹¶æ‰“å°
            log.info("="*80)
            log.info(f"âœ… é—®é¢˜å¤„ç†å®Œæˆ")
            log.info(f"   é—®é¢˜: {question}")
            log.info(f"   ç­”æ¡ˆé•¿åº¦: {len(answer.strip())} å­—ç¬¦")
            log.info(f"   å¼•ç”¨æ–‡æ¡£æ•°: {len(sources)}")
            log.info(f"   æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}")
            if suggest_web:
                log.info(f"   ğŸ’¡ å»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢è¡¥å……ä¿¡æ¯")
            log.info("="*80)
            
            self._save_query_result(question, "final_result", {
                "success": True,
                "answer_length": len(answer.strip()),
                "sources_count": len(sources),
                "max_similarity": max_similarity,
                "suggest_web_search": suggest_web,
                "sources": sources[:10]  # åªä¿å­˜å‰10ä¸ªæ¥æº
            }, query_timestamp)
            
            return result
            
        except Exception as e:
            log.error(f"å®æ—¶æœç´¢æ¨¡å¼å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "answer": f"å®æ—¶æœç´¢å¤±è´¥: {str(e)}",
                "sources": [],
            }
    
    def _detect_question_type(self, question: str) -> Dict[str, Any]:
        """
        æ£€æµ‹é—®é¢˜ç±»å‹ï¼šæ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢ vs å†…å®¹é—®ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            {
                "type": "document_list" | "content_qa" | "mixed",
                "confidence": 0.0-1.0,
                "keywords": ["å…³é”®è¯åˆ—è¡¨"]
            }
        """
        question_lower = question.lower()
        
        # æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢çš„å…³é”®è¯æ¨¡å¼
        list_patterns = [
            "æœ‰å“ªäº›", "å“ªäº›æ–‡æ¡£", "ç›¸å…³æ–‡æ¡£", "æ–‡æ¡£åˆ—è¡¨", "åˆ—å‡º", 
            "æ‰¾åˆ°", "æœç´¢", "æŸ¥æ‰¾", "æ–‡æ¡£", "å“ªäº›æ–‡æ¡£",
            "ä»€ä¹ˆæ–‡æ¡£", "æœ‰ä»€ä¹ˆæ–‡æ¡£", "åŒ…å«å“ªäº›", "æ¶‰åŠå“ªäº›",
            "what documents", "list", "find documents", "search documents",
            "ç›¸å…³", "å…³äº.*çš„æ–‡æ¡£", ".*æ–‡æ¡£.*æœ‰å“ªäº›"
        ]
        
        # ç»Ÿè®¡æŸ¥è¯¢çš„å…³é”®è¯
        stats_patterns = [
            "æœ‰å¤šå°‘", "æ•°é‡", "ç»Ÿè®¡", "æ€»æ•°", "å‡ ä¸ª", "å¤šå°‘æ–‡æ¡£",
            "how many", "count", "number of"
        ]
        
        # å¯¹æ¯”æŸ¥è¯¢çš„å…³é”®è¯
        comparison_patterns = [
            "å¯¹æ¯”", "åŒºåˆ«", "å·®å¼‚", "æ¯”è¾ƒ", "vs", "versus", "å’Œ.*çš„åŒºåˆ«",
            "compare", "difference", "vs"
        ]
        
        # æ£€æŸ¥æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢
        list_score = 0.0
        for pattern in list_patterns:
            if pattern in question_lower:
                list_score += 0.3
                if pattern in ["æœ‰å“ªäº›", "å“ªäº›æ–‡æ¡£", "æ–‡æ¡£åˆ—è¡¨", "list"]:
                    list_score += 0.4  # æ›´å¼ºçš„ä¿¡å·
        
        # æ£€æŸ¥ç»Ÿè®¡æŸ¥è¯¢
        stats_score = 0.0
        for pattern in stats_patterns:
            if pattern in question_lower:
                stats_score += 0.5
        
        # æ£€æŸ¥å¯¹æ¯”æŸ¥è¯¢
        comparison_score = 0.0
        for pattern in comparison_patterns:
            if pattern in question_lower:
                comparison_score += 0.5
        
        # æå–å…³é”®è¯ï¼ˆç”¨äºåç»­æœç´¢ï¼‰
        keywords = self._extract_keywords(question)
        
        # åˆ¤æ–­é—®é¢˜ç±»å‹
        if list_score >= 0.5:
            return {
                "type": "document_list",
                "confidence": min(list_score, 1.0),
                "keywords": keywords,
                "subtype": "stats" if stats_score > 0.3 else "list"
            }
        elif stats_score >= 0.3:
            return {
                "type": "document_list",  # ç»Ÿè®¡æŸ¥è¯¢ä¹Ÿå½’ç±»ä¸ºæ–‡æ¡£åˆ—è¡¨
                "confidence": min(stats_score, 1.0),
                "keywords": keywords,
                "subtype": "stats"
            }
        elif comparison_score >= 0.3:
            return {
                "type": "content_qa",  # å¯¹æ¯”æŸ¥è¯¢éœ€è¦å†…å®¹åˆ†æ
                "confidence": min(comparison_score, 1.0),
                "keywords": keywords,
                "subtype": "comparison"
            }
        else:
            # é»˜è®¤æ˜¯å†…å®¹é—®ç­”
            return {
                "type": "content_qa",
                "confidence": 0.5,
                "keywords": keywords,
                "subtype": "normal"
            }
    
    def _analyze_question_with_ai(self, question: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨AIåˆ†æé—®é¢˜å¹¶æå–æœç´¢å…³é”®è¯å’Œç­–ç•¥ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            åŒ…å«å…³é”®è¯ã€æœç´¢æŸ¥è¯¢å’Œç›¸å…³æ¦‚å¿µçš„å­—å…¸
        """
        try:
            from infrastructure.llm.service import LLMService
            import json
            import re
            
            llm_service = LLMService()
            
            prompt = f"""åˆ†æä»¥ä¸‹é—®é¢˜ï¼Œæå–ç”¨äºæœç´¢çŸ¥è¯†åº“çš„å…³é”®è¯å’ŒæŸ¥è¯¢ç­–ç•¥ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åˆ†æï¼š
1. é—®é¢˜çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
2. éœ€è¦æœç´¢å“ªäº›å…³é”®è¯ï¼Ÿï¼ˆæå–2-5ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼‰
3. æœ‰å“ªäº›åŒä¹‰è¯æˆ–ç›¸å…³æ¦‚å¿µï¼Ÿ
4. å¯ä»¥å°è¯•å“ªäº›ä¸åŒçš„æœç´¢æŸ¥è¯¢ï¼Ÿï¼ˆç”Ÿæˆ3-5ä¸ªä¸åŒçš„æœç´¢æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åŸé—®é¢˜çš„ä¸åŒè¡¨è¾¾æ–¹å¼ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
    "search_queries": ["æœç´¢æŸ¥è¯¢1", "æœç´¢æŸ¥è¯¢2", "æœç´¢æŸ¥è¯¢3"],
    "related_concepts": ["ç›¸å…³æ¦‚å¿µ1", "ç›¸å…³æ¦‚å¿µ2"]
}}

è¦æ±‚ï¼š
- keywordsï¼šæå–çš„æ ¸å¿ƒå…³é”®è¯ï¼Œå»é™¤ç–‘é—®è¯ï¼ˆä»€ä¹ˆã€å¦‚ä½•ã€æ€ä¹ˆç­‰ï¼‰
- search_queriesï¼šå¤šä¸ªæœç´¢æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åŸé—®é¢˜çš„ä¸åŒè¡¨è¾¾æ–¹å¼ã€ç®€åŒ–ç‰ˆæœ¬ã€å…³é”®è¯ç»„åˆç­‰
- related_conceptsï¼šç›¸å…³æ¦‚å¿µæˆ–åŒä¹‰è¯

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
"""
            
            log.info("ä½¿ç”¨AIåˆ†æé—®é¢˜å¹¶æå–æœç´¢ç­–ç•¥...")
            response = llm_service.generate(prompt)
            
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            json_match = re.search(r'\{[^{}]*"keywords"[^{}]*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•è§£ææ•´ä¸ªå“åº”
                json_str = response.strip()
                # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                json_str = re.sub(r'```json\s*', '', json_str)
                json_str = re.sub(r'```\s*', '', json_str)
                json_str = json_str.strip()
            
            try:
                result = json.loads(json_str)
                
                # éªŒè¯å’Œæ¸…ç†ç»“æœ
                keywords = result.get("keywords", [])
                search_queries = result.get("search_queries", [])
                related_concepts = result.get("related_concepts", [])
                
                # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæœç´¢æŸ¥è¯¢
                if not search_queries:
                    search_queries = [question]
                else:
                    # ç¡®ä¿åŸé—®é¢˜åœ¨æœç´¢æŸ¥è¯¢ä¸­
                    if question not in search_queries:
                        search_queries.insert(0, question)
                
                # é™åˆ¶æ•°é‡
                keywords = keywords[:5]
                search_queries = search_queries[:5]
                related_concepts = related_concepts[:3]
                
                return {
                    "keywords": keywords,
                    "search_queries": search_queries,
                    "related_concepts": related_concepts,
                }
            except json.JSONDecodeError as e:
                log.warning(f"AIè¿”å›çš„JSONè§£æå¤±è´¥: {e}ï¼Œå“åº”: {response[:200]}")
                # å›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®è¯
                return self._fallback_extract_keywords(question)
                
        except Exception as e:
            log.warning(f"AIåˆ†æé—®é¢˜å¤±è´¥: {e}ï¼Œå›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼æå–")
            # å›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®è¯
            return self._fallback_extract_keywords(question)
    
    def _fallback_extract_keywords(self, question: str) -> Dict[str, Any]:
        """
        å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®è¯ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            åŒ…å«å…³é”®è¯å’Œæœç´¢æŸ¥è¯¢çš„å­—å…¸
        """
        keywords = self._extract_keywords(question)
        search_queries = [question] + keywords[:2]
        
        return {
            "keywords": keywords,
            "search_queries": search_queries,
            "related_concepts": [],
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        ä»é—®é¢˜ä¸­æå–å…³é”®è¯ã€‚
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        import re
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™ç©ºæ ¼
        text_clean = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
        
        keywords = []
        
        # æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2-4ä¸ªå­—ç¬¦ï¼Œé¿å…æå–æ•´ä¸ªé—®é¢˜ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', text_clean)
        keywords.extend(chinese_words)
        
        # æå–è‹±æ–‡å•è¯ï¼ˆ3ä¸ªå­—ç¬¦ä»¥ä¸Šï¼‰
        english_words = re.findall(r'\b[a-zA-Z]{3,}\b', text_clean)
        keywords.extend(english_words)
        
        # è¿‡æ»¤å¸¸è§åœç”¨è¯å’Œç–‘é—®è¯
        stop_words = {
            'ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›', 'è¿™ä¸ª', 'é‚£ä¸ª', 
            'æ˜¯', 'çš„', 'äº†', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä¸º',
            'æ˜¯ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›',
            'the', 'is', 'are', 'a', 'an', 'and', 'or', 'what', 'how', 'why'
        }
        keywords = [kw for kw in keywords if kw not in stop_words and len(kw) >= 2]
        
        # è¿›ä¸€æ­¥è¿‡æ»¤ï¼šå¦‚æœå…³é”®è¯åŒ…å«åœç”¨è¯ï¼Œå°è¯•æå–æ ¸å¿ƒéƒ¨åˆ†
        filtered_keywords = []
        for kw in keywords:
            # ç§»é™¤å¸¸è§çš„ç–‘é—®è¯å‰ç¼€/åç¼€
            kw_clean = kw
            for stop in ['ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'æ˜¯', 'çš„']:
                if kw_clean.startswith(stop):
                    kw_clean = kw_clean[len(stop):]
                if kw_clean.endswith(stop):
                    kw_clean = kw_clean[:-len(stop)]
            if kw_clean and len(kw_clean) >= 2 and kw_clean not in stop_words:
                filtered_keywords.append(kw_clean)
        
        keywords = filtered_keywords if filtered_keywords else keywords
        
        # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
        seen = set()
        unique_keywords = []
        for kw in keywords:
            kw_lower = kw.lower()
            if kw_lower not in seen:
                seen.add(kw_lower)
                unique_keywords.append(kw)
        
        # å¦‚æœæå–çš„å…³é”®è¯å¤ªå°‘ï¼Œå°è¯•æå–2-3å­—çš„çŸ­è¯­
        if len(unique_keywords) < 2:
            # æå–2-3å­—çš„ä¸­æ–‡çŸ­è¯­
            phrases = re.findall(r'[\u4e00-\u9fff]{2,3}', text)
            for phrase in phrases:
                if phrase not in stop_words and phrase not in seen:
                    seen.add(phrase.lower())
                    unique_keywords.append(phrase)
                    if len(unique_keywords) >= 3:
                        break
        
        return unique_keywords[:5]  # æœ€å¤šè¿”å›5ä¸ªå…³é”®è¯
    
    def _format_document_list(self, documents: List[Dict[str, Any]], question: str, subtype: str = "list") -> str:
        """
        æ ¼å¼åŒ–æ–‡æ¡£åˆ—è¡¨ä¸ºç­”æ¡ˆæ–‡æœ¬ã€‚
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            question: ç”¨æˆ·é—®é¢˜
            subtype: é—®é¢˜å­ç±»å‹ï¼ˆlist/statsï¼‰
            
        Returns:
            æ ¼å¼åŒ–åçš„ç­”æ¡ˆæ–‡æœ¬
        """
        if not documents:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
        
        # ç»Ÿè®¡æŸ¥è¯¢
        if subtype == "stats":
            answer = f"æ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£ï¼š\n\n"
        else:
            answer = f"æ‰¾åˆ°ä»¥ä¸‹ {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£ï¼š\n\n"
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†ç»„ï¼ˆé«˜/ä¸­/ä½ï¼‰
        high_relevance = [d for d in documents if d.get("similarity", 0) >= 0.5]
        medium_relevance = [d for d in documents if 0.3 <= d.get("similarity", 0) < 0.5]
        low_relevance = [d for d in documents if d.get("similarity", 0) < 0.3]
        
        # æ ¼å¼åŒ–æ–‡æ¡£åˆ—è¡¨
        doc_index = 1
        if high_relevance:
            answer += "**é«˜ç›¸å…³æ€§æ–‡æ¡£ï¼š**\n"
            for doc in high_relevance:
                similarity = doc.get("similarity", 0)
                similarity_str = f"ï¼ˆç›¸å…³æ€§: {similarity:.1%}ï¼‰" if similarity > 0 else ""
                answer += f"{doc_index}. {doc['title']}{similarity_str}\n"
                doc_index += 1
            answer += "\n"
        
        if medium_relevance:
            answer += "**ä¸­ç­‰ç›¸å…³æ€§æ–‡æ¡£ï¼š**\n"
            for doc in medium_relevance:
                similarity = doc.get("similarity", 0)
                similarity_str = f"ï¼ˆç›¸å…³æ€§: {similarity:.1%}ï¼‰" if similarity > 0 else ""
                answer += f"{doc_index}. {doc['title']}{similarity_str}\n"
                doc_index += 1
            answer += "\n"
        
        if low_relevance:
            answer += "**å…¶ä»–ç›¸å…³æ–‡æ¡£ï¼š**\n"
            for doc in low_relevance:
                similarity = doc.get("similarity", 0)
                similarity_str = f"ï¼ˆç›¸å…³æ€§: {similarity:.1%}ï¼‰" if similarity > 0 else ""
                answer += f"{doc_index}. {doc['title']}{similarity_str}\n"
                doc_index += 1
        
        # æ·»åŠ æç¤º
        answer += "\nğŸ’¡ æç¤ºï¼šç‚¹å‡»æ–‡æ¡£æ ‡é¢˜å¯ä»¥æŸ¥çœ‹å®Œæ•´å†…å®¹ã€‚"
        
        return answer
    
    def _extract_relevant_chunk(self, content: str, question: str, keywords: List[str], chunk_size: int = 4000) -> str:
        """
        ä»æ–‡æ¡£ä¸­æå–ä¸é—®é¢˜æœ€ç›¸å…³çš„ç‰‡æ®µã€‚
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            question: ç”¨æˆ·é—®é¢˜
            keywords: å…³é”®è¯åˆ—è¡¨
            chunk_size: ç‰‡æ®µå¤§å°ï¼ˆå¢åŠ åˆ°2000ä»¥æä¾›æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
            
        Returns:
            ç›¸å…³ç‰‡æ®µ
        """
        import re
        
        if not content:
            return ""
        
        # å¦‚æœæ–‡æ¡£è¾ƒçŸ­ï¼Œç›´æ¥è¿”å›
        if len(content) <= chunk_size:
            return content
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n+', content)
        
        # è®¡ç®—æ¯ä¸ªæ®µè½çš„ç›¸å…³æ€§åˆ†æ•°
        scored_paragraphs = []
        for para in paragraphs:
            if not para.strip():
                continue
            
            score = 0
            para_lower = para.lower()
            question_lower = question.lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜ä¸­çš„å…³é”®è¯
            for keyword in keywords:
                if keyword.lower() in para_lower:
                    score += 2
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜ä¸­çš„å®Œæ•´çŸ­è¯­
            if question_lower in para_lower:
                score += 5
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜ä¸­çš„éƒ¨åˆ†è¯æ±‡
            question_words = question_lower.split()
            for word in question_words:
                if len(word) >= 2 and word in para_lower:
                    score += 1
            
            if score > 0:
                scored_paragraphs.append((score, para))
        
        # æŒ‰åˆ†æ•°æ’åº
        scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
        
        # é€‰æ‹©æœ€ç›¸å…³çš„æ®µè½ç»„åˆï¼ˆå¢åŠ åˆ°æœ€å¤š15ä¸ªæ®µè½ï¼Œæä¾›æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
        # ä½¿ç”¨embeddingè®¡ç®—æ®µè½ç›¸ä¼¼åº¦ï¼Œè€Œä¸æ˜¯ç®€å•çš„å…³é”®è¯åŒ¹é…
        selected_text = ""
        selected_count = 0
        max_paragraphs = 15  # å¢åŠ æ®µè½æ•°é‡
        
        # å¦‚æœæ®µè½æ•°é‡è¾ƒå¤šï¼Œå°è¯•ä½¿ç”¨embeddingè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ›´å‡†ç¡®ï¼‰
        if len(scored_paragraphs) > 5:
            try:
                from infrastructure.embedding.service import EmbeddingService
                import numpy as np
                
                embedding_service = EmbeddingService()
                question_vector = np.array(embedding_service.embed_text(question))
                
                # é‡æ–°è®¡ç®—æ¯ä¸ªæ®µè½çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆç»“åˆå…³é”®è¯åŒ¹é…å’Œè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
                enhanced_scored_paragraphs = []
                for score, para in scored_paragraphs[:20]:  # åªå¤„ç†å‰20ä¸ªæ®µè½ä»¥æé«˜æ€§èƒ½
                    # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
                    para_vector = np.array(embedding_service.embed_text(para[:500]))
                    semantic_score = np.dot(question_vector, para_vector) / (
                        np.linalg.norm(question_vector) * np.linalg.norm(para_vector) + 1e-8
                    )
                    # ç»“åˆå…³é”®è¯åŒ¹é…åˆ†æ•°å’Œè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°
                    combined_score = score * 0.4 + semantic_score * 100 * 0.6
                    enhanced_scored_paragraphs.append((combined_score, para))
                
                # é‡æ–°æ’åº
                enhanced_scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
                scored_paragraphs = enhanced_scored_paragraphs
            except Exception as e:
                log.debug(f"ä½¿ç”¨embeddingè®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯åŒ¹é…: {e}")
                # å¦‚æœå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæ¥çš„å…³é”®è¯åŒ¹é…ç»“æœ
        
        # é€‰æ‹©æ®µè½æ—¶ï¼Œä¿ç•™ä¸Šä¸‹æ–‡çª—å£ï¼ˆæ¯ä¸ªç›¸å…³æ®µè½å‰åå„ä¿ç•™ä¸€ä¸ªæ®µè½ï¼‰
        selected_indices = set()
        
        # æ„å»ºæ®µè½åˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼‰
        para_to_index = {}
        for idx, orig_para in enumerate(paragraphs):
            if orig_para.strip():
                para_key = orig_para.strip()[:100]  # ä½¿ç”¨å‰100å­—ç¬¦ä½œä¸ºkey
                if para_key not in para_to_index:
                    para_to_index[para_key] = []
                para_to_index[para_key].append(idx)
        
        # é€‰æ‹©æœ€ç›¸å…³çš„æ®µè½åŠå…¶ä¸Šä¸‹æ–‡
        for score, para in scored_paragraphs[:max_paragraphs]:
            # æ‰¾åˆ°è¿™ä¸ªæ®µè½åœ¨åŸæ–‡ä¸­çš„ç´¢å¼•
            para_key = para.strip()[:100]
            para_indices = para_to_index.get(para_key, [])
            
            if para_indices:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„ç´¢å¼•
                para_index = para_indices[0]
                # é€‰æ‹©å½“å‰æ®µè½åŠå…¶å‰åå„ä¸€ä¸ªæ®µè½ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰
                for ctx_idx in range(max(0, para_index - 1), min(len(paragraphs), para_index + 2)):
                    selected_indices.add(ctx_idx)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                para_stripped = para.strip()
                for idx, orig_para in enumerate(paragraphs):
                    if orig_para.strip() and para_stripped[:50] in orig_para.strip():
                        para_index = idx
                        # é€‰æ‹©å½“å‰æ®µè½åŠå…¶å‰åå„ä¸€ä¸ªæ®µè½
                        for ctx_idx in range(max(0, para_index - 1), min(len(paragraphs), para_index + 2)):
                            selected_indices.add(ctx_idx)
                        break
        
        # æŒ‰é¡ºåºæå–é€‰ä¸­çš„æ®µè½
        for idx in sorted(selected_indices):
            para = paragraphs[idx]
            if not para.strip():
                continue
            
            if len(selected_text) + len(para) <= chunk_size:
                selected_text += para + "\n\n"
            else:
                # å¦‚æœè¶…è¿‡é•¿åº¦é™åˆ¶ï¼Œå°è¯•æˆªå–éƒ¨åˆ†
                remaining = chunk_size - len(selected_text)
                if remaining > 200:  # è‡³å°‘ä¿ç•™200å­—ç¬¦
                    selected_text += para[:remaining] + "..."
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ®µè½ï¼Œè¿”å›å¼€å¤´éƒ¨åˆ†ï¼ˆå¢åŠ é•¿åº¦ï¼‰
        if not selected_text:
            # è¿”å›æ›´å¤šå†…å®¹ï¼ŒåŒ…æ‹¬æ–‡æ¡£å¼€å¤´
            selected_text = content[:chunk_size] + "..."
        
        return selected_text.strip()
    
    def _calculate_similarity(self, question: str, content: str) -> float:
        """
        è®¡ç®—é—®é¢˜å’Œæ–‡æ¡£å†…å®¹çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨embeddingï¼‰ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            content: æ–‡æ¡£å†…å®¹
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        try:
            from infrastructure.embedding.service import EmbeddingService
            import numpy as np
            
            # åˆå§‹åŒ–embeddingæœåŠ¡
            embedding_service = EmbeddingService()
            
            # å‘é‡åŒ–é—®é¢˜å’Œå†…å®¹
            # æ³¨æ„ï¼šcontentåº”è¯¥æ˜¯å·²ç»æå–çš„ç›¸å…³ç‰‡æ®µï¼Œä¸éœ€è¦å†æˆªå–å‰500å­—ç¬¦
            # å¦‚æœcontentå¤ªé•¿ï¼ˆè¶…è¿‡2000å­—ç¬¦ï¼‰ï¼Œæˆªå–å‰2000å­—ç¬¦ä»¥æé«˜æ€§èƒ½
            content_to_embed = content[:2000] if len(content) > 2000 else content
            
            # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
            if not content_to_embed or not content_to_embed.strip():
                log.warning(f"å†…å®¹ä¸ºç©ºï¼Œè¿”å›ç›¸ä¼¼åº¦0.0")
                return 0.0
            
            # è®°å½•embeddingæœåŠ¡ä¿¡æ¯
            model_name = embedding_service.get_model_name()
            log.debug(f"ä½¿ç”¨embeddingæ¨¡å‹: {model_name}")
            
            # å‘é‡åŒ–é—®é¢˜
            question_vector_raw = embedding_service.embed_text(question)
            question_vector = np.array(question_vector_raw)
            
            # å‘é‡åŒ–å†…å®¹
            content_vector_raw = embedding_service.embed_text(content_to_embed)
            content_vector = np.array(content_vector_raw)
            
            # éªŒè¯å‘é‡æ˜¯å¦æœ‰æ•ˆ
            if question_vector.size == 0 or content_vector.size == 0:
                log.warning(f"å‘é‡ä¸ºç©ºï¼Œè¿”å›ç›¸ä¼¼åº¦0.0 (question_size={question_vector.size}, content_size={content_vector.size})")
                return 0.0
            
            # æ£€æŸ¥å‘é‡ç»´åº¦æ˜¯å¦åŒ¹é…
            if question_vector.shape != content_vector.shape:
                log.error(f"å‘é‡ç»´åº¦ä¸åŒ¹é…: question={question_vector.shape}, content={content_vector.shape}")
                return 0.0
            
            # æ£€æŸ¥å‘é‡æ˜¯å¦å…¨ä¸ºé›¶
            if np.all(question_vector == 0) or np.all(content_vector == 0):
                log.warning(f"æ£€æµ‹åˆ°é›¶å‘é‡: question_all_zero={np.all(question_vector == 0)}, content_all_zero={np.all(content_vector == 0)}")
                log.warning(f"é—®é¢˜å‘é‡å‰5ä¸ªå€¼: {question_vector[:5]}")
                log.warning(f"å†…å®¹å‘é‡å‰5ä¸ªå€¼: {content_vector[:5]}")
                # å¦‚æœå‘é‡å…¨ä¸ºé›¶ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ä½œä¸ºå›é€€
                keywords = self._extract_keywords(question)
                content_lower = content.lower() if content else ""
                match_count = sum(1 for kw in keywords if kw.lower() in content_lower)
                estimated_similarity = min(0.4, 0.1 + match_count * 0.05)
                log.info(f"æ£€æµ‹åˆ°é›¶å‘é‡ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ä¼°è®¡ç›¸ä¼¼åº¦: {estimated_similarity:.3f} (åŒ¹é…å…³é”®è¯æ•°: {match_count})")
                return estimated_similarity
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(question_vector, content_vector)
            norm_q = np.linalg.norm(question_vector)
            norm_c = np.linalg.norm(content_vector)
            
            if norm_q == 0 or norm_c == 0:
                log.warning(f"å‘é‡æ¨¡é•¿ä¸º0ï¼Œè¿”å›ç›¸ä¼¼åº¦0.0 (norm_q={norm_q}, norm_c={norm_c})")
                return 0.0
            
            similarity = dot_product / (norm_q * norm_c)
            
            # ğŸ”´ ä¿®å¤ï¼šå¤„ç†è´Ÿæ•°ç›¸ä¼¼åº¦
            # ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´æ˜¯-1åˆ°1ï¼Œè´Ÿæ•°è¡¨ç¤ºå‘é‡æ–¹å‘ç›¸åæˆ–æ¥è¿‘å‚ç›´
            # è´Ÿæ•°ç›¸ä¼¼åº¦åº”è¯¥è¢«è§†ä¸ºä½ç›¸å…³æ€§ï¼Œä½†ä¸åº”è¯¥è¢«ç›´æ¥æˆªæ–­ä¸º0.0
            if similarity < 0:
                # è´Ÿæ•°ç›¸ä¼¼åº¦è¡¨ç¤ºä¸ç›¸å…³ï¼Œè®¾ä¸º0.0
                # ä½†è®°å½•æ—¥å¿—ä»¥ä¾¿æ’æŸ¥é—®é¢˜
                log.debug(f"æ£€æµ‹åˆ°è´Ÿæ•°ç›¸ä¼¼åº¦: {similarity:.3f} (é—®é¢˜: {question[:50]}..., å†…å®¹é•¿åº¦: {len(content_to_embed)})")
                log.debug(f"ç‚¹ç§¯: {dot_product:.3f}, norm_q: {norm_q:.3f}, norm_c: {norm_c:.3f}")
                similarity = 0.0
            else:
            # ç¡®ä¿ç›¸ä¼¼åº¦åœ¨0-1èŒƒå›´å†…
                similarity = min(1.0, float(similarity))
            
            # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼ˆä»…åœ¨ç›¸ä¼¼åº¦å¼‚å¸¸æ—¶ï¼‰
            if similarity < 0.1:
                log.debug(f"ç›¸ä¼¼åº¦è¾ƒä½: {similarity:.3f} (é—®é¢˜: {question[:50]}..., å†…å®¹é•¿åº¦: {len(content_to_embed)}, å‘é‡ç»´åº¦: {question_vector.shape[0]})")
                log.debug(f"é—®é¢˜å‘é‡ç»Ÿè®¡: min={question_vector.min():.3f}, max={question_vector.max():.3f}, mean={question_vector.mean():.3f}")
                log.debug(f"å†…å®¹å‘é‡ç»Ÿè®¡: min={content_vector.min():.3f}, max={content_vector.max():.3f}, mean={content_vector.mean():.3f}")
            
            return similarity
            
        except Exception as e:
            log.error(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ä¼°è®¡å€¼")
            import traceback
            log.debug(traceback.format_exc())
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼ŒåŸºäºå…³é”®è¯åŒ¹é…è¿”å›ä¸€ä¸ªä¼°è®¡å€¼ï¼ˆä½†æ ‡è®°ä¸ºä½ç›¸ä¼¼åº¦ï¼‰
            keywords = self._extract_keywords(question)
            content_lower = content.lower() if content else ""
            match_count = sum(1 for kw in keywords if kw.lower() in content_lower)
            # é™ä½é»˜è®¤å€¼ï¼Œé¿å…è¯¯åˆ¤ä¸ºç›¸å…³
            estimated_similarity = min(0.4, 0.1 + match_count * 0.05)  # åŸºç¡€åˆ†æ•°0.1ï¼Œæ¯ä¸ªå…³é”®è¯åŒ¹é…+0.05ï¼Œæœ€é«˜0.4
            log.info(f"ä½¿ç”¨å…³é”®è¯åŒ¹é…ä¼°è®¡ç›¸ä¼¼åº¦: {estimated_similarity:.3f} (åŒ¹é…å…³é”®è¯æ•°: {match_count})")
            return estimated_similarity
    
    def _analyze_search_results_with_ai(
        self, 
        question: str, 
        search_results: List[Dict[str, Any]], 
        keywords: List[str],
        related_concepts: List[str]
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨AIåˆ†ææœç´¢ç»“æœçš„ç›¸å…³æ€§å’Œå…³é”®ä¿¡æ¯ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            keywords: æå–çš„å…³é”®è¯
            related_concepts: ç›¸å…³æ¦‚å¿µ
            
        Returns:
            åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
            - relevance_summary: ç›¸å…³æ€§æ€»ç»“
            - key_points: å…³é”®ä¿¡æ¯ç‚¹
            - answer_strategy: ç­”æ¡ˆç”Ÿæˆç­–ç•¥
        """
        try:
            from infrastructure.llm.service import LLMService
            import json
            import re
            
            llm_service = LLMService()
            
            # æ„å»ºæœç´¢ç»“æœæ‘˜è¦ï¼ˆåªåŒ…å«æ ‡é¢˜å’Œç›¸ä¼¼åº¦ï¼Œé¿å…tokenè¿‡å¤šï¼‰
            results_summary = []
            for i, result in enumerate(search_results[:5], 1):
                results_summary.append({
                    "åºå·": i,
                    "æ ‡é¢˜": result.get("title", "æœªçŸ¥"),
                    "ç›¸ä¼¼åº¦": f"{result.get('similarity', 0):.2f}",
                    "å†…å®¹æ‘˜è¦": result.get("content", "")[:200] + "..." if len(result.get("content", "")) > 200 else result.get("content", "")
                })
            
            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œéœ€è¦åˆ†ææœç´¢ç»“æœä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æå–çš„å…³é”®è¯ã€‘
{', '.join(keywords) if keywords else 'æ— '}

ã€ç›¸å…³æ¦‚å¿µã€‘
{', '.join(related_concepts) if related_concepts else 'æ— '}

ã€æœç´¢ç»“æœã€‘
{json.dumps(results_summary, ensure_ascii=False, indent=2)}

è¯·åˆ†æï¼š
1. è¿™äº›æœç´¢ç»“æœä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§å¦‚ä½•ï¼Ÿ
2. å“ªäº›ç»“æœæœ€ç›¸å…³ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
3. ä»è¿™äº›ç»“æœä¸­å¯ä»¥æå–å“ªäº›å…³é”®ä¿¡æ¯ç‚¹ï¼Ÿ
4. åº”è¯¥å¦‚ä½•ç»„ç»‡ç­”æ¡ˆï¼Ÿï¼ˆç›´æ¥å›ç­”ã€åˆ†ç‚¹è¯´æ˜ã€å¯¹æ¯”è¯´æ˜ç­‰ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "relevance_summary": "ç›¸å…³æ€§æ€»ç»“ï¼ˆ1-2å¥è¯ï¼‰",
    "key_points": ["å…³é”®ä¿¡æ¯ç‚¹1", "å…³é”®ä¿¡æ¯ç‚¹2", "å…³é”®ä¿¡æ¯ç‚¹3"],
    "answer_strategy": "ç­”æ¡ˆç”Ÿæˆç­–ç•¥ï¼ˆå¦‚ï¼šç›´æ¥å›ç­”ã€åˆ†ç‚¹è¯´æ˜ã€å¯¹æ¯”è¯´æ˜ç­‰ï¼‰",
    "most_relevant_results": [1, 2]  // æœ€ç›¸å…³çš„ç»“æœåºå·åˆ—è¡¨
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
"""
            
            log.info("ä½¿ç”¨AIåˆ†ææœç´¢ç»“æœ...")
            response = llm_service.generate(prompt)
            
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            json_match = re.search(r'\{[^{}]*"relevance_summary"[^{}]*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                json_str = response.strip()
                json_str = re.sub(r'```json\s*', '', json_str)
                json_str = re.sub(r'```\s*', '', json_str)
                json_str = json_str.strip()
            
            try:
                result = json.loads(json_str)
                return result
            except json.JSONDecodeError as e:
                log.warning(f"AIåˆ†æç»“æœJSONè§£æå¤±è´¥: {e}ï¼Œå“åº”: {response[:200]}")
                return {
                    "relevance_summary": "æœç´¢ç»“æœä¸é—®é¢˜ç›¸å…³",
                    "key_points": [],
                    "answer_strategy": "ç›´æ¥å›ç­”",
                    "most_relevant_results": [1, 2, 3],
                }
                
        except Exception as e:
            log.warning(f"AIåˆ†ææœç´¢ç»“æœå¤±è´¥: {e}")
            return {
                "relevance_summary": "æœç´¢ç»“æœä¸é—®é¢˜ç›¸å…³",
                "key_points": [],
                "answer_strategy": "ç›´æ¥å›ç­”",
                "most_relevant_results": [1, 2, 3],
            }
    
    def _build_answer_prompt(
        self,
        question: str,
        context: str,
        search_results: List[Dict[str, Any]],
        analysis_result: Dict[str, Any],
        keywords: List[str]
    ) -> str:
        """
        æ„å»ºç­”æ¡ˆç”Ÿæˆçš„Promptï¼Œè®©AIæ›´å¥½åœ°åˆ©ç”¨æœç´¢ç»“æœã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            analysis_result: AIåˆ†æç»“æœ
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            å®Œæ•´çš„Prompt
        """
        # æ„å»ºæœç´¢ç»“æœä¿¡æ¯
        results_info = []
        for i, result in enumerate(search_results, 1):
            similarity = result.get("similarity", 0)
            title = result.get("title", "æœªçŸ¥")
            results_info.append(f"{i}. {title} (ç›¸ä¼¼åº¦: {similarity:.2f})")
        
        results_summary = "\n".join(results_info)
        
        # è·å–AIåˆ†æçš„å…³é”®ä¿¡æ¯
        relevance_summary = analysis_result.get("relevance_summary", "")
        key_points = analysis_result.get("key_points", [])
        answer_strategy = analysis_result.get("answer_strategy", "ç›´æ¥å›ç­”")
        most_relevant = analysis_result.get("most_relevant_results", [])
        
        # æ„å»ºå…³é”®ä¿¡æ¯ç‚¹
        key_points_text = ""
        if key_points:
            key_points_text = "\n".join([f"- {point}" for point in key_points[:5]])
        
        # æ„å»ºæœ€ç›¸å…³ç»“æœæç¤º
        most_relevant_text = ""
        if most_relevant:
            most_relevant_titles = [
                search_results[i-1].get("title", "") 
                for i in most_relevant 
                if 1 <= i <= len(search_results)
            ]
            if most_relevant_titles:
                most_relevant_text = f"\nã€æœ€ç›¸å…³æ–‡æ¡£ã€‘ä¼˜å…ˆå‚è€ƒä»¥ä¸‹æ–‡æ¡£ï¼š{', '.join(most_relevant_titles)}"
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„AIçŸ¥è¯†åº“åŠ©æ‰‹ï¼Œæ“…é•¿æ·±å…¥åˆ†ææ–‡æ¡£å†…å®¹å¹¶ç”Ÿæˆé«˜è´¨é‡ã€ç»“æ„åŒ–çš„ç­”æ¡ˆã€‚

ã€ä»»åŠ¡ã€‘
åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæ·±å…¥åˆ†æå¹¶å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ä½ éœ€è¦ï¼š
1. **æ·±å…¥ç†è§£**ï¼šä»”ç»†é˜…è¯»æ–‡æ¡£å†…å®¹ï¼Œç†è§£ä¸Šä¸‹æ–‡å’Œç»†èŠ‚
2. **æå–å…³é”®ä¿¡æ¯**ï¼šè¯†åˆ«ä¸é—®é¢˜ç›¸å…³çš„æ ¸å¿ƒä¿¡æ¯ã€å…³é”®æ­¥éª¤ã€é‡è¦æ¦‚å¿µ
3. **ç»¼åˆåˆ†æ**ï¼šå¦‚æœæ¶‰åŠå¤šä¸ªæ–‡æ¡£ï¼Œè¦ç»¼åˆä¸åŒæ–‡æ¡£çš„ä¿¡æ¯ï¼Œå½¢æˆå®Œæ•´çš„ç­”æ¡ˆ
4. **ç»“æ„åŒ–ç»„ç»‡**ï¼šæŒ‰ç…§é€»è¾‘é¡ºåºç»„ç»‡ç­”æ¡ˆï¼Œä½¿ç”¨æ¸…æ™°çš„æ®µè½å’Œåˆ†ç‚¹è¯´æ˜
5. **æ·±å…¥é˜è¿°**ï¼šä¸ä»…è¦å¼•ç”¨æ–‡æ¡£å†…å®¹ï¼Œè¿˜è¦è¿›è¡Œè§£é‡Šã€åˆ†æå’Œæ€»ç»“

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æå–çš„å…³é”®è¯ã€‘
{', '.join(keywords) if keywords else 'æ— '}

ã€æœç´¢ç»“æœåˆ†æã€‘
{relevance_summary if relevance_summary else 'æœç´¢ç»“æœä¸é—®é¢˜ç›¸å…³'}

ã€å…³é”®ä¿¡æ¯ç‚¹ã€‘
{key_points_text if key_points_text else 'éœ€è¦ä»æ–‡æ¡£ä¸­æå–'}

ã€ç­”æ¡ˆç”Ÿæˆç­–ç•¥ã€‘
{answer_strategy}{most_relevant_text}

ã€æ–‡æ¡£å†…å®¹ã€‘
{context}

ã€æœç´¢ç»“æœåˆ—è¡¨ã€‘
{results_summary}

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. **æ·±åº¦åˆ†æ**ï¼š
   - ä¸è¦ç®€å•å¼•ç”¨æ–‡æ¡£ä¸­çš„ä¸€ä¸¤å¥è¯
   - è¦æ·±å…¥ç†è§£æ–‡æ¡£å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶è¿›è¡Œè§£é‡Š
   - å¦‚æœæ–‡æ¡£æåˆ°æŸä¸ªåŠŸèƒ½æˆ–æ¦‚å¿µï¼Œè¦è¯¦ç»†è¯´æ˜å…¶ä½œç”¨ã€ä½¿ç”¨æ–¹æ³•ã€æ³¨æ„äº‹é¡¹ç­‰

2. **å®Œæ•´æ€§**ï¼š
   - å¦‚æœæ–‡æ¡£ä¸­æœ‰å¤šä¸ªç›¸å…³ä¿¡æ¯ç‚¹ï¼Œè¦å…¨éƒ¨æå–å¹¶ç»¼åˆå›ç­”
   - ä¸è¦é—æ¼é‡è¦çš„ç»†èŠ‚ã€æ­¥éª¤ã€æ¡ä»¶ã€é™åˆ¶ç­‰
   - å¦‚æœæ¶‰åŠå¤šä¸ªæ–¹é¢ï¼Œè¦å…¨é¢è¦†ç›–

3. **ç»“æ„åŒ–ç»„ç»‡**ï¼š
   - ä½¿ç”¨æ¸…æ™°çš„æ®µè½ç»“æ„
   - å¯¹äºå¤æ‚é—®é¢˜ï¼Œä½¿ç”¨åˆ†ç‚¹è¯´æ˜ï¼ˆ1. 2. 3.ï¼‰æˆ–åˆ†ç±»è¯´æ˜
   - æŒ‰ç…§é€»è¾‘é¡ºåºç»„ç»‡ï¼šæ¦‚è¿° â†’ è¯¦ç»†è¯´æ˜ â†’ æ€»ç»“

4. **å¯è¯»æ€§å’Œä¸“ä¸šæ€§**ï¼š
   - ä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼Œè¯­è¨€æµç•…è‡ªç„¶
   - ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œä½†ç¡®ä¿æ˜“äºç†è§£
   - é¿å…å†—ä½™å’Œé‡å¤
   - é€‚å½“ä½¿ç”¨è¿‡æ¸¡è¯ï¼Œä½¿ç­”æ¡ˆè¿è´¯

5. **å¼•ç”¨å’Œæ ‡æ³¨**ï¼š
   - åœ¨ç­”æ¡ˆå¼€å¤´æˆ–å…³é”®éƒ¨åˆ†æåŠæ–‡æ¡£æ¥æºï¼ˆå¦‚"æ ¹æ®ã€ŠXXXæ–‡æ¡£ã€‹..."ï¼‰
   - å¦‚æœä¿¡æ¯æ¥è‡ªå¤šä¸ªæ–‡æ¡£ï¼Œå¯ä»¥åˆ†åˆ«æ ‡æ³¨

ã€ç­”æ¡ˆç»“æ„å»ºè®®ã€‘
- **å¼€å¤´**ï¼šç®€è¦è¯´æ˜æ‰¾åˆ°äº†å“ªäº›ç›¸å…³ä¿¡æ¯ï¼ˆå¯æåŠæ–‡æ¡£åç§°ï¼‰
- **ä¸»ä½“**ï¼šè¯¦ç»†å›ç­”é—®é¢˜çš„å„ä¸ªæ–¹é¢ï¼Œä½¿ç”¨åˆ†ç‚¹æˆ–åˆ†æ®µè¯´æ˜
- **ç»“å°¾**ï¼šå¦‚æœ‰å¿…è¦ï¼Œè¿›è¡Œæ€»ç»“æˆ–è¡¥å……è¯´æ˜

ã€æ³¨æ„äº‹é¡¹ã€‘
- **ç›¸å…³æ€§æ£€æŸ¥ï¼ˆæœ€é‡è¦ï¼‰**ï¼š
  - é¦–å…ˆåˆ¤æ–­æ–‡æ¡£å†…å®¹æ˜¯å¦çœŸçš„ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³
  - å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜**å®Œå…¨ä¸ç›¸å…³**æˆ–**ç›¸å…³æ€§å¾ˆä½**ï¼ˆç›¸ä¼¼åº¦<0.5ï¼‰ï¼Œå¿…é¡»æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯"
  - **ä¸è¦**å¼ºè¡Œå…³è”ä¸ç›¸å…³çš„å†…å®¹
  - **ä¸è¦**åŸºäºä¸ç›¸å…³çš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
  - å¦‚æœæ–‡æ¡£ç›¸ä¼¼åº¦å¾ˆä½ï¼Œåº”è¯¥æ˜ç¡®æ‹’ç»å›ç­”ï¼Œè€Œä¸æ˜¯å¼ºè¡Œç”Ÿæˆç­”æ¡ˆ

- å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›´æ¥å›ç­”é—®é¢˜çš„ä¿¡æ¯ï¼Œå¯ä»¥åŸºäºç›¸å…³å†…å®¹è¿›è¡Œåˆç†æ¨æ–­ï¼Œä½†è¦è¯´æ˜è¿™æ˜¯åŸºäºæ–‡æ¡£çš„æ¨æ–­
- å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜ä¸å®Œå…¨åŒ¹é…ï¼Œè¯´æ˜æ–‡æ¡£ä¸­æ‰¾åˆ°äº†å“ªäº›ç›¸å…³ä¿¡æ¯ï¼Œå¹¶è§£é‡Šè¿™äº›ä¿¡æ¯å¦‚ä½•å¸®åŠ©å›ç­”é—®é¢˜
- å¦‚æœå¤šä¸ªæ–‡æ¡£æœ‰å†²çªä¿¡æ¯ï¼Œè¦å¯¹æ¯”è¯´æ˜å¹¶æŒ‡å‡ºå·®å¼‚
- å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

ã€ç­”æ¡ˆã€‘
è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£å†…å®¹ï¼Œæ·±å…¥åˆ†æå¹¶å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¦æ±‚ç­”æ¡ˆå®Œæ•´ã€æ·±å…¥ã€æœ‰æ¡ç†ï¼š
"""
        
        return prompt

    def _verify_answer_relevance(self, question: str, answer: str, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        éªŒè¯ç­”æ¡ˆæ˜¯å¦çœŸçš„å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            éªŒè¯ç»“æœï¼ŒåŒ…å«is_relevantå’Œreason
        """
        try:
            # å¦‚æœæœç´¢ç»“æœçš„å¹³å‡ç›¸ä¼¼åº¦å¾ˆä½ï¼Œç›´æ¥è®¤ä¸ºä¸ç›¸å…³
            if search_results:
                avg_similarity = sum([r.get("similarity", 0) for r in search_results]) / len(search_results)
                if avg_similarity < 0.4:
                    return {
                        "is_relevant": False,
                        "reason": f"æœç´¢ç»“æœå¹³å‡ç›¸ä¼¼åº¦è¿‡ä½ ({avg_similarity:.3f} < 0.4)"
                    }
            
            # æå–é—®é¢˜å…³é”®è¯
            question_keywords = self._extract_keywords(question)
            if not question_keywords:
                return {"is_relevant": True, "reason": "æ— æ³•æå–é—®é¢˜å…³é”®è¯"}
            
            # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åŒ…å«é—®é¢˜çš„ä¸»è¦å…³é”®è¯
            answer_lower = answer.lower()
            matched_keywords = [kw for kw in question_keywords if kw.lower() in answer_lower]
            match_ratio = len(matched_keywords) / len(question_keywords) if question_keywords else 0
            
            # å¦‚æœåŒ¹é…çš„å…³é”®è¯å°‘äº50%ï¼Œè®¤ä¸ºä¸ç›¸å…³
            if match_ratio < 0.5:
                return {
                    "is_relevant": False,
                    "reason": f"ç­”æ¡ˆä¸­åŒ¹é…çš„å…³é”®è¯æ¯”ä¾‹è¿‡ä½ ({match_ratio:.2%} < 50%)"
                }
            
            return {"is_relevant": True, "reason": "ç­”æ¡ˆç›¸å…³æ€§éªŒè¯é€šè¿‡"}
            
        except Exception as e:
            log.warning(f"ç­”æ¡ˆç›¸å…³æ€§éªŒè¯å¤±è´¥: {e}")
            # éªŒè¯å¤±è´¥æ—¶ï¼Œé»˜è®¤è®¤ä¸ºç›¸å…³ï¼ˆé¿å…è¯¯åˆ¤ï¼‰
            return {"is_relevant": True, "reason": f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}"}
    
    def _should_use_web_search(self, question: str, kb_result: Dict[str, Any]) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨ç½‘ç»œæœç´¢ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            kb_result: çŸ¥è¯†åº“æœç´¢ç»“æœ
            
        Returns:
            æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
        """
        # å¦‚æœçŸ¥è¯†åº“æœç´¢æˆåŠŸä¸”æœ‰ç›¸å…³æ–‡æ¡£ï¼Œæ£€æŸ¥ç›¸ä¼¼åº¦
        if kb_result.get("success") and len(kb_result.get("sources", [])) > 0:
            sources = kb_result.get("sources", [])
            max_similarity = max([s.get("similarity", 0) for s in sources])
            
            # å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦>=0.6ï¼Œè®¤ä¸ºçŸ¥è¯†åº“ç»“æœè¶³å¤Ÿå¥½ï¼Œä¸éœ€è¦ç½‘ç»œæœç´¢
            if max_similarity >= 0.6:
                return False
            
            # å¦‚æœç›¸ä¼¼åº¦åœ¨0.5-0.6ä¹‹é—´ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯é€šç”¨æ¦‚å¿µé—®é¢˜
            if max_similarity >= 0.5:
                # åˆ¤æ–­æ˜¯å¦æ˜¯é€šç”¨æ¦‚å¿µé—®é¢˜ï¼ˆå¦‚"æ˜¯ä»€ä¹ˆ"ã€"å®šä¹‰"ç­‰ï¼‰
                if self._is_general_concept_question(question):
                    log.info(f"æ£€æµ‹åˆ°é€šç”¨æ¦‚å¿µé—®é¢˜ï¼Œä¸”æ–‡æ¡£ç›¸ä¼¼åº¦è¾ƒä½({max_similarity:.3f})ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢")
                    return True
            
            # å¦‚æœç›¸ä¼¼åº¦<0.5ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢
            if max_similarity < 0.5:
                log.info(f"æ–‡æ¡£ç›¸ä¼¼åº¦è¿‡ä½({max_similarity:.3f})ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢")
                return True
        
        # å¦‚æœçŸ¥è¯†åº“æœç´¢å¤±è´¥æˆ–æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢
        if not kb_result.get("success") or len(kb_result.get("sources", [])) == 0:
            log.info("çŸ¥è¯†åº“æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢")
            return True
        
        return False
    
    def _is_general_concept_question(self, question: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦æ˜¯é€šç”¨æ¦‚å¿µé—®é¢˜ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            æ˜¯å¦æ˜¯é€šç”¨æ¦‚å¿µé—®é¢˜
        """
        # é€šç”¨æ¦‚å¿µé—®é¢˜çš„å…³é”®è¯
        concept_keywords = [
            "æ˜¯ä»€ä¹ˆ", "ä»€ä¹ˆæ˜¯", "å®šä¹‰", "å«ä¹‰", "æ„æ€", "æ¦‚å¿µ",
            "ä»‹ç»", "è¯´æ˜", "è§£é‡Š", "å¦‚ä½•ç†è§£", "æ€ä¹ˆç†è§£"
        ]
        
        question_lower = question.lower()
        for keyword in concept_keywords:
            if keyword in question_lower:
                return True
        
        return False
    
    def _search_web_and_merge(self, question: str, kb_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        æœç´¢ç½‘ç»œå¹¶åˆå¹¶ç»“æœã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            kb_result: çŸ¥è¯†åº“æœç´¢ç»“æœ
            
        Returns:
            åˆå¹¶åçš„ç»“æœ
        """
        try:
            web_service = self.web_search_service
            if not web_service:
                log.warning("ç½‘ç»œæœç´¢æœåŠ¡ä¸å¯ç”¨ï¼Œè¿”å›çŸ¥è¯†åº“ç»“æœ")
                return kb_result
            
            # æœç´¢ç½‘ç»œ
            web_results = web_service.search(question, max_results=5)
            
            if not web_results:
                log.warning("ç½‘ç»œæœç´¢æœªæ‰¾åˆ°ç»“æœï¼Œè¿”å›çŸ¥è¯†åº“ç»“æœ")
                return kb_result
            
            # ä½¿ç”¨LLMåˆå¹¶çŸ¥è¯†åº“å’Œç½‘ç»œæœç´¢ç»“æœ
            from infrastructure.llm.service import LLMService
            llm_service = LLMService()
            
            # æ„å»ºåˆå¹¶æç¤ºè¯
            kb_answer = kb_result.get("answer", "")
            kb_sources = kb_result.get("sources", [])
            
            # æ„å»ºç½‘ç»œæœç´¢ç»“æœæ‘˜è¦
            web_summary = "\n".join([
                f"- {r.get('title', '')}: {r.get('snippet', '')[:200]}..."
                for r in web_results[:3]
            ])
            
            # æ„å»ºåˆå¹¶æç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œéœ€è¦ç»“åˆçŸ¥è¯†åº“ä¿¡æ¯å’Œç½‘ç»œæœç´¢ç»“æœæ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘
{'æ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³æ–‡æ¡£ï¼š' if kb_sources else 'æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£'}
{chr(10).join([f'- {s.get("title", "")} (ç›¸ä¼¼åº¦: {s.get("similarity", 0):.2f})' for s in kb_sources[:3]]) if kb_sources else 'æ— '}

{'ã€çŸ¥è¯†åº“ç­”æ¡ˆã€‘' if kb_answer and kb_result.get('success') else ''}
{kb_answer if kb_answer and kb_result.get('success') else 'çŸ¥è¯†åº“æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯'}

ã€ç½‘ç»œæœç´¢ç»“æœã€‘
{web_summary}

ã€è¦æ±‚ã€‘
1. ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ä¿¡æ¯ï¼ˆå¦‚æœçŸ¥è¯†åº“æœ‰ç›¸å…³ä¿¡æ¯ï¼‰
2. ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœè¡¥å……çŸ¥è¯†åº“ä¿¡æ¯çš„ä¸è¶³
3. æ˜ç¡®æ ‡æ³¨ä¿¡æ¯æ¥æºï¼š
   - å¦‚æœä¿¡æ¯æ¥è‡ªçŸ¥è¯†åº“ï¼Œæ ‡æ³¨"æ ¹æ®çŸ¥è¯†åº“æ–‡æ¡£..."
   - å¦‚æœä¿¡æ¯æ¥è‡ªç½‘ç»œæœç´¢ï¼Œæ ‡æ³¨"æ ¹æ®ç½‘ç»œæœç´¢..."
4. å¦‚æœçŸ¥è¯†åº“å’Œç½‘ç»œä¿¡æ¯æœ‰å†²çªï¼Œä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ä¿¡æ¯
5. ç­”æ¡ˆè¦å®Œæ•´ã€å‡†ç¡®ã€æœ‰æ¡ç†
6. ä½¿ç”¨ç®€ä½“ä¸­æ–‡å›ç­”

ã€ç­”æ¡ˆã€‘
è¯·ç»“åˆä»¥ä¸Šä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼š
"""
            
            # ç”Ÿæˆåˆå¹¶åçš„ç­”æ¡ˆ
            merged_answer = llm_service.generate(prompt)
            
            # åˆå¹¶æ¥æº
            merged_sources = list(kb_sources)
            for web_result in web_results[:3]:
                merged_sources.append({
                    "title": web_result.get("title", ""),
                    "url": web_result.get("url", ""),
                    "source": "web_search",
                    "similarity": 0.0,  # ç½‘ç»œæœç´¢ç»“æœæ²¡æœ‰ç›¸ä¼¼åº¦
                })
            
            # ä¿å­˜ç½‘ç»œæœç´¢ç»“æœ
            query_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            self._save_query_result(question, "web_search", {
                "results_count": len(web_results),
                "results": web_results[:5]
            }, query_timestamp)
            
            log.info("âœ… ç½‘ç»œæœç´¢ç»“æœå·²åˆå¹¶åˆ°ç­”æ¡ˆä¸­")
            
            return {
                "success": True,
                "answer": merged_answer.strip(),
                "sources": merged_sources,
                "has_web_search": True,  # æ ‡è®°ä½¿ç”¨äº†ç½‘ç»œæœç´¢
                "suggest_web_search": False,  # å·²ç»ä½¿ç”¨äº†ï¼Œä¸å†å»ºè®®
                "max_similarity": max([s.get("similarity", 0) for s in kb_sources]) if kb_sources else 0.0,
            }
            
        except Exception as e:
            log.error(f"ç½‘ç»œæœç´¢å’Œåˆå¹¶å¤±è´¥: {e}")
            # å¦‚æœç½‘ç»œæœç´¢å¤±è´¥ï¼Œè¿”å›çŸ¥è¯†åº“ç»“æœ
            return kb_result
    
    def get_collection_info(self) -> Dict[str, Any]:
        """
        è·å–å‘é‡å­˜å‚¨ä¿¡æ¯ã€‚

        Returns:
            é›†åˆä¿¡æ¯
        """
        try:
            info = self.rag_engine.vector_store.get_collection_info()
            return {
                "success": True,
                "info": info,
            }
        except Exception as e:
            log.error(f"è·å–é›†åˆä¿¡æ¯å¤±è´¥: {e}")
            return {
                "success": False,
                "info": {},
            }

