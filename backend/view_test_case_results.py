#!/usr/bin/env python3
# encoding: utf-8
"""æŸ¥çœ‹æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœçš„å·¥å…·è„šæœ¬"""
import sys
import json
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))


def find_latest_test_case_file():
    """æŸ¥æ‰¾æœ€æ–°çš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœæ–‡ä»¶"""
    # ä¼˜å…ˆä½¿ç”¨é¡¹ç›®ç›®å½•ä¸‹çš„è·¯å¾„
    project_root = Path(__file__).parent.parent
    possible_paths = [
        project_root / "data/debug/ai_runs",  # é¡¹ç›®ç›®å½•ä¸‹ï¼ˆä¼˜å…ˆï¼‰
        Path("data/debug/ai_runs"),  # ç›¸å¯¹è·¯å¾„ï¼ˆå¤‡ç”¨ï¼‰
    ]
    
    debug_dir = None
    for path in possible_paths:
        if path.exists():
            debug_dir = path
            break
    
    if not debug_dir:
        print("âŒ è°ƒè¯•ç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•è¿‡çš„è·¯å¾„:")
        for path in possible_paths:
            print(f"  - {path}")
        return None
    
    # æŸ¥æ‰¾æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœæ–‡ä»¶
    test_case_files = list(debug_dir.glob("*generate_test_cases*.json"))
    if not test_case_files:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœæ–‡ä»¶")
        return None
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    test_case_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    return test_case_files[0]


def assess_test_case_quality(test_case: Dict[str, Any], index: int) -> Dict[str, Any]:
    """è¯„ä¼°å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„è´¨é‡"""
    issues = []
    score = 1.0
    
    # æ£€æŸ¥å¿…å¡«å­—æ®µ
    if not test_case.get("case_name") or not str(test_case.get("case_name", "")).strip():
        issues.append("ç”¨ä¾‹åç§°ä¸ºç©º")
        score -= 0.3
    
    if not test_case.get("module_name") or not str(test_case.get("module_name", "")).strip():
        issues.append("åŠŸèƒ½æ¨¡å—ä¸ºç©º")
        score -= 0.2
    
    # æ£€æŸ¥æ­¥éª¤
    steps = test_case.get("steps", [])
    if not isinstance(steps, list):
        steps = []
    if len(steps) < 2:
        issues.append(f"æ­¥éª¤æ•°ä¸è¶³ï¼ˆ{len(steps)}æ­¥ï¼Œå»ºè®®è‡³å°‘2æ­¥ï¼‰")
        score -= 0.2
    elif len(steps) < 3:
        issues.append(f"æ­¥éª¤æ•°è¾ƒå°‘ï¼ˆ{len(steps)}æ­¥ï¼Œå»ºè®®è‡³å°‘3æ­¥ï¼‰")
        score -= 0.1
    
    # æ£€æŸ¥é¢„æœŸç»“æœ
    expected_result = str(test_case.get("expected_result", "")).strip()
    if not expected_result:
        issues.append("é¢„æœŸç»“æœä¸ºç©º")
        score -= 0.3
    else:
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†é€šç”¨é¢„æœŸç»“æœ
        generic_patterns = ["æ­£ç¡®æ˜¾ç¤º", "æ­£å¸¸æ˜¾ç¤º", "éªŒè¯é€šè¿‡", "ç¬¦åˆé¢„æœŸ", "æ»¡è¶³è¦æ±‚", "ç‚¹å‡»å…³é—­ç›´æ¥æ¶ˆå¤±"]
        if any(pattern in expected_result for pattern in generic_patterns):
            issues.append("ä½¿ç”¨äº†é€šç”¨é¢„æœŸç»“æœï¼Œå»ºè®®ä½¿ç”¨å…·ä½“æè¿°")
            score -= 0.1
        # æ£€æŸ¥é¢„æœŸç»“æœé•¿åº¦
        if len(expected_result) < 5:
            issues.append("é¢„æœŸç»“æœè¿‡çŸ­ï¼Œå¯èƒ½ä¸å¤Ÿå…·ä½“")
            score -= 0.1
    
    # æ£€æŸ¥å‰ç½®æ¡ä»¶ï¼ˆå¯é€‰ï¼‰
    preconditions = str(test_case.get("preconditions", "")).strip()
    if preconditions and len(preconditions) < 3:
        issues.append("å‰ç½®æ¡ä»¶è¿‡çŸ­")
        score -= 0.05
    
    # æ£€æŸ¥æ­¥éª¤è´¨é‡
    for step_index, step in enumerate(steps, 1):
        step_str = str(step).strip()
        if len(step_str) < 5:
            issues.append(f"æ­¥éª¤{step_index}æè¿°è¿‡çŸ­æˆ–ä¸æ¸…æ™°")
            score -= 0.05
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¦æ­¢çš„æ“ä½œ
        banned_actions = ["ç™»å½•åå°", "æŸ¥çœ‹æ•°æ®åº“", "æ‰‹åŠ¨æŠ•æ”¾", "åå°æ“ä½œ"]
        if any(action in step_str for action in banned_actions):
            issues.append(f"æ­¥éª¤{step_index}åŒ…å«ä¸å¯æ‰§è¡Œçš„æ“ä½œ")
            score -= 0.1
    
    score = max(0, min(1, score))  # é™åˆ¶åœ¨0-1ä¹‹é—´
    
    return {"score": score, "issues": issues}


def view_test_case_results(file_path: Optional[Path] = None):
    """æŸ¥çœ‹æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœ"""
    if file_path is None:
        file_path = find_latest_test_case_file()
        if file_path is None:
            return
    
    print(f"ğŸ“„ æ–‡ä»¶è·¯å¾„: {file_path}\n")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æå–æµ‹è¯•ç”¨ä¾‹æ•°æ®
    payload = data.get("payload", {})
    test_cases = payload.get("test_cases", [])
    by_function_point = payload.get("by_function_point", {})
    meta = payload.get("meta", {})
    
    if not test_cases:
        print("âš ï¸  æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹æ•°æ®")
        print(f"\næ–‡ä»¶å†…å®¹é”®: {list(payload.keys())}")
        return
    
    print("="*80)
    print("æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœ")
    print("="*80)
    
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  åŠŸèƒ½ç‚¹æ€»æ•°: {meta.get('total_function_points', 0)}")
    print(f"  å·²å¤„ç†åŠŸèƒ½ç‚¹: {meta.get('processed_function_points', 0)}")
    print(f"  ç”Ÿæˆç”¨ä¾‹æ•°: {len(test_cases)}")
    print(f"  è­¦å‘Šæ•°: {meta.get('total_warnings', 0)}")
    
    # è´¨é‡è¯„ä¼°
    quality_results = [assess_test_case_quality(tc, i+1) for i, tc in enumerate(test_cases)]
    average_score = sum(q["score"] for q in quality_results) / len(quality_results) if quality_results else 0
    problem_count = sum(1 for q in quality_results if q["score"] < 0.8 or len(q["issues"]) > 0)
    
    print(f"\nğŸ“ˆ è´¨é‡è¯„ä¼°:")
    print(f"  å¹³å‡è´¨é‡è¯„åˆ†: {average_score:.2%}")
    print(f"  æœ‰è´¨é‡é—®é¢˜çš„ç”¨ä¾‹: {problem_count} / {len(test_cases)}")
    
    # æ˜¾ç¤ºç”¨ä¾‹åˆ—è¡¨
    print(f"\nğŸ“‹ æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ï¼ˆå…±{len(test_cases)}ä¸ªï¼‰:")
    print("-"*80)
    
    for index, test_case in enumerate(test_cases, 1):
        quality = quality_results[index - 1]
        print(f"\nã€ç”¨ä¾‹ {index}ã€‘")
        print(f"  åŠŸèƒ½æ¨¡å—: {test_case.get('module_name', 'N/A')}")
        if test_case.get('sub_module'):
            print(f"  å­åŠŸèƒ½ç‚¹: {test_case.get('sub_module')}")
        print(f"  ç”¨ä¾‹åç§°: {test_case.get('case_name', 'N/A')}")
        print(f"  å‰ç½®æ¡ä»¶: {test_case.get('preconditions', 'æ— ')}")
        print(f"  ä¼˜å…ˆçº§: {test_case.get('priority', 'æœªè®¾ç½®')}")
        print(f"  æ­¥éª¤æ•°: {len(test_case.get('steps', []))}")
        print(f"  æµ‹è¯•æ­¥éª¤:")
        for step_idx, step in enumerate(test_case.get('steps', []), 1):
            print(f"    {step_idx}. {step}")
        print(f"  é¢„æœŸç»“æœ: {test_case.get('expected_result', 'N/A')}")
        print(f"  è´¨é‡è¯„åˆ†: {quality['score']:.2%}")
        if quality['issues']:
            print(f"  è´¨é‡é—®é¢˜: {'; '.join(quality['issues'])}")
    
    print("\n" + "="*80)
    print(f"\nğŸ’¡ æç¤º:")
    print(f"  - æ–‡ä»¶ä½ç½®: {file_path}")
    print(f"  - å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹å®Œæ•´JSON: cat {file_path} | jq '.payload.test_cases'")
    print(f"  - è´¨é‡è¯„åˆ†è¯´æ˜: 1.0=ä¼˜ç§€, 0.8-0.9=è‰¯å¥½, 0.7-0.8=ä¸€èˆ¬, <0.7=éœ€è¦æ”¹è¿›")


def list_test_case_files():
    """åˆ—å‡ºæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœæ–‡ä»¶"""
    # ä¼˜å…ˆä½¿ç”¨é¡¹ç›®ç›®å½•ä¸‹çš„è·¯å¾„
    project_root = Path(__file__).parent.parent
    possible_paths = [
        project_root / "data/debug/ai_runs",  # é¡¹ç›®ç›®å½•ä¸‹ï¼ˆä¼˜å…ˆï¼‰
        Path("data/debug/ai_runs"),  # ç›¸å¯¹è·¯å¾„ï¼ˆå¤‡ç”¨ï¼‰
    ]
    
    debug_dir = None
    for path in possible_paths:
        if path.exists():
            debug_dir = path
            break
    
    if not debug_dir:
        print("âŒ è°ƒè¯•ç›®å½•ä¸å­˜åœ¨")
        return
    
    test_case_files = list(debug_dir.glob("*generate_test_cases*.json"))
    if not test_case_files:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœæ–‡ä»¶")
        return
    
    test_case_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    
    print(f"ğŸ“ æ‰¾åˆ° {len(test_case_files)} ä¸ªæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœæ–‡ä»¶:\n")
    
    for i, file_path in enumerate(test_case_files[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
        mtime = file_path.stat().st_mtime
        mtime_str = datetime.fromtimestamp(mtime).strftime("%Y-%m-%d %H:%M:%S")
        
        # è¯»å–æ–‡ä»¶è·å–ç”¨ä¾‹æ•°é‡
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                payload = data.get("payload", {})
                test_cases = payload.get("test_cases", [])
                case_count = len(test_cases) if isinstance(test_cases, list) else 0
        except:
            case_count = 0
        
        print(f"  {i}. {file_path.name}")
        print(f"     æ—¶é—´: {mtime_str}")
        print(f"     ç”¨ä¾‹æ•°: {case_count}")
        print()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æŸ¥çœ‹æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœ")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœæ–‡ä»¶")
    parser.add_argument("--file", type=str, help="æŸ¥çœ‹æŒ‡å®šæ–‡ä»¶")
    args = parser.parse_args()
    
    if args.list:
        list_test_case_files()
    elif args.file:
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        project_root = Path(__file__).parent.parent
        possible_paths = [
            project_root / "data/debug/ai_runs" / args.file,  # é¡¹ç›®ç›®å½•ä¸‹ï¼ˆä¼˜å…ˆï¼‰
            Path("data/debug/ai_runs") / args.file,  # ç›¸å¯¹è·¯å¾„ï¼ˆå¤‡ç”¨ï¼‰
        ]
        file_path = None
        for path in possible_paths:
            if path.exists():
                file_path = path
                break
        if file_path:
            view_test_case_results(file_path)
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
    else:
        view_test_case_results()

